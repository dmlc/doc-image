Namespace(aux=True, aux_weight=0.5, backbone='resnet50', base_size=520, batch_size=16, checkname='deeplab_res200_ade', crop_size=480, ctx=[gpu(0), gpu(1), gpu(2), gpu(3), gpu(4), gpu(5), gpu(6), gpu(7)], dataset='ade20k', dtype='float32', epochs=120, eval=False, kvstore='device', log_interval=20, logging_file='train.log', lr=0.01, mode=None, model='fcn', model_zoo='deeplab_resnest200_ade', momentum=0.9, ngpus=8, no_cuda=False, no_val=False, no_wd=False, norm_kwargs={'num_devices': 8}, norm_layer=<class 'mxnet.gluon.contrib.nn.basic_layers.SyncBatchNorm'>, optimizer='sgd', resume=None, save_dir='runs/ade20k/fcn/resnet50/', start_epoch=0, syncbn=True, test_batch_size=16, train_split='train', weight_decay=0.0001, workers=48)
Namespace(aux=True, aux_weight=0.5, backbone='resnet50', base_size=520, batch_size=16, checkname='deeplab_res200_ade', crop_size=480, ctx=[gpu(0), gpu(1), gpu(2), gpu(3), gpu(4), gpu(5), gpu(6), gpu(7)], dataset='ade20k', dtype='float32', epochs=180, eval=False, kvstore='device', log_interval=20, logging_file='train.log', lr=0.01, mode=None, model='fcn', model_zoo='deeplab_resnest200_ade', momentum=0.9, ngpus=8, no_cuda=False, no_val=False, no_wd=False, norm_kwargs={'num_devices': 8}, norm_layer=<class 'mxnet.gluon.contrib.nn.basic_layers.SyncBatchNorm'>, optimizer='sgd', resume=None, save_dir='runs/ade20k/fcn/resnet50/', start_epoch=0, syncbn=True, test_batch_size=16, train_split='train', weight_decay=0.0001, workers=48)
Namespace(aux=True, aux_weight=0.5, backbone='resnet50', base_size=520, batch_size=16, checkname='deeplab_res200_ade', crop_size=480, ctx=[gpu(0), gpu(1), gpu(2), gpu(3), gpu(4), gpu(5), gpu(6), gpu(7)], dataset='ade20k', dtype='float32', epochs=180, eval=False, kvstore='device', log_interval=20, logging_file='train.log', lr=0.01, mode=None, model='fcn', model_zoo='deeplab_resnest200_ade', momentum=0.9, ngpus=8, no_cuda=False, no_val=False, no_wd=False, norm_kwargs={'num_devices': 8}, norm_layer=<class 'mxnet.gluon.contrib.nn.basic_layers.SyncBatchNorm'>, optimizer='sgd', resume=None, save_dir='runs/ade20k/fcn/resnet50/', start_epoch=0, syncbn=True, test_batch_size=16, train_split='train', weight_decay=0.0001, workers=48)
Model file not found. Downloading.
Namespace(aux=True, aux_weight=0.5, backbone='resnet50', base_size=520, batch_size=16, checkname='deeplab_res200_ade', crop_size=480, ctx=[gpu(0), gpu(1), gpu(2), gpu(3), gpu(4), gpu(5), gpu(6), gpu(7)], dataset='ade20k', dtype='float32', epochs=180, eval=False, kvstore='device', log_interval=20, logging_file='train.log', lr=0.01, mode=None, model='fcn', model_zoo='deeplab_resnest200_ade', momentum=0.9, ngpus=8, no_cuda=False, no_val=False, no_wd=False, norm_kwargs={'num_devices': 8}, norm_layer=<class 'mxnet.gluon.contrib.nn.basic_layers.SyncBatchNorm'>, optimizer='sgd', resume=None, save_dir='runs/ade20k/fcn/resnet50/', start_epoch=0, syncbn=True, test_batch_size=16, train_split='train', weight_decay=0.0001, workers=48)
DeepLabV3(
  (conv1): HybridSequential(
    (0): Conv2D(3 -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_syncbatchnorm0_', in_channels=64)
    (2): Activation(relu)
    (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (4): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_syncbatchnorm1_', in_channels=64)
    (5): Activation(relu)
    (6): Conv2D(64 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_syncbatchnorm2_', in_channels=128)
  (relu): Activation(relu)
  (maxpool): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)
  (layer1): HybridSequential(
    (0): Bottleneck(
      (conv1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm0_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(32 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm1_', in_channels=128)
        (relu): Activation(relu)
        (fc1): Conv2D(64 -> 32, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm2_', in_channels=32)
        (relu1): Activation(relu)
        (fc2): Conv2D(32 -> 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm3_', in_channels=256)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(128 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down1_syncbatchnorm0_', in_channels=256)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm4_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(32 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm5_', in_channels=128)
        (relu): Activation(relu)
        (fc1): Conv2D(64 -> 32, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm6_', in_channels=32)
        (relu1): Activation(relu)
        (fc2): Conv2D(32 -> 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm7_', in_channels=256)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm8_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(32 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm9_', in_channels=128)
        (relu): Activation(relu)
        (fc1): Conv2D(64 -> 32, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm10_', in_channels=32)
        (relu1): Activation(relu)
        (fc2): Conv2D(32 -> 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm11_', in_channels=256)
      (relu3): Activation(relu)
    )
  )
  (layer2): HybridSequential(
    (0): Bottleneck(
      (conv1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm0_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm1_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm2_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm3_', in_channels=512)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down2_syncbatchnorm0_', in_channels=512)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm4_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm5_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm6_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm7_', in_channels=512)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm8_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm9_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm10_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm11_', in_channels=512)
      (relu3): Activation(relu)
    )
    (3): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm12_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm13_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm14_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm15_', in_channels=512)
      (relu3): Activation(relu)
    )
    (4): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm16_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm17_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm18_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm19_', in_channels=512)
      (relu3): Activation(relu)
    )
    (5): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm20_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm21_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm22_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm23_', in_channels=512)
      (relu3): Activation(relu)
    )
    (6): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm24_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm25_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm26_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm27_', in_channels=512)
      (relu3): Activation(relu)
    )
    (7): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm28_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm29_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm30_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm31_', in_channels=512)
      (relu3): Activation(relu)
    )
    (8): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm32_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm33_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm34_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm35_', in_channels=512)
      (relu3): Activation(relu)
    )
    (9): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm36_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm37_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm38_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm39_', in_channels=512)
      (relu3): Activation(relu)
    )
    (10): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm40_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm41_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm42_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm43_', in_channels=512)
      (relu3): Activation(relu)
    )
    (11): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm44_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm45_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm46_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm47_', in_channels=512)
      (relu3): Activation(relu)
    )
    (12): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm48_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm49_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm50_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm51_', in_channels=512)
      (relu3): Activation(relu)
    )
    (13): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm52_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm53_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm54_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm55_', in_channels=512)
      (relu3): Activation(relu)
    )
    (14): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm56_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm57_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm58_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm59_', in_channels=512)
      (relu3): Activation(relu)
    )
    (15): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm60_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm61_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm62_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm63_', in_channels=512)
      (relu3): Activation(relu)
    )
    (16): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm64_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm65_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm66_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm67_', in_channels=512)
      (relu3): Activation(relu)
    )
    (17): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm68_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm69_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm70_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm71_', in_channels=512)
      (relu3): Activation(relu)
    )
    (18): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm72_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm73_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm74_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm75_', in_channels=512)
      (relu3): Activation(relu)
    )
    (19): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm76_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm77_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm78_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm79_', in_channels=512)
      (relu3): Activation(relu)
    )
    (20): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm80_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm81_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm82_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm83_', in_channels=512)
      (relu3): Activation(relu)
    )
    (21): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm84_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm85_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm86_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm87_', in_channels=512)
      (relu3): Activation(relu)
    )
    (22): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm88_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm89_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm90_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm91_', in_channels=512)
      (relu3): Activation(relu)
    )
    (23): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm92_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm93_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm94_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm95_', in_channels=512)
      (relu3): Activation(relu)
    )
  )
  (layer3): HybridSequential(
    (0): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm0_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm1_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm2_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm3_', in_channels=1024)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(512 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down3_syncbatchnorm0_', in_channels=1024)
      )
    )
    (1): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm4_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm5_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm6_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm7_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm8_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm9_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm10_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm11_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (3): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm12_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm13_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm14_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm15_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (4): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm16_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm17_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm18_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm19_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (5): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm20_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm21_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm22_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm23_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (6): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm24_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm25_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm26_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm27_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (7): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm28_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm29_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm30_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm31_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (8): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm32_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm33_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm34_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm35_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (9): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm36_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm37_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm38_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm39_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (10): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm40_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm41_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm42_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm43_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (11): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm44_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm45_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm46_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm47_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (12): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm48_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm49_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm50_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm51_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (13): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm52_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm53_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm54_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm55_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (14): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm56_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm57_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm58_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm59_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (15): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm60_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm61_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm62_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm63_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (16): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm64_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm65_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm66_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm67_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (17): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm68_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm69_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm70_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm71_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (18): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm72_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm73_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm74_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm75_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (19): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm76_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm77_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm78_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm79_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (20): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm80_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm81_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm82_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm83_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (21): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm84_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm85_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm86_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm87_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (22): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm88_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm89_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm90_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm91_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (23): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm92_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm93_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm94_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm95_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (24): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm96_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm97_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm98_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm99_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (25): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm100_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm101_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm102_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm103_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (26): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm104_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm105_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm106_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm107_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (27): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm108_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm109_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm110_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm111_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (28): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm112_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm113_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm114_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm115_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (29): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm116_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm117_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm118_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm119_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (30): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm120_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm121_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm122_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm123_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (31): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm124_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm125_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm126_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm127_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (32): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm128_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm129_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm130_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm131_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (33): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm132_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm133_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm134_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm135_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (34): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm136_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm137_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm138_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm139_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (35): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm140_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm141_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm142_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm143_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
  )
  (layer4): HybridSequential(
    (0): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm0_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm1_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm2_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm3_', in_channels=2048)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(1024 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down4_syncbatchnorm0_', in_channels=2048)
      )
    )
    (1): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm4_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm5_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm6_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm7_', in_channels=2048)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm8_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm9_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm10_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm11_', in_channels=2048)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
  )
  (head): _DeepLabHead(
    (aspp): _ASPP(
      (concurent): HybridConcurrent(
        (0): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential0_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (1): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential1_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (2): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential2_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (3): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential3_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (4): _AsppPooling(
          (gap): HybridSequential(
            (0): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=True, pool_type=avg, layout=NCHW)
            (1): Conv2D(2048 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential4_syncbatchnorm0_', in_channels=256)
            (3): Activation(relu)
          )
        )
      )
      (project): HybridSequential(
        (0): Conv2D(1280 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential5_syncbatchnorm0_', in_channels=256)
        (2): Activation(relu)
        (3): Dropout(p = 0.5, axes=())
      )
    )
    (block): HybridSequential(
      (0): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_syncbatchnorm0_', in_channels=256)
      (2): Activation(relu)
      (3): Dropout(p = 0.1, axes=())
      (4): Conv2D(256 -> 150, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (auxlayer): _FCNHead(
    (block): HybridSequential(
      (0): Conv2D(1024 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__fcnhead0_hybridsequential0_syncbatchnorm0_', in_channels=256)
      (2): Activation(relu)
      (3): Dropout(p = 0.1, axes=())
      (4): Conv2D(256 -> 150, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
Starting Epoch: 0
Total Epochs: 180
Epoch 0 iteration 0020/1263: training loss nan
Epoch 0 iteration 0040/1263: training loss nan
Namespace(aux=True, aux_weight=0.5, backbone='resnet50', base_size=520, batch_size=16, checkname='deeplab_res200_ade', crop_size=480, ctx=[gpu(0), gpu(1), gpu(2), gpu(3), gpu(4), gpu(5), gpu(6), gpu(7)], dataset='ade20k', dtype='float32', epochs=180, eval=False, kvstore='device', log_interval=20, logging_file='train.log', lr=0.005, mode=None, model='fcn', model_zoo='deeplab_resnest200_ade', momentum=0.9, ngpus=8, no_cuda=False, no_val=False, no_wd=False, norm_kwargs={'num_devices': 8}, norm_layer=<class 'mxnet.gluon.contrib.nn.basic_layers.SyncBatchNorm'>, optimizer='sgd', resume=None, save_dir='runs/ade20k/fcn/resnet50/', start_epoch=0, syncbn=True, test_batch_size=16, train_split='train', weight_decay=0.0001, workers=48)
DeepLabV3(
  (conv1): HybridSequential(
    (0): Conv2D(3 -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_syncbatchnorm0_', in_channels=64)
    (2): Activation(relu)
    (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (4): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_syncbatchnorm1_', in_channels=64)
    (5): Activation(relu)
    (6): Conv2D(64 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_syncbatchnorm2_', in_channels=128)
  (relu): Activation(relu)
  (maxpool): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)
  (layer1): HybridSequential(
    (0): Bottleneck(
      (conv1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm0_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(32 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm1_', in_channels=128)
        (relu): Activation(relu)
        (fc1): Conv2D(64 -> 32, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm2_', in_channels=32)
        (relu1): Activation(relu)
        (fc2): Conv2D(32 -> 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm3_', in_channels=256)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(128 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down1_syncbatchnorm0_', in_channels=256)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm4_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(32 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm5_', in_channels=128)
        (relu): Activation(relu)
        (fc1): Conv2D(64 -> 32, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm6_', in_channels=32)
        (relu1): Activation(relu)
        (fc2): Conv2D(32 -> 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm7_', in_channels=256)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm8_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(32 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm9_', in_channels=128)
        (relu): Activation(relu)
        (fc1): Conv2D(64 -> 32, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm10_', in_channels=32)
        (relu1): Activation(relu)
        (fc2): Conv2D(32 -> 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm11_', in_channels=256)
      (relu3): Activation(relu)
    )
  )
  (layer2): HybridSequential(
    (0): Bottleneck(
      (conv1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm0_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm1_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm2_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm3_', in_channels=512)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down2_syncbatchnorm0_', in_channels=512)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm4_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm5_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm6_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm7_', in_channels=512)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm8_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm9_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm10_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm11_', in_channels=512)
      (relu3): Activation(relu)
    )
    (3): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm12_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm13_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm14_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm15_', in_channels=512)
      (relu3): Activation(relu)
    )
    (4): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm16_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm17_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm18_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm19_', in_channels=512)
      (relu3): Activation(relu)
    )
    (5): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm20_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm21_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm22_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm23_', in_channels=512)
      (relu3): Activation(relu)
    )
    (6): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm24_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm25_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm26_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm27_', in_channels=512)
      (relu3): Activation(relu)
    )
    (7): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm28_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm29_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm30_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm31_', in_channels=512)
      (relu3): Activation(relu)
    )
    (8): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm32_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm33_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm34_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm35_', in_channels=512)
      (relu3): Activation(relu)
    )
    (9): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm36_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm37_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm38_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm39_', in_channels=512)
      (relu3): Activation(relu)
    )
    (10): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm40_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm41_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm42_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm43_', in_channels=512)
      (relu3): Activation(relu)
    )
    (11): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm44_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm45_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm46_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm47_', in_channels=512)
      (relu3): Activation(relu)
    )
    (12): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm48_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm49_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm50_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm51_', in_channels=512)
      (relu3): Activation(relu)
    )
    (13): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm52_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm53_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm54_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm55_', in_channels=512)
      (relu3): Activation(relu)
    )
    (14): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm56_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm57_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm58_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm59_', in_channels=512)
      (relu3): Activation(relu)
    )
    (15): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm60_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm61_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm62_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm63_', in_channels=512)
      (relu3): Activation(relu)
    )
    (16): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm64_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm65_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm66_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm67_', in_channels=512)
      (relu3): Activation(relu)
    )
    (17): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm68_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm69_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm70_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm71_', in_channels=512)
      (relu3): Activation(relu)
    )
    (18): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm72_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm73_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm74_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm75_', in_channels=512)
      (relu3): Activation(relu)
    )
    (19): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm76_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm77_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm78_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm79_', in_channels=512)
      (relu3): Activation(relu)
    )
    (20): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm80_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm81_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm82_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm83_', in_channels=512)
      (relu3): Activation(relu)
    )
    (21): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm84_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm85_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm86_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm87_', in_channels=512)
      (relu3): Activation(relu)
    )
    (22): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm88_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm89_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm90_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm91_', in_channels=512)
      (relu3): Activation(relu)
    )
    (23): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm92_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm93_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm94_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm95_', in_channels=512)
      (relu3): Activation(relu)
    )
  )
  (layer3): HybridSequential(
    (0): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm0_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm1_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm2_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm3_', in_channels=1024)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(512 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down3_syncbatchnorm0_', in_channels=1024)
      )
    )
    (1): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm4_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm5_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm6_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm7_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm8_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm9_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm10_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm11_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (3): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm12_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm13_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm14_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm15_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (4): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm16_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm17_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm18_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm19_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (5): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm20_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm21_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm22_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm23_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (6): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm24_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm25_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm26_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm27_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (7): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm28_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm29_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm30_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm31_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (8): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm32_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm33_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm34_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm35_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (9): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm36_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm37_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm38_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm39_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (10): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm40_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm41_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm42_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm43_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (11): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm44_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm45_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm46_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm47_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (12): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm48_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm49_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm50_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm51_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (13): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm52_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm53_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm54_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm55_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (14): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm56_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm57_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm58_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm59_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (15): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm60_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm61_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm62_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm63_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (16): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm64_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm65_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm66_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm67_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (17): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm68_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm69_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm70_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm71_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (18): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm72_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm73_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm74_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm75_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (19): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm76_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm77_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm78_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm79_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (20): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm80_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm81_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm82_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm83_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (21): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm84_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm85_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm86_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm87_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (22): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm88_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm89_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm90_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm91_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (23): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm92_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm93_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm94_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm95_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (24): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm96_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm97_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm98_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm99_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (25): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm100_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm101_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm102_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm103_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (26): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm104_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm105_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm106_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm107_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (27): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm108_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm109_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm110_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm111_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (28): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm112_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm113_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm114_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm115_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (29): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm116_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm117_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm118_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm119_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (30): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm120_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm121_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm122_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm123_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (31): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm124_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm125_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm126_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm127_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (32): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm128_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm129_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm130_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm131_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (33): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm132_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm133_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm134_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm135_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (34): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm136_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm137_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm138_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm139_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (35): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm140_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm141_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm142_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm143_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
  )
  (layer4): HybridSequential(
    (0): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm0_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm1_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm2_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm3_', in_channels=2048)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(1024 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down4_syncbatchnorm0_', in_channels=2048)
      )
    )
    (1): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm4_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm5_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm6_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm7_', in_channels=2048)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm8_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm9_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm10_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm11_', in_channels=2048)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
  )
  (head): _DeepLabHead(
    (aspp): _ASPP(
      (concurent): HybridConcurrent(
        (0): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential0_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (1): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential1_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (2): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential2_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (3): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential3_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (4): _AsppPooling(
          (gap): HybridSequential(
            (0): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=True, pool_type=avg, layout=NCHW)
            (1): Conv2D(2048 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential4_syncbatchnorm0_', in_channels=256)
            (3): Activation(relu)
          )
        )
      )
      (project): HybridSequential(
        (0): Conv2D(1280 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential5_syncbatchnorm0_', in_channels=256)
        (2): Activation(relu)
        (3): Dropout(p = 0.5, axes=())
      )
    )
    (block): HybridSequential(
      (0): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_syncbatchnorm0_', in_channels=256)
      (2): Activation(relu)
      (3): Dropout(p = 0.1, axes=())
      (4): Conv2D(256 -> 150, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (auxlayer): _FCNHead(
    (block): HybridSequential(
      (0): Conv2D(1024 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__fcnhead0_hybridsequential0_syncbatchnorm0_', in_channels=256)
      (2): Activation(relu)
      (3): Dropout(p = 0.1, axes=())
      (4): Conv2D(256 -> 150, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
Starting Epoch: 0
Total Epochs: 180
Namespace(aux=True, aux_weight=0.5, backbone='resnet50', base_size=520, batch_size=16, checkname='deeplab_res200_ade', crop_size=480, ctx=[gpu(0), gpu(1), gpu(2), gpu(3), gpu(4), gpu(5), gpu(6), gpu(7)], dataset='ade20k', dtype='float32', epochs=180, eval=False, kvstore='device', log_interval=20, logging_file='train.log', lr=0.01, mode=None, model='fcn', model_zoo='deeplab_resnest200_ade', momentum=0.9, ngpus=8, no_cuda=False, no_val=False, no_wd=False, norm_kwargs={'num_devices': 8}, norm_layer=<class 'mxnet.gluon.contrib.nn.basic_layers.SyncBatchNorm'>, optimizer='sgd', resume=None, save_dir='runs/ade20k/fcn/resnet50/', start_epoch=0, syncbn=True, test_batch_size=16, train_split='train', warmup_epochs=5, weight_decay=0.0001, workers=48)
DeepLabV3(
  (conv1): HybridSequential(
    (0): Conv2D(3 -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_syncbatchnorm0_', in_channels=64)
    (2): Activation(relu)
    (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (4): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_syncbatchnorm1_', in_channels=64)
    (5): Activation(relu)
    (6): Conv2D(64 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_syncbatchnorm2_', in_channels=128)
  (relu): Activation(relu)
  (maxpool): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)
  (layer1): HybridSequential(
    (0): Bottleneck(
      (conv1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm0_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(32 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm1_', in_channels=128)
        (relu): Activation(relu)
        (fc1): Conv2D(64 -> 32, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm2_', in_channels=32)
        (relu1): Activation(relu)
        (fc2): Conv2D(32 -> 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm3_', in_channels=256)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(128 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down1_syncbatchnorm0_', in_channels=256)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm4_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(32 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm5_', in_channels=128)
        (relu): Activation(relu)
        (fc1): Conv2D(64 -> 32, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm6_', in_channels=32)
        (relu1): Activation(relu)
        (fc2): Conv2D(32 -> 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm7_', in_channels=256)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm8_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(32 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm9_', in_channels=128)
        (relu): Activation(relu)
        (fc1): Conv2D(64 -> 32, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm10_', in_channels=32)
        (relu1): Activation(relu)
        (fc2): Conv2D(32 -> 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm11_', in_channels=256)
      (relu3): Activation(relu)
    )
  )
  (layer2): HybridSequential(
    (0): Bottleneck(
      (conv1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm0_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm1_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm2_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm3_', in_channels=512)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down2_syncbatchnorm0_', in_channels=512)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm4_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm5_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm6_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm7_', in_channels=512)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm8_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm9_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm10_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm11_', in_channels=512)
      (relu3): Activation(relu)
    )
    (3): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm12_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm13_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm14_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm15_', in_channels=512)
      (relu3): Activation(relu)
    )
    (4): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm16_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm17_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm18_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm19_', in_channels=512)
      (relu3): Activation(relu)
    )
    (5): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm20_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm21_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm22_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm23_', in_channels=512)
      (relu3): Activation(relu)
    )
    (6): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm24_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm25_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm26_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm27_', in_channels=512)
      (relu3): Activation(relu)
    )
    (7): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm28_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm29_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm30_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm31_', in_channels=512)
      (relu3): Activation(relu)
    )
    (8): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm32_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm33_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm34_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm35_', in_channels=512)
      (relu3): Activation(relu)
    )
    (9): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm36_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm37_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm38_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm39_', in_channels=512)
      (relu3): Activation(relu)
    )
    (10): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm40_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm41_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm42_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm43_', in_channels=512)
      (relu3): Activation(relu)
    )
    (11): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm44_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm45_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm46_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm47_', in_channels=512)
      (relu3): Activation(relu)
    )
    (12): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm48_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm49_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm50_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm51_', in_channels=512)
      (relu3): Activation(relu)
    )
    (13): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm52_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm53_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm54_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm55_', in_channels=512)
      (relu3): Activation(relu)
    )
    (14): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm56_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm57_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm58_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm59_', in_channels=512)
      (relu3): Activation(relu)
    )
    (15): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm60_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm61_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm62_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm63_', in_channels=512)
      (relu3): Activation(relu)
    )
    (16): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm64_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm65_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm66_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm67_', in_channels=512)
      (relu3): Activation(relu)
    )
    (17): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm68_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm69_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm70_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm71_', in_channels=512)
      (relu3): Activation(relu)
    )
    (18): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm72_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm73_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm74_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm75_', in_channels=512)
      (relu3): Activation(relu)
    )
    (19): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm76_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm77_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm78_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm79_', in_channels=512)
      (relu3): Activation(relu)
    )
    (20): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm80_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm81_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm82_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm83_', in_channels=512)
      (relu3): Activation(relu)
    )
    (21): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm84_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm85_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm86_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm87_', in_channels=512)
      (relu3): Activation(relu)
    )
    (22): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm88_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm89_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm90_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm91_', in_channels=512)
      (relu3): Activation(relu)
    )
    (23): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm92_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm93_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm94_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm95_', in_channels=512)
      (relu3): Activation(relu)
    )
  )
  (layer3): HybridSequential(
    (0): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm0_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm1_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm2_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm3_', in_channels=1024)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(512 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down3_syncbatchnorm0_', in_channels=1024)
      )
    )
    (1): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm4_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm5_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm6_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm7_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm8_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm9_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm10_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm11_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (3): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm12_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm13_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm14_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm15_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (4): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm16_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm17_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm18_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm19_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (5): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm20_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm21_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm22_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm23_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (6): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm24_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm25_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm26_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm27_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (7): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm28_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm29_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm30_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm31_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (8): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm32_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm33_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm34_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm35_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (9): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm36_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm37_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm38_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm39_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (10): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm40_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm41_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm42_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm43_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (11): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm44_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm45_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm46_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm47_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (12): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm48_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm49_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm50_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm51_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (13): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm52_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm53_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm54_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm55_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (14): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm56_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm57_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm58_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm59_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (15): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm60_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm61_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm62_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm63_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (16): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm64_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm65_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm66_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm67_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (17): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm68_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm69_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm70_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm71_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (18): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm72_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm73_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm74_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm75_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (19): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm76_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm77_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm78_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm79_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (20): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm80_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm81_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm82_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm83_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (21): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm84_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm85_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm86_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm87_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (22): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm88_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm89_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm90_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm91_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (23): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm92_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm93_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm94_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm95_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (24): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm96_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm97_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm98_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm99_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (25): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm100_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm101_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm102_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm103_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (26): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm104_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm105_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm106_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm107_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (27): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm108_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm109_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm110_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm111_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (28): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm112_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm113_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm114_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm115_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (29): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm116_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm117_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm118_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm119_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (30): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm120_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm121_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm122_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm123_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (31): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm124_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm125_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm126_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm127_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (32): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm128_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm129_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm130_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm131_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (33): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm132_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm133_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm134_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm135_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (34): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm136_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm137_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm138_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm139_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (35): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm140_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm141_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm142_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm143_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
  )
  (layer4): HybridSequential(
    (0): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm0_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm1_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm2_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm3_', in_channels=2048)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(1024 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down4_syncbatchnorm0_', in_channels=2048)
      )
    )
    (1): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm4_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm5_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm6_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm7_', in_channels=2048)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm8_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm9_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm10_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm11_', in_channels=2048)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
  )
  (head): _DeepLabHead(
    (aspp): _ASPP(
      (concurent): HybridConcurrent(
        (0): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential0_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (1): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential1_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (2): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential2_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (3): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential3_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (4): _AsppPooling(
          (gap): HybridSequential(
            (0): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=True, pool_type=avg, layout=NCHW)
            (1): Conv2D(2048 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential4_syncbatchnorm0_', in_channels=256)
            (3): Activation(relu)
          )
        )
      )
      (project): HybridSequential(
        (0): Conv2D(1280 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential5_syncbatchnorm0_', in_channels=256)
        (2): Activation(relu)
        (3): Dropout(p = 0.5, axes=())
      )
    )
    (block): HybridSequential(
      (0): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_syncbatchnorm0_', in_channels=256)
      (2): Activation(relu)
      (3): Dropout(p = 0.1, axes=())
      (4): Conv2D(256 -> 150, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (auxlayer): _FCNHead(
    (block): HybridSequential(
      (0): Conv2D(1024 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__fcnhead0_hybridsequential0_syncbatchnorm0_', in_channels=256)
      (2): Activation(relu)
      (3): Dropout(p = 0.1, axes=())
      (4): Conv2D(256 -> 150, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
Starting Epoch: 0
Total Epochs: 180
Epoch 0 iteration 0020/1263: training loss 5.900
Epoch 0 iteration 0040/1263: training loss 5.712
Epoch 0 iteration 0060/1263: training loss 5.638
Epoch 0 iteration 0080/1263: training loss 5.572
Epoch 0 iteration 0100/1263: training loss 5.524
Epoch 0 iteration 0120/1263: training loss 5.405
Epoch 0 iteration 0140/1263: training loss 5.311
Epoch 0 iteration 0160/1263: training loss nan
Namespace(aux=True, aux_weight=0.5, backbone='resnet50', base_size=520, batch_size=16, checkname='deeplab_res200_ade', crop_size=480, ctx=[gpu(0), gpu(1), gpu(2), gpu(3), gpu(4), gpu(5), gpu(6), gpu(7)], dataset='ade20k', dtype='float32', epochs=180, eval=False, kvstore='device', log_interval=20, logging_file='train.log', lr=0.01, mode=None, model='fcn', model_zoo='deeplab_resnest200_ade', momentum=0.9, ngpus=8, no_cuda=False, no_val=False, no_wd=False, norm_kwargs={'num_devices': 8}, norm_layer=<class 'mxnet.gluon.contrib.nn.basic_layers.SyncBatchNorm'>, optimizer='sgd', resume=None, save_dir='runs/ade20k/fcn/resnet50/', start_epoch=0, syncbn=True, test_batch_size=16, train_split='train', warmup_epochs=5, weight_decay=0.0001, workers=48)
DeepLabV3(
  (conv1): HybridSequential(
    (0): Conv2D(3 -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_syncbatchnorm0_', in_channels=64)
    (2): Activation(relu)
    (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (4): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_syncbatchnorm1_', in_channels=64)
    (5): Activation(relu)
    (6): Conv2D(64 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_syncbatchnorm2_', in_channels=128)
  (relu): Activation(relu)
  (maxpool): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)
  (layer1): HybridSequential(
    (0): Bottleneck(
      (conv1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm0_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(32 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm1_', in_channels=128)
        (relu): Activation(relu)
        (fc1): Conv2D(64 -> 32, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm2_', in_channels=32)
        (relu1): Activation(relu)
        (fc2): Conv2D(32 -> 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm3_', in_channels=256)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(128 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down1_syncbatchnorm0_', in_channels=256)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm4_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(32 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm5_', in_channels=128)
        (relu): Activation(relu)
        (fc1): Conv2D(64 -> 32, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm6_', in_channels=32)
        (relu1): Activation(relu)
        (fc2): Conv2D(32 -> 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm7_', in_channels=256)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm8_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(32 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm9_', in_channels=128)
        (relu): Activation(relu)
        (fc1): Conv2D(64 -> 32, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm10_', in_channels=32)
        (relu1): Activation(relu)
        (fc2): Conv2D(32 -> 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm11_', in_channels=256)
      (relu3): Activation(relu)
    )
  )
  (layer2): HybridSequential(
    (0): Bottleneck(
      (conv1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm0_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm1_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm2_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm3_', in_channels=512)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down2_syncbatchnorm0_', in_channels=512)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm4_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm5_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm6_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm7_', in_channels=512)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm8_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm9_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm10_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm11_', in_channels=512)
      (relu3): Activation(relu)
    )
    (3): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm12_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm13_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm14_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm15_', in_channels=512)
      (relu3): Activation(relu)
    )
    (4): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm16_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm17_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm18_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm19_', in_channels=512)
      (relu3): Activation(relu)
    )
    (5): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm20_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm21_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm22_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm23_', in_channels=512)
      (relu3): Activation(relu)
    )
    (6): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm24_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm25_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm26_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm27_', in_channels=512)
      (relu3): Activation(relu)
    )
    (7): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm28_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm29_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm30_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm31_', in_channels=512)
      (relu3): Activation(relu)
    )
    (8): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm32_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm33_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm34_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm35_', in_channels=512)
      (relu3): Activation(relu)
    )
    (9): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm36_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm37_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm38_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm39_', in_channels=512)
      (relu3): Activation(relu)
    )
    (10): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm40_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm41_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm42_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm43_', in_channels=512)
      (relu3): Activation(relu)
    )
    (11): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm44_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm45_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm46_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm47_', in_channels=512)
      (relu3): Activation(relu)
    )
    (12): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm48_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm49_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm50_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm51_', in_channels=512)
      (relu3): Activation(relu)
    )
    (13): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm52_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm53_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm54_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm55_', in_channels=512)
      (relu3): Activation(relu)
    )
    (14): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm56_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm57_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm58_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm59_', in_channels=512)
      (relu3): Activation(relu)
    )
    (15): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm60_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm61_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm62_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm63_', in_channels=512)
      (relu3): Activation(relu)
    )
    (16): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm64_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm65_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm66_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm67_', in_channels=512)
      (relu3): Activation(relu)
    )
    (17): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm68_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm69_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm70_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm71_', in_channels=512)
      (relu3): Activation(relu)
    )
    (18): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm72_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm73_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm74_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm75_', in_channels=512)
      (relu3): Activation(relu)
    )
    (19): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm76_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm77_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm78_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm79_', in_channels=512)
      (relu3): Activation(relu)
    )
    (20): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm80_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm81_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm82_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm83_', in_channels=512)
      (relu3): Activation(relu)
    )
    (21): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm84_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm85_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm86_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm87_', in_channels=512)
      (relu3): Activation(relu)
    )
    (22): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm88_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm89_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm90_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm91_', in_channels=512)
      (relu3): Activation(relu)
    )
    (23): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm92_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm93_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm94_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm95_', in_channels=512)
      (relu3): Activation(relu)
    )
  )
  (layer3): HybridSequential(
    (0): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm0_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm1_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm2_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm3_', in_channels=1024)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(512 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down3_syncbatchnorm0_', in_channels=1024)
      )
    )
    (1): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm4_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm5_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm6_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm7_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm8_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm9_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm10_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm11_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (3): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm12_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm13_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm14_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm15_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (4): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm16_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm17_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm18_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm19_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (5): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm20_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm21_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm22_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm23_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (6): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm24_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm25_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm26_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm27_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (7): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm28_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm29_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm30_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm31_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (8): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm32_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm33_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm34_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm35_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (9): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm36_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm37_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm38_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm39_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (10): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm40_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm41_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm42_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm43_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (11): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm44_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm45_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm46_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm47_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (12): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm48_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm49_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm50_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm51_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (13): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm52_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm53_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm54_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm55_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (14): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm56_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm57_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm58_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm59_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (15): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm60_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm61_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm62_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm63_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (16): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm64_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm65_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm66_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm67_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (17): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm68_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm69_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm70_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm71_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (18): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm72_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm73_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm74_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm75_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (19): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm76_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm77_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm78_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm79_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (20): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm80_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm81_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm82_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm83_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (21): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm84_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm85_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm86_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm87_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (22): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm88_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm89_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm90_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm91_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (23): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm92_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm93_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm94_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm95_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (24): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm96_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm97_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm98_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm99_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (25): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm100_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm101_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm102_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm103_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (26): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm104_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm105_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm106_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm107_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (27): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm108_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm109_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm110_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm111_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (28): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm112_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm113_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm114_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm115_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (29): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm116_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm117_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm118_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm119_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (30): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm120_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm121_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm122_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm123_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (31): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm124_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm125_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm126_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm127_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (32): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm128_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm129_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm130_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm131_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (33): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm132_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm133_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm134_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm135_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (34): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm136_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm137_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm138_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm139_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (35): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm140_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm141_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm142_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm143_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
  )
  (layer4): HybridSequential(
    (0): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm0_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm1_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm2_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm3_', in_channels=2048)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(1024 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down4_syncbatchnorm0_', in_channels=2048)
      )
    )
    (1): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm4_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm5_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm6_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm7_', in_channels=2048)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm8_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm9_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm10_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm11_', in_channels=2048)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
    )
  )
  (head): _DeepLabHead(
    (aspp): _ASPP(
      (concurent): HybridConcurrent(
        (0): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential0_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (1): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential1_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (2): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential2_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (3): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential3_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (4): _AsppPooling(
          (gap): HybridSequential(
            (0): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=True, pool_type=avg, layout=NCHW)
            (1): Conv2D(2048 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential4_syncbatchnorm0_', in_channels=256)
            (3): Activation(relu)
          )
        )
      )
      (project): HybridSequential(
        (0): Conv2D(1280 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential5_syncbatchnorm0_', in_channels=256)
        (2): Activation(relu)
        (3): Dropout(p = 0.5, axes=())
      )
    )
    (block): HybridSequential(
      (0): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_syncbatchnorm0_', in_channels=256)
      (2): Activation(relu)
      (3): Dropout(p = 0.1, axes=())
      (4): Conv2D(256 -> 150, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (auxlayer): _FCNHead(
    (block): HybridSequential(
      (0): Conv2D(1024 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__fcnhead0_hybridsequential0_syncbatchnorm0_', in_channels=256)
      (2): Activation(relu)
      (3): Dropout(p = 0.1, axes=())
      (4): Conv2D(256 -> 150, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
Starting Epoch: 0
Total Epochs: 180
Epoch 0 iteration 0020/1263: training loss 5.862
Epoch 0 iteration 0040/1263: training loss 5.723
Epoch 0 iteration 0060/1263: training loss 5.703
Epoch 0 iteration 0080/1263: training loss 5.609
Epoch 0 iteration 0100/1263: training loss 5.516
Epoch 0 iteration 0120/1263: training loss 5.471
Epoch 0 iteration 0140/1263: training loss 5.378
Epoch 0 iteration 0160/1263: training loss 5.279
Epoch 0 iteration 0180/1263: training loss 5.193
Epoch 0 iteration 0200/1263: training loss 5.105
Epoch 0 iteration 0220/1263: training loss 5.017
Epoch 0 iteration 0240/1263: training loss 4.924
Epoch 0 iteration 0260/1263: training loss 4.848
Epoch 0 iteration 0280/1263: training loss 4.771
Epoch 0 iteration 0300/1263: training loss 4.692
Epoch 0 iteration 0320/1263: training loss 4.632
Epoch 0 iteration 0340/1263: training loss 4.570
Epoch 0 iteration 0360/1263: training loss 4.513
Epoch 0 iteration 0380/1263: training loss 4.454
Epoch 0 iteration 0400/1263: training loss 4.394
Epoch 0 iteration 0420/1263: training loss 4.326
Epoch 0 iteration 0440/1263: training loss 4.261
Epoch 0 iteration 0460/1263: training loss 4.207
Epoch 0 iteration 0480/1263: training loss 4.144
Epoch 0 iteration 0500/1263: training loss 4.088
Epoch 0 iteration 0520/1263: training loss 4.036
Epoch 0 iteration 0540/1263: training loss 3.981
Epoch 0 iteration 0560/1263: training loss 3.929
Epoch 0 iteration 0580/1263: training loss 3.880
Epoch 0 iteration 0600/1263: training loss 3.830
Epoch 0 iteration 0620/1263: training loss 3.785
Epoch 0 iteration 0640/1263: training loss 3.741
Epoch 0 iteration 0660/1263: training loss 3.696
Epoch 0 iteration 0680/1263: training loss 3.655
Epoch 0 iteration 0700/1263: training loss 3.616
Epoch 0 iteration 0720/1263: training loss 3.578
Epoch 0 iteration 0740/1263: training loss 3.535
Epoch 0 iteration 0760/1263: training loss 3.497
Epoch 0 iteration 0780/1263: training loss 3.460
Epoch 0 iteration 0800/1263: training loss 3.428
Epoch 0 iteration 0820/1263: training loss 3.393
Epoch 0 iteration 0840/1263: training loss 3.358
Epoch 0 iteration 0860/1263: training loss 3.327
Epoch 0 iteration 0880/1263: training loss 3.295
Epoch 0 iteration 0900/1263: training loss 3.265
Epoch 0 iteration 0920/1263: training loss 3.236
Epoch 0 iteration 0940/1263: training loss 3.207
Epoch 0 iteration 0960/1263: training loss 3.179
Epoch 0 iteration 0980/1263: training loss 3.153
Epoch 0 iteration 1000/1263: training loss 3.127
Epoch 0 iteration 1020/1263: training loss 3.104
Epoch 0 iteration 1040/1263: training loss 3.082
Epoch 0 iteration 1060/1263: training loss 3.060
Epoch 0 iteration 1080/1263: training loss 3.036
Epoch 0 iteration 1100/1263: training loss 3.016
Epoch 0 iteration 1120/1263: training loss 2.994
Epoch 0 iteration 1140/1263: training loss 2.971
Epoch 0 iteration 1160/1263: training loss 2.950
Epoch 0 iteration 1180/1263: training loss 2.931
Epoch 0 iteration 1200/1263: training loss 2.913
Epoch 0 iteration 1220/1263: training loss 2.895
Epoch 0 iteration 1240/1263: training loss 2.876
Epoch 0 iteration 1260/1263: training loss 2.859
Epoch 0 validation pixAcc: 0.668, mIoU: 0.100
Epoch 1 iteration 0020/1263: training loss 1.573
Epoch 1 iteration 0040/1263: training loss 1.646
Epoch 1 iteration 0060/1263: training loss 1.673
Epoch 1 iteration 0080/1263: training loss 1.685
Epoch 1 iteration 0100/1263: training loss 1.675
Epoch 1 iteration 0120/1263: training loss 1.648
Epoch 1 iteration 0140/1263: training loss 1.636
Epoch 1 iteration 0160/1263: training loss 1.630
Epoch 1 iteration 0180/1263: training loss 1.633
Epoch 1 iteration 0200/1263: training loss 1.626
Epoch 1 iteration 0220/1263: training loss 1.622
Epoch 1 iteration 0240/1263: training loss 1.617
Epoch 1 iteration 0260/1263: training loss 1.614
Epoch 1 iteration 0280/1263: training loss 1.617
Epoch 1 iteration 0300/1263: training loss 1.619
Epoch 1 iteration 0320/1263: training loss 1.622
Epoch 1 iteration 0340/1263: training loss 1.618
Epoch 1 iteration 0360/1263: training loss 1.626
Epoch 1 iteration 0380/1263: training loss 1.620
Epoch 1 iteration 0400/1263: training loss 1.617
Epoch 1 iteration 0420/1263: training loss 1.617
Epoch 1 iteration 0440/1263: training loss 1.608
Epoch 1 iteration 0460/1263: training loss 1.603
Epoch 1 iteration 0480/1263: training loss 1.599
Epoch 1 iteration 0500/1263: training loss 1.594
Epoch 1 iteration 0520/1263: training loss 1.597
Epoch 1 iteration 0540/1263: training loss 1.597
Epoch 1 iteration 0560/1263: training loss 1.596
Epoch 1 iteration 0580/1263: training loss 1.594
Epoch 1 iteration 0600/1263: training loss 1.589
Epoch 1 iteration 0620/1263: training loss 1.584
Epoch 1 iteration 0640/1263: training loss 1.579
Epoch 1 iteration 0660/1263: training loss 1.574
Epoch 1 iteration 0680/1263: training loss 1.568
Epoch 1 iteration 0700/1263: training loss 1.570
Epoch 1 iteration 0720/1263: training loss 1.569
Epoch 1 iteration 0740/1263: training loss 1.571
Epoch 1 iteration 0760/1263: training loss 1.567
Epoch 1 iteration 0780/1263: training loss 1.571
Epoch 1 iteration 0800/1263: training loss 1.569
Epoch 1 iteration 0820/1263: training loss 1.565
Epoch 1 iteration 0840/1263: training loss 1.563
Epoch 1 iteration 0860/1263: training loss 1.561
Epoch 1 iteration 0880/1263: training loss 1.556
Epoch 1 iteration 0900/1263: training loss 1.554
Epoch 1 iteration 0920/1263: training loss 1.550
Epoch 1 iteration 0940/1263: training loss 1.548
Epoch 1 iteration 0960/1263: training loss 1.545
Epoch 1 iteration 0980/1263: training loss 1.543
Epoch 1 iteration 1000/1263: training loss 1.539
Epoch 1 iteration 1020/1263: training loss 1.536
Epoch 1 iteration 1040/1263: training loss 1.533
Epoch 1 iteration 1060/1263: training loss 1.531
Epoch 1 iteration 1080/1263: training loss 1.530
Epoch 1 iteration 1100/1263: training loss 1.527
Epoch 1 iteration 1120/1263: training loss 1.525
Epoch 1 iteration 1140/1263: training loss 1.524
Epoch 1 iteration 1160/1263: training loss 1.524
Epoch 1 iteration 1180/1263: training loss 1.522
Epoch 1 iteration 1200/1263: training loss 1.520
Epoch 1 iteration 1220/1263: training loss 1.518
Epoch 1 iteration 1240/1263: training loss 1.515
Epoch 1 iteration 1260/1263: training loss 1.512
Epoch 1 validation pixAcc: 0.692, mIoU: 0.205
Epoch 2 iteration 0020/1263: training loss 1.424
Epoch 2 iteration 0040/1263: training loss 1.326
Epoch 2 iteration 0060/1263: training loss 1.302
Epoch 2 iteration 0080/1263: training loss 1.298
Epoch 2 iteration 0100/1263: training loss 1.284
Epoch 2 iteration 0120/1263: training loss 1.279
Epoch 2 iteration 0140/1263: training loss 1.287
Epoch 2 iteration 0160/1263: training loss 1.293
Epoch 2 iteration 0180/1263: training loss 1.302
Epoch 2 iteration 0200/1263: training loss 1.300
Epoch 2 iteration 0220/1263: training loss 1.311
Epoch 2 iteration 0240/1263: training loss 1.320
Epoch 2 iteration 0260/1263: training loss 1.324
Epoch 2 iteration 0280/1263: training loss 1.329
Epoch 2 iteration 0300/1263: training loss 1.334
Epoch 2 iteration 0320/1263: training loss 1.328
Epoch 2 iteration 0340/1263: training loss 1.331
Epoch 2 iteration 0360/1263: training loss 1.333
Epoch 2 iteration 0380/1263: training loss 1.331
Epoch 2 iteration 0400/1263: training loss 1.343
Epoch 2 iteration 0420/1263: training loss 1.342
Epoch 2 iteration 0440/1263: training loss 1.338
Epoch 2 iteration 0460/1263: training loss 1.335
Epoch 2 iteration 0480/1263: training loss 1.330
Epoch 2 iteration 0500/1263: training loss 1.329
Epoch 2 iteration 0520/1263: training loss 1.326
Epoch 2 iteration 0540/1263: training loss 1.330
Epoch 2 iteration 0560/1263: training loss 1.326
Epoch 2 iteration 0580/1263: training loss 1.325
Epoch 2 iteration 0600/1263: training loss 1.326
Epoch 2 iteration 0620/1263: training loss 1.324
Epoch 2 iteration 0640/1263: training loss 1.321
Epoch 2 iteration 0660/1263: training loss 1.321
Epoch 2 iteration 0680/1263: training loss 1.321
Epoch 2 iteration 0700/1263: training loss 1.320
Epoch 2 iteration 0720/1263: training loss 1.321
Epoch 2 iteration 0740/1263: training loss 1.319
Epoch 2 iteration 0760/1263: training loss 1.317
Epoch 2 iteration 0780/1263: training loss 1.314
Epoch 2 iteration 0800/1263: training loss 1.313
Epoch 2 iteration 0820/1263: training loss 1.311
Epoch 2 iteration 0840/1263: training loss 1.310
Epoch 2 iteration 0860/1263: training loss 1.307
Epoch 2 iteration 0880/1263: training loss 1.309
Epoch 2 iteration 0900/1263: training loss 1.310
Epoch 2 iteration 0920/1263: training loss 1.313
Epoch 2 iteration 0940/1263: training loss 1.311
Epoch 2 iteration 0960/1263: training loss 1.312
Epoch 2 iteration 0980/1263: training loss 1.312
Epoch 2 iteration 1000/1263: training loss 1.312
Epoch 2 iteration 1020/1263: training loss 1.311
Epoch 2 iteration 1040/1263: training loss 1.312
Epoch 2 iteration 1060/1263: training loss 1.311
Epoch 2 iteration 1080/1263: training loss 1.311
Epoch 2 iteration 1100/1263: training loss 1.312
Epoch 2 iteration 1120/1263: training loss 1.311
Epoch 2 iteration 1140/1263: training loss 1.309
Epoch 2 iteration 1160/1263: training loss 1.311
Epoch 2 iteration 1180/1263: training loss 1.311
Epoch 2 iteration 1200/1263: training loss 1.313
Epoch 2 iteration 1220/1263: training loss 1.314
Epoch 2 iteration 1240/1263: training loss 1.314
Epoch 2 iteration 1260/1263: training loss 1.314
Epoch 2 validation pixAcc: 0.695, mIoU: 0.240
Epoch 3 iteration 0020/1263: training loss 1.273
Epoch 3 iteration 0040/1263: training loss 1.238
Epoch 3 iteration 0060/1263: training loss 1.218
Epoch 3 iteration 0080/1263: training loss 1.209
Epoch 3 iteration 0100/1263: training loss 1.212
Epoch 3 iteration 0120/1263: training loss 1.214
Epoch 3 iteration 0140/1263: training loss 1.222
Epoch 3 iteration 0160/1263: training loss 1.223
Epoch 3 iteration 0180/1263: training loss 1.225
Epoch 3 iteration 0200/1263: training loss 1.222
Epoch 3 iteration 0220/1263: training loss 1.220
Epoch 3 iteration 0240/1263: training loss 1.221
Epoch 3 iteration 0260/1263: training loss 1.228
Epoch 3 iteration 0280/1263: training loss 1.229
Epoch 3 iteration 0300/1263: training loss 1.234
Epoch 3 iteration 0320/1263: training loss 1.239
Epoch 3 iteration 0340/1263: training loss 1.245
Epoch 3 iteration 0360/1263: training loss 1.254
Epoch 3 iteration 0380/1263: training loss 1.260
Epoch 3 iteration 0400/1263: training loss 1.257
Epoch 3 iteration 0420/1263: training loss 1.254
Epoch 3 iteration 0440/1263: training loss 1.255
Epoch 3 iteration 0460/1263: training loss 1.256
Epoch 3 iteration 0480/1263: training loss 1.257
Epoch 3 iteration 0500/1263: training loss 1.256
Epoch 3 iteration 0520/1263: training loss 1.256
Epoch 3 iteration 0540/1263: training loss 1.259
Epoch 3 iteration 0560/1263: training loss 1.262
Epoch 3 iteration 0580/1263: training loss 1.259
Epoch 3 iteration 0600/1263: training loss 1.259
Epoch 3 iteration 0620/1263: training loss 1.257
Epoch 3 iteration 0640/1263: training loss 1.256
Epoch 3 iteration 0660/1263: training loss 1.256
Epoch 3 iteration 0680/1263: training loss 1.257
Epoch 3 iteration 0700/1263: training loss 1.253
Epoch 3 iteration 0720/1263: training loss 1.250
Epoch 3 iteration 0740/1263: training loss 1.252
Epoch 3 iteration 0760/1263: training loss 1.256
Epoch 3 iteration 0780/1263: training loss 1.259
Epoch 3 iteration 0800/1263: training loss 1.259
Epoch 3 iteration 0820/1263: training loss 1.257
Epoch 3 iteration 0840/1263: training loss 1.258
Epoch 3 iteration 0860/1263: training loss 1.260
Epoch 3 iteration 0880/1263: training loss 1.260
Epoch 3 iteration 0900/1263: training loss 1.257
Epoch 3 iteration 0920/1263: training loss 1.256
Epoch 3 iteration 0940/1263: training loss 1.253
Epoch 3 iteration 0960/1263: training loss 1.255
Epoch 3 iteration 0980/1263: training loss 1.258
Epoch 3 iteration 1000/1263: training loss 1.255
Epoch 3 iteration 1020/1263: training loss 1.256
Epoch 3 iteration 1040/1263: training loss 1.255
Epoch 3 iteration 1060/1263: training loss 1.255
Epoch 3 iteration 1080/1263: training loss 1.255
Epoch 3 iteration 1100/1263: training loss 1.256
Epoch 3 iteration 1120/1263: training loss 1.256
Epoch 3 iteration 1140/1263: training loss 1.257
Epoch 3 iteration 1160/1263: training loss 1.256
Epoch 3 iteration 1180/1263: training loss 1.255
Epoch 3 iteration 1200/1263: training loss 1.255
Epoch 3 iteration 1220/1263: training loss 1.255
Epoch 3 iteration 1240/1263: training loss 1.254
Epoch 3 iteration 1260/1263: training loss 1.253
Epoch 3 validation pixAcc: 0.710, mIoU: 0.266
Epoch 4 iteration 0020/1263: training loss 1.162
Epoch 4 iteration 0040/1263: training loss 1.173
Epoch 4 iteration 0060/1263: training loss 1.204
Epoch 4 iteration 0080/1263: training loss 1.208
Epoch 4 iteration 0100/1263: training loss 1.214
Epoch 4 iteration 0120/1263: training loss 1.204
Epoch 4 iteration 0140/1263: training loss 1.207
Epoch 4 iteration 0160/1263: training loss 1.204
Epoch 4 iteration 0180/1263: training loss 1.192
Epoch 4 iteration 0200/1263: training loss 1.186
Epoch 4 iteration 0220/1263: training loss 1.185
Epoch 4 iteration 0240/1263: training loss 1.180
Epoch 4 iteration 0260/1263: training loss 1.179
Epoch 4 iteration 0280/1263: training loss 1.175
Epoch 4 iteration 0300/1263: training loss 1.180
Epoch 4 iteration 0320/1263: training loss 1.176
Epoch 4 iteration 0340/1263: training loss 1.171
Epoch 4 iteration 0360/1263: training loss 1.172
Epoch 4 iteration 0380/1263: training loss 1.177
Epoch 4 iteration 0400/1263: training loss 1.177
Epoch 4 iteration 0420/1263: training loss 1.180
Epoch 4 iteration 0440/1263: training loss 1.183
Epoch 4 iteration 0460/1263: training loss 1.181
Epoch 4 iteration 0480/1263: training loss 1.182
Epoch 4 iteration 0500/1263: training loss 1.182
Epoch 4 iteration 0520/1263: training loss 1.186
Epoch 4 iteration 0540/1263: training loss 1.191
Epoch 4 iteration 0560/1263: training loss 1.191
Epoch 4 iteration 0580/1263: training loss 1.195
Epoch 4 iteration 0600/1263: training loss 1.197
Epoch 4 iteration 0620/1263: training loss 1.199
Epoch 4 iteration 0640/1263: training loss 1.202
Epoch 4 iteration 0660/1263: training loss 1.201
Epoch 4 iteration 0680/1263: training loss 1.201
Epoch 4 iteration 0700/1263: training loss 1.201
Epoch 4 iteration 0720/1263: training loss 1.199
Epoch 4 iteration 0740/1263: training loss 1.200
Epoch 4 iteration 0760/1263: training loss 1.201
Epoch 4 iteration 0780/1263: training loss 1.201
Epoch 4 iteration 0800/1263: training loss 1.201
Epoch 4 iteration 0820/1263: training loss 1.201
Epoch 4 iteration 0840/1263: training loss 1.200
Epoch 4 iteration 0860/1263: training loss 1.202
Epoch 4 iteration 0880/1263: training loss 1.201
Epoch 4 iteration 0900/1263: training loss 1.201
Epoch 4 iteration 0920/1263: training loss 1.201
Epoch 4 iteration 0940/1263: training loss 1.204
Epoch 4 iteration 0960/1263: training loss 1.204
Epoch 4 iteration 0980/1263: training loss 1.206
Epoch 4 iteration 1000/1263: training loss 1.205
Epoch 4 iteration 1020/1263: training loss 1.207
Epoch 4 iteration 1040/1263: training loss 1.208
Epoch 4 iteration 1060/1263: training loss 1.209
Epoch 4 iteration 1080/1263: training loss 1.209
Epoch 4 iteration 1100/1263: training loss 1.207
Epoch 4 iteration 1120/1263: training loss 1.208
Epoch 4 iteration 1140/1263: training loss 1.209
Epoch 4 iteration 1160/1263: training loss 1.209
Epoch 4 iteration 1180/1263: training loss 1.210
Epoch 4 iteration 1200/1263: training loss 1.209
Epoch 4 iteration 1220/1263: training loss 1.211
Epoch 4 iteration 1240/1263: training loss 1.210
Epoch 4 iteration 1260/1263: training loss 1.211
Epoch 4 validation pixAcc: 0.708, mIoU: 0.282
Epoch 5 iteration 0020/1263: training loss 1.115
Epoch 5 iteration 0040/1263: training loss 1.145
Epoch 5 iteration 0060/1263: training loss 1.126
Epoch 5 iteration 0080/1263: training loss 1.113
Epoch 5 iteration 0100/1263: training loss 1.129
Epoch 5 iteration 0120/1263: training loss 1.140
Epoch 5 iteration 0140/1263: training loss 1.142
Epoch 5 iteration 0160/1263: training loss 1.144
Epoch 5 iteration 0180/1263: training loss 1.147
Epoch 5 iteration 0200/1263: training loss 1.146
Epoch 5 iteration 0220/1263: training loss 1.145
Epoch 5 iteration 0240/1263: training loss 1.149
Epoch 5 iteration 0260/1263: training loss 1.147
Epoch 5 iteration 0280/1263: training loss 1.148
Epoch 5 iteration 0300/1263: training loss 1.153
Epoch 5 iteration 0320/1263: training loss 1.151
Epoch 5 iteration 0340/1263: training loss 1.149
Epoch 5 iteration 0360/1263: training loss 1.147
Epoch 5 iteration 0380/1263: training loss 1.146
Epoch 5 iteration 0400/1263: training loss 1.149
Epoch 5 iteration 0420/1263: training loss 1.149
Epoch 5 iteration 0440/1263: training loss 1.150
Epoch 5 iteration 0460/1263: training loss 1.156
Epoch 5 iteration 0480/1263: training loss 1.153
Epoch 5 iteration 0500/1263: training loss 1.153
Epoch 5 iteration 0520/1263: training loss 1.154
Epoch 5 iteration 0540/1263: training loss 1.153
Epoch 5 iteration 0560/1263: training loss 1.155
Epoch 5 iteration 0580/1263: training loss 1.156
Epoch 5 iteration 0600/1263: training loss 1.151
Epoch 5 iteration 0620/1263: training loss 1.150
Epoch 5 iteration 0640/1263: training loss 1.147
Epoch 5 iteration 0660/1263: training loss 1.146
Epoch 5 iteration 0680/1263: training loss 1.146
Epoch 5 iteration 0700/1263: training loss 1.146
Epoch 5 iteration 0720/1263: training loss 1.149
Epoch 5 iteration 0740/1263: training loss 1.149
Epoch 5 iteration 0760/1263: training loss 1.149
Epoch 5 iteration 0780/1263: training loss 1.149
Epoch 5 iteration 0800/1263: training loss 1.146
Epoch 5 iteration 0820/1263: training loss 1.145
Epoch 5 iteration 0840/1263: training loss 1.144
Epoch 5 iteration 0860/1263: training loss 1.144
Epoch 5 iteration 0880/1263: training loss 1.143
Epoch 5 iteration 0900/1263: training loss 1.143
Epoch 5 iteration 0920/1263: training loss 1.142
Epoch 5 iteration 0940/1263: training loss 1.143
Epoch 5 iteration 0960/1263: training loss 1.142
Epoch 5 iteration 0980/1263: training loss 1.146
Epoch 5 iteration 1000/1263: training loss 1.148
Epoch 5 iteration 1020/1263: training loss 1.149
Epoch 5 iteration 1040/1263: training loss 1.148
Epoch 5 iteration 1060/1263: training loss 1.149
Epoch 5 iteration 1080/1263: training loss 1.150
Epoch 5 iteration 1100/1263: training loss 1.151
Epoch 5 iteration 1120/1263: training loss 1.149
Epoch 5 iteration 1140/1263: training loss 1.150
Epoch 5 iteration 1160/1263: training loss 1.150
Epoch 5 iteration 1180/1263: training loss 1.147
Epoch 5 iteration 1200/1263: training loss 1.146
Epoch 5 iteration 1220/1263: training loss 1.148
Epoch 5 iteration 1240/1263: training loss 1.147
Epoch 5 iteration 1260/1263: training loss 1.147
Epoch 5 validation pixAcc: 0.722, mIoU: 0.306
Epoch 6 iteration 0020/1263: training loss 1.093
Epoch 6 iteration 0040/1263: training loss 1.113
Epoch 6 iteration 0060/1263: training loss 1.084
Epoch 6 iteration 0080/1263: training loss 1.101
Epoch 6 iteration 0100/1263: training loss 1.091
Epoch 6 iteration 0120/1263: training loss 1.089
Epoch 6 iteration 0140/1263: training loss 1.081
Epoch 6 iteration 0160/1263: training loss 1.085
Epoch 6 iteration 0180/1263: training loss 1.094
Epoch 6 iteration 0200/1263: training loss 1.080
Epoch 6 iteration 0220/1263: training loss 1.079
Epoch 6 iteration 0240/1263: training loss 1.076
Epoch 6 iteration 0260/1263: training loss 1.076
Epoch 6 iteration 0280/1263: training loss 1.074
Epoch 6 iteration 0300/1263: training loss 1.067
Epoch 6 iteration 0320/1263: training loss 1.071
Epoch 6 iteration 0340/1263: training loss 1.067
Epoch 6 iteration 0360/1263: training loss 1.064
Epoch 6 iteration 0380/1263: training loss 1.061
Epoch 6 iteration 0400/1263: training loss 1.058
Epoch 6 iteration 0420/1263: training loss 1.061
Epoch 6 iteration 0440/1263: training loss 1.058
Epoch 6 iteration 0460/1263: training loss 1.057
Epoch 6 iteration 0480/1263: training loss 1.054
Epoch 6 iteration 0500/1263: training loss 1.051
Epoch 6 iteration 0520/1263: training loss 1.056
Epoch 6 iteration 0540/1263: training loss 1.057
Epoch 6 iteration 0560/1263: training loss 1.058
Epoch 6 iteration 0580/1263: training loss 1.064
Epoch 6 iteration 0600/1263: training loss 1.065
Epoch 6 iteration 0620/1263: training loss 1.067
Epoch 6 iteration 0640/1263: training loss 1.070
Epoch 6 iteration 0660/1263: training loss 1.068
Epoch 6 iteration 0680/1263: training loss 1.070
Epoch 6 iteration 0700/1263: training loss 1.069
Epoch 6 iteration 0720/1263: training loss 1.070
Epoch 6 iteration 0740/1263: training loss 1.069
Epoch 6 iteration 0760/1263: training loss 1.067
Epoch 6 iteration 0780/1263: training loss 1.066
Epoch 6 iteration 0800/1263: training loss 1.065
Epoch 6 iteration 0820/1263: training loss 1.067
Epoch 6 iteration 0840/1263: training loss 1.068
Epoch 6 iteration 0860/1263: training loss 1.068
Epoch 6 iteration 0880/1263: training loss 1.068
Epoch 6 iteration 0900/1263: training loss 1.068
Epoch 6 iteration 0920/1263: training loss 1.068
Epoch 6 iteration 0940/1263: training loss 1.068
Epoch 6 iteration 0960/1263: training loss 1.066
Epoch 6 iteration 0980/1263: training loss 1.065
Epoch 6 iteration 1000/1263: training loss 1.064
Epoch 6 iteration 1020/1263: training loss 1.062
Epoch 6 iteration 1040/1263: training loss 1.063
Epoch 6 iteration 1060/1263: training loss 1.063
Epoch 6 iteration 1080/1263: training loss 1.062
Epoch 6 iteration 1100/1263: training loss 1.062
Epoch 6 iteration 1120/1263: training loss 1.062
Epoch 6 iteration 1140/1263: training loss 1.062
Epoch 6 iteration 1160/1263: training loss 1.062
Epoch 6 iteration 1180/1264: training loss 1.062
Epoch 6 iteration 1200/1264: training loss 1.062
Epoch 6 iteration 1220/1264: training loss 1.061
Epoch 6 iteration 1240/1264: training loss 1.060
Epoch 6 iteration 1260/1264: training loss 1.059
Epoch 6 validation pixAcc: 0.736, mIoU: 0.320
Epoch 7 iteration 0020/1263: training loss 1.017
Epoch 7 iteration 0040/1263: training loss 0.988
Epoch 7 iteration 0060/1263: training loss 0.968
Epoch 7 iteration 0080/1263: training loss 0.966
Epoch 7 iteration 0100/1263: training loss 0.971
Epoch 7 iteration 0120/1263: training loss 0.971
Epoch 7 iteration 0140/1263: training loss 0.979
Epoch 7 iteration 0160/1263: training loss 0.964
Epoch 7 iteration 0180/1263: training loss 0.965
Epoch 7 iteration 0200/1263: training loss 0.964
Epoch 7 iteration 0220/1263: training loss 0.960
Epoch 7 iteration 0240/1263: training loss 0.963
Epoch 7 iteration 0260/1263: training loss 0.964
Epoch 7 iteration 0280/1263: training loss 0.966
Epoch 7 iteration 0300/1263: training loss 0.965
Epoch 7 iteration 0320/1263: training loss 0.963
Epoch 7 iteration 0340/1263: training loss 0.966
Epoch 7 iteration 0360/1263: training loss 0.966
Epoch 7 iteration 0380/1263: training loss 0.975
Epoch 7 iteration 0400/1263: training loss 0.979
Epoch 7 iteration 0420/1263: training loss 0.979
Epoch 7 iteration 0440/1263: training loss 0.979
Epoch 7 iteration 0460/1263: training loss 0.980
Epoch 7 iteration 0480/1263: training loss 0.981
Epoch 7 iteration 0500/1263: training loss 0.980
Epoch 7 iteration 0520/1263: training loss 0.981
Epoch 7 iteration 0540/1263: training loss 0.980
Epoch 7 iteration 0560/1263: training loss 0.979
Epoch 7 iteration 0580/1263: training loss 0.984
Epoch 7 iteration 0600/1263: training loss 0.986
Epoch 7 iteration 0620/1263: training loss 0.987
Epoch 7 iteration 0640/1263: training loss 0.988
Epoch 7 iteration 0660/1263: training loss 0.990
Epoch 7 iteration 0680/1263: training loss 0.991
Epoch 7 iteration 0700/1263: training loss 0.991
Epoch 7 iteration 0720/1263: training loss 0.991
Epoch 7 iteration 0740/1263: training loss 0.991
Epoch 7 iteration 0760/1263: training loss 0.992
Epoch 7 iteration 0780/1263: training loss 0.992
Epoch 7 iteration 0800/1263: training loss 0.991
Epoch 7 iteration 0820/1263: training loss 0.993
Epoch 7 iteration 0840/1263: training loss 0.994
Epoch 7 iteration 0860/1263: training loss 0.997
Epoch 7 iteration 0880/1263: training loss 0.997
Epoch 7 iteration 0900/1263: training loss 0.997
Epoch 7 iteration 0920/1263: training loss 0.999
Epoch 7 iteration 0940/1263: training loss 0.997
Epoch 7 iteration 0960/1263: training loss 0.996
Epoch 7 iteration 0980/1263: training loss 0.997
Epoch 7 iteration 1000/1263: training loss 0.998
Epoch 7 iteration 1020/1263: training loss 1.001
Epoch 7 iteration 1040/1263: training loss 1.001
Epoch 7 iteration 1060/1263: training loss 1.003
Epoch 7 iteration 1080/1263: training loss 1.004
Epoch 7 iteration 1100/1263: training loss 1.004
Epoch 7 iteration 1120/1263: training loss 1.005
Epoch 7 iteration 1140/1263: training loss 1.004
Epoch 7 iteration 1160/1263: training loss 1.005
Epoch 7 iteration 1180/1263: training loss 1.005
Epoch 7 iteration 1200/1263: training loss 1.005
Epoch 7 iteration 1220/1263: training loss 1.005
Epoch 7 iteration 1240/1263: training loss 1.006
Epoch 7 iteration 1260/1263: training loss 1.006
Epoch 7 validation pixAcc: 0.721, mIoU: 0.313
Epoch 8 iteration 0020/1263: training loss 1.073
Epoch 8 iteration 0040/1263: training loss 1.035
Epoch 8 iteration 0060/1263: training loss 0.979
Epoch 8 iteration 0080/1263: training loss 0.971
Epoch 8 iteration 0100/1263: training loss 0.988
Epoch 8 iteration 0120/1263: training loss 0.995
Epoch 8 iteration 0140/1263: training loss 0.992
Epoch 8 iteration 0160/1263: training loss 0.996
Epoch 8 iteration 0180/1263: training loss 0.991
Epoch 8 iteration 0200/1263: training loss 0.994
Epoch 8 iteration 0220/1263: training loss 0.992
Epoch 8 iteration 0240/1263: training loss 0.990
Epoch 8 iteration 0260/1263: training loss 0.986
Epoch 8 iteration 0280/1263: training loss 0.992
Epoch 8 iteration 0300/1263: training loss 0.991
Epoch 8 iteration 0320/1263: training loss 0.990
Epoch 8 iteration 0340/1263: training loss 0.989
Epoch 8 iteration 0360/1263: training loss 0.988
Epoch 8 iteration 0380/1263: training loss 0.989
Epoch 8 iteration 0400/1263: training loss 0.988
Epoch 8 iteration 0420/1263: training loss 0.983
Epoch 8 iteration 0440/1263: training loss 0.978
Epoch 8 iteration 0460/1263: training loss 0.979
Epoch 8 iteration 0480/1263: training loss 0.979
Epoch 8 iteration 0500/1263: training loss 0.979
Epoch 8 iteration 0520/1263: training loss 0.978
Epoch 8 iteration 0540/1263: training loss 0.977
Epoch 8 iteration 0560/1263: training loss 0.974
Epoch 8 iteration 0580/1263: training loss 0.973
Epoch 8 iteration 0600/1263: training loss 0.974
Epoch 8 iteration 0620/1263: training loss 0.970
Epoch 8 iteration 0640/1263: training loss 0.969
Epoch 8 iteration 0660/1263: training loss 0.970
Epoch 8 iteration 0680/1263: training loss 0.971
Epoch 8 iteration 0700/1263: training loss 0.970
Epoch 8 iteration 0720/1263: training loss 0.971
Epoch 8 iteration 0740/1263: training loss 0.971
Epoch 8 iteration 0760/1263: training loss 0.971
Epoch 8 iteration 0780/1263: training loss 0.969
Epoch 8 iteration 0800/1263: training loss 0.967
Epoch 8 iteration 0820/1263: training loss 0.965
Epoch 8 iteration 0840/1263: training loss 0.965
Epoch 8 iteration 0860/1263: training loss 0.967
Epoch 8 iteration 0880/1263: training loss 0.966
Epoch 8 iteration 0900/1263: training loss 0.967
Epoch 8 iteration 0920/1263: training loss 0.967
Epoch 8 iteration 0940/1263: training loss 0.966
Epoch 8 iteration 0960/1263: training loss 0.967
Epoch 8 iteration 0980/1263: training loss 0.965
Epoch 8 iteration 1000/1263: training loss 0.963
Epoch 8 iteration 1020/1263: training loss 0.963
Epoch 8 iteration 1040/1263: training loss 0.963
Epoch 8 iteration 1060/1263: training loss 0.963
Epoch 8 iteration 1080/1263: training loss 0.964
Epoch 8 iteration 1100/1263: training loss 0.966
Epoch 8 iteration 1120/1263: training loss 0.966
Epoch 8 iteration 1140/1263: training loss 0.970
Epoch 8 iteration 1160/1263: training loss 0.971
Epoch 8 iteration 1180/1263: training loss 0.972
Epoch 8 iteration 1200/1263: training loss 0.972
Epoch 8 iteration 1220/1263: training loss 0.973
Epoch 8 iteration 1240/1263: training loss 0.973
Epoch 8 iteration 1260/1263: training loss 0.975
Epoch 8 validation pixAcc: 0.749, mIoU: 0.350
Epoch 9 iteration 0020/1263: training loss 0.844
Epoch 9 iteration 0040/1263: training loss 0.865
Epoch 9 iteration 0060/1263: training loss 0.860
Epoch 9 iteration 0080/1263: training loss 0.889
Epoch 9 iteration 0100/1263: training loss 0.896
Epoch 9 iteration 0120/1263: training loss 0.892
Epoch 9 iteration 0140/1263: training loss 0.893
Epoch 9 iteration 0160/1263: training loss 0.889
Epoch 9 iteration 0180/1263: training loss 0.886
Epoch 9 iteration 0200/1263: training loss 0.889
Epoch 9 iteration 0220/1263: training loss 0.896
Epoch 9 iteration 0240/1263: training loss 0.895
Epoch 9 iteration 0260/1263: training loss 0.894
Epoch 9 iteration 0280/1263: training loss 0.897
Epoch 9 iteration 0300/1263: training loss 0.898
Epoch 9 iteration 0320/1263: training loss 0.898
Epoch 9 iteration 0340/1263: training loss 0.899
Epoch 9 iteration 0360/1263: training loss 0.901
Epoch 9 iteration 0380/1263: training loss 0.905
Epoch 9 iteration 0400/1263: training loss 0.905
Epoch 9 iteration 0420/1263: training loss 0.904
Epoch 9 iteration 0440/1263: training loss 0.903
Epoch 9 iteration 0460/1263: training loss 0.903
Epoch 9 iteration 0480/1263: training loss 0.901
Epoch 9 iteration 0500/1263: training loss 0.900
Epoch 9 iteration 0520/1263: training loss 0.901
Epoch 9 iteration 0540/1263: training loss 0.899
Epoch 9 iteration 0560/1263: training loss 0.902
Epoch 9 iteration 0580/1263: training loss 0.902
Epoch 9 iteration 0600/1263: training loss 0.903
Epoch 9 iteration 0620/1263: training loss 0.903
Epoch 9 iteration 0640/1263: training loss 0.901
Epoch 9 iteration 0660/1263: training loss 0.902
Epoch 9 iteration 0680/1263: training loss 0.899
Epoch 9 iteration 0700/1263: training loss 0.901
Epoch 9 iteration 0720/1263: training loss 0.899
Epoch 9 iteration 0740/1263: training loss 0.897
Epoch 9 iteration 0760/1263: training loss 0.899
Epoch 9 iteration 0780/1263: training loss 0.900
Epoch 9 iteration 0800/1263: training loss 0.900
Epoch 9 iteration 0820/1263: training loss 0.900
Epoch 9 iteration 0840/1263: training loss 0.900
Epoch 9 iteration 0860/1263: training loss 0.897
Epoch 9 iteration 0880/1263: training loss 0.899
Epoch 9 iteration 0900/1263: training loss 0.899
Epoch 9 iteration 0920/1263: training loss 0.898
Epoch 9 iteration 0940/1263: training loss 0.898
Epoch 9 iteration 0960/1263: training loss 0.900
Epoch 9 iteration 0980/1263: training loss 0.900
Epoch 9 iteration 1000/1263: training loss 0.900
Epoch 9 iteration 1020/1263: training loss 0.902
Epoch 9 iteration 1040/1263: training loss 0.901
Epoch 9 iteration 1060/1263: training loss 0.899
Epoch 9 iteration 1080/1263: training loss 0.900
Epoch 9 iteration 1100/1263: training loss 0.901
Epoch 9 iteration 1120/1263: training loss 0.901
Epoch 9 iteration 1140/1263: training loss 0.901
Epoch 9 iteration 1160/1263: training loss 0.902
Epoch 9 iteration 1180/1263: training loss 0.901
Epoch 9 iteration 1200/1263: training loss 0.902
Epoch 9 iteration 1220/1263: training loss 0.902
Epoch 9 iteration 1240/1263: training loss 0.902
Epoch 9 iteration 1260/1263: training loss 0.904
Epoch 9 validation pixAcc: 0.750, mIoU: 0.347
Epoch 10 iteration 0020/1263: training loss 0.881
Epoch 10 iteration 0040/1263: training loss 0.881
Epoch 10 iteration 0060/1263: training loss 0.874
Epoch 10 iteration 0080/1263: training loss 0.871
Epoch 10 iteration 0100/1263: training loss 0.880
Epoch 10 iteration 0120/1263: training loss 0.899
Epoch 10 iteration 0140/1263: training loss 0.898
Epoch 10 iteration 0160/1263: training loss 0.900
Epoch 10 iteration 0180/1263: training loss 0.901
Epoch 10 iteration 0200/1263: training loss 0.903
Epoch 10 iteration 0220/1263: training loss 0.897
Epoch 10 iteration 0240/1263: training loss 0.900
Epoch 10 iteration 0260/1263: training loss 0.896
Epoch 10 iteration 0280/1263: training loss 0.893
Epoch 10 iteration 0300/1263: training loss 0.891
Epoch 10 iteration 0320/1263: training loss 0.895
Epoch 10 iteration 0340/1263: training loss 0.896
Epoch 10 iteration 0360/1263: training loss 0.893
Epoch 10 iteration 0380/1263: training loss 0.888
Epoch 10 iteration 0400/1263: training loss 0.890
Epoch 10 iteration 0420/1263: training loss 0.889
Epoch 10 iteration 0440/1263: training loss 0.891
Epoch 10 iteration 0460/1263: training loss 0.893
Epoch 10 iteration 0480/1263: training loss 0.895
Epoch 10 iteration 0500/1263: training loss 0.894
Epoch 10 iteration 0520/1263: training loss 0.893
Epoch 10 iteration 0540/1263: training loss 0.893
Epoch 10 iteration 0560/1263: training loss 0.891
Epoch 10 iteration 0580/1263: training loss 0.892
Epoch 10 iteration 0600/1263: training loss 0.892
Epoch 10 iteration 0620/1263: training loss 0.893
Epoch 10 iteration 0640/1263: training loss 0.891
Epoch 10 iteration 0660/1263: training loss 0.893
Epoch 10 iteration 0680/1263: training loss 0.894
Epoch 10 iteration 0700/1263: training loss 0.897
Epoch 10 iteration 0720/1263: training loss 0.896
Epoch 10 iteration 0740/1263: training loss 0.898
Epoch 10 iteration 0760/1263: training loss 0.897
Epoch 10 iteration 0780/1263: training loss 0.897
Epoch 10 iteration 0800/1263: training loss 0.897
Epoch 10 iteration 0820/1263: training loss 0.898
Epoch 10 iteration 0840/1263: training loss 0.898
Epoch 10 iteration 0860/1263: training loss 0.899
Epoch 10 iteration 0880/1263: training loss 0.900
Epoch 10 iteration 0900/1263: training loss 0.899
Epoch 10 iteration 0920/1263: training loss 0.900
Epoch 10 iteration 0940/1263: training loss 0.900
Epoch 10 iteration 0960/1263: training loss 0.900
Epoch 10 iteration 0980/1263: training loss 0.901
Epoch 10 iteration 1000/1263: training loss 0.901
Epoch 10 iteration 1020/1263: training loss 0.900
Epoch 10 iteration 1040/1263: training loss 0.899
Epoch 10 iteration 1060/1263: training loss 0.903
Epoch 10 iteration 1080/1263: training loss 0.903
Epoch 10 iteration 1100/1263: training loss 0.902
Epoch 10 iteration 1120/1263: training loss 0.902
Epoch 10 iteration 1140/1263: training loss 0.901
Epoch 10 iteration 1160/1263: training loss 0.902
Epoch 10 iteration 1180/1263: training loss 0.901
Epoch 10 iteration 1200/1263: training loss 0.900
Epoch 10 iteration 1220/1263: training loss 0.900
Epoch 10 iteration 1240/1263: training loss 0.901
Epoch 10 iteration 1260/1263: training loss 0.901
Epoch 10 validation pixAcc: 0.755, mIoU: 0.358
Epoch 11 iteration 0020/1263: training loss 0.787
Epoch 11 iteration 0040/1263: training loss 0.816
Epoch 11 iteration 0060/1263: training loss 0.837
Epoch 11 iteration 0080/1263: training loss 0.845
Epoch 11 iteration 0100/1263: training loss 0.829
Epoch 11 iteration 0120/1263: training loss 0.825
Epoch 11 iteration 0140/1263: training loss 0.819
Epoch 11 iteration 0160/1263: training loss 0.823
Epoch 11 iteration 0180/1263: training loss 0.822
Epoch 11 iteration 0200/1263: training loss 0.828
Epoch 11 iteration 0220/1263: training loss 0.827
Epoch 11 iteration 0240/1263: training loss 0.828
Epoch 11 iteration 0260/1263: training loss 0.834
Epoch 11 iteration 0280/1263: training loss 0.834
Epoch 11 iteration 0300/1263: training loss 0.838
Epoch 11 iteration 0320/1263: training loss 0.834
Epoch 11 iteration 0340/1263: training loss 0.840
Epoch 11 iteration 0360/1263: training loss 0.841
Epoch 11 iteration 0380/1263: training loss 0.846
Epoch 11 iteration 0400/1263: training loss 0.848
Epoch 11 iteration 0420/1263: training loss 0.851
Epoch 11 iteration 0440/1263: training loss 0.853
Epoch 11 iteration 0460/1263: training loss 0.856
Epoch 11 iteration 0480/1263: training loss 0.856
Epoch 11 iteration 0500/1263: training loss 0.856
Epoch 11 iteration 0520/1263: training loss 0.855
Epoch 11 iteration 0540/1263: training loss 0.855
Epoch 11 iteration 0560/1263: training loss 0.854
Epoch 11 iteration 0580/1263: training loss 0.854
Epoch 11 iteration 0600/1263: training loss 0.855
Epoch 11 iteration 0620/1263: training loss 0.856
Epoch 11 iteration 0640/1263: training loss 0.858
Epoch 11 iteration 0660/1263: training loss 0.859
Epoch 11 iteration 0680/1263: training loss 0.857
Epoch 11 iteration 0700/1263: training loss 0.858
Epoch 11 iteration 0720/1263: training loss 0.858
Epoch 11 iteration 0740/1263: training loss 0.858
Epoch 11 iteration 0760/1263: training loss 0.858
Epoch 11 iteration 0780/1263: training loss 0.858
Epoch 11 iteration 0800/1263: training loss 0.859
Epoch 11 iteration 0820/1263: training loss 0.860
Epoch 11 iteration 0840/1263: training loss 0.860
Epoch 11 iteration 0860/1263: training loss 0.861
Epoch 11 iteration 0880/1263: training loss 0.860
Epoch 11 iteration 0900/1263: training loss 0.861
Epoch 11 iteration 0920/1263: training loss 0.862
Epoch 11 iteration 0940/1263: training loss 0.860
Epoch 11 iteration 0960/1263: training loss 0.860
Epoch 11 iteration 0980/1263: training loss 0.859
Epoch 11 iteration 1000/1263: training loss 0.859
Epoch 11 iteration 1020/1263: training loss 0.860
Epoch 11 iteration 1040/1263: training loss 0.860
Epoch 11 iteration 1060/1263: training loss 0.861
Epoch 11 iteration 1080/1263: training loss 0.859
Epoch 11 iteration 1100/1263: training loss 0.862
Epoch 11 iteration 1120/1263: training loss 0.862
Epoch 11 iteration 1140/1263: training loss 0.862
Epoch 11 iteration 1160/1263: training loss 0.864
Epoch 11 iteration 1180/1263: training loss 0.866
Epoch 11 iteration 1200/1263: training loss 0.867
Epoch 11 iteration 1220/1263: training loss 0.866
Epoch 11 iteration 1240/1263: training loss 0.866
Epoch 11 iteration 1260/1263: training loss 0.865
Epoch 11 validation pixAcc: 0.766, mIoU: 0.380
Epoch 12 iteration 0020/1263: training loss 0.756
Epoch 12 iteration 0040/1263: training loss 0.773
Epoch 12 iteration 0060/1263: training loss 0.786
Epoch 12 iteration 0080/1263: training loss 0.799
Epoch 12 iteration 0100/1263: training loss 0.827
Epoch 12 iteration 0120/1263: training loss 0.823
Epoch 12 iteration 0140/1263: training loss 0.819
Epoch 12 iteration 0160/1263: training loss 0.814
Epoch 12 iteration 0180/1263: training loss 0.815
Epoch 12 iteration 0200/1263: training loss 0.811
Epoch 12 iteration 0220/1263: training loss 0.811
Epoch 12 iteration 0240/1263: training loss 0.810
Epoch 12 iteration 0260/1263: training loss 0.807
Epoch 12 iteration 0280/1263: training loss 0.810
Epoch 12 iteration 0300/1263: training loss 0.812
Epoch 12 iteration 0320/1263: training loss 0.812
Epoch 12 iteration 0340/1263: training loss 0.810
Epoch 12 iteration 0360/1263: training loss 0.807
Epoch 12 iteration 0380/1263: training loss 0.807
Epoch 12 iteration 0400/1263: training loss 0.811
Epoch 12 iteration 0420/1263: training loss 0.813
Epoch 12 iteration 0440/1263: training loss 0.815
Epoch 12 iteration 0460/1263: training loss 0.815
Epoch 12 iteration 0480/1263: training loss 0.818
Epoch 12 iteration 0500/1263: training loss 0.822
Epoch 12 iteration 0520/1263: training loss 0.824
Epoch 12 iteration 0540/1263: training loss 0.823
Epoch 12 iteration 0560/1263: training loss 0.823
Epoch 12 iteration 0580/1263: training loss 0.825
Epoch 12 iteration 0600/1263: training loss 0.829
Epoch 12 iteration 0620/1263: training loss 0.829
Epoch 12 iteration 0640/1263: training loss 0.829
Epoch 12 iteration 0660/1263: training loss 0.830
Epoch 12 iteration 0680/1263: training loss 0.833
Epoch 12 iteration 0700/1263: training loss 0.834
Epoch 12 iteration 0720/1263: training loss 0.836
Epoch 12 iteration 0740/1263: training loss 0.835
Epoch 12 iteration 0760/1263: training loss 0.837
Epoch 12 iteration 0780/1263: training loss 0.840
Epoch 12 iteration 0800/1263: training loss 0.842
Epoch 12 iteration 0820/1263: training loss 0.843
Epoch 12 iteration 0840/1263: training loss 0.844
Epoch 12 iteration 0860/1263: training loss 0.843
Epoch 12 iteration 0880/1263: training loss 0.842
Epoch 12 iteration 0900/1263: training loss 0.842
Epoch 12 iteration 0920/1263: training loss 0.844
Epoch 12 iteration 0940/1263: training loss 0.846
Epoch 12 iteration 0960/1263: training loss 0.847
Epoch 12 iteration 0980/1263: training loss 0.847
Epoch 12 iteration 1000/1263: training loss 0.848
Epoch 12 iteration 1020/1263: training loss 0.850
Epoch 12 iteration 1040/1263: training loss 0.849
Epoch 12 iteration 1060/1263: training loss 0.849
Epoch 12 iteration 1080/1263: training loss 0.850
Epoch 12 iteration 1100/1263: training loss 0.850
Epoch 12 iteration 1120/1263: training loss 0.851
Epoch 12 iteration 1140/1263: training loss 0.850
Epoch 12 iteration 1160/1263: training loss 0.850
Epoch 12 iteration 1180/1263: training loss 0.849
Epoch 12 iteration 1200/1263: training loss 0.850
Epoch 12 iteration 1220/1263: training loss 0.849
Epoch 12 iteration 1240/1263: training loss 0.850
Epoch 12 iteration 1260/1263: training loss 0.851
Epoch 12 validation pixAcc: 0.749, mIoU: 0.354
Epoch 13 iteration 0020/1263: training loss 0.857
Epoch 13 iteration 0040/1263: training loss 0.799
Epoch 13 iteration 0060/1263: training loss 0.798
Epoch 13 iteration 0080/1263: training loss 0.793
Epoch 13 iteration 0100/1263: training loss 0.804
Epoch 13 iteration 0120/1263: training loss 0.804
Epoch 13 iteration 0140/1263: training loss 0.821
Epoch 13 iteration 0160/1263: training loss 0.823
Epoch 13 iteration 0180/1263: training loss 0.826
Epoch 13 iteration 0200/1263: training loss 0.820
Epoch 13 iteration 0220/1263: training loss 0.820
Epoch 13 iteration 0240/1263: training loss 0.818
Epoch 13 iteration 0260/1263: training loss 0.821
Epoch 13 iteration 0280/1263: training loss 0.817
Epoch 13 iteration 0300/1263: training loss 0.816
Epoch 13 iteration 0320/1263: training loss 0.814
Epoch 13 iteration 0340/1263: training loss 0.815
Epoch 13 iteration 0360/1263: training loss 0.816
Epoch 13 iteration 0380/1263: training loss 0.816
Epoch 13 iteration 0400/1263: training loss 0.810
Epoch 13 iteration 0420/1263: training loss 0.809
Epoch 13 iteration 0440/1263: training loss 0.808
Epoch 13 iteration 0460/1263: training loss 0.809
Epoch 13 iteration 0480/1263: training loss 0.809
Epoch 13 iteration 0500/1263: training loss 0.808
Epoch 13 iteration 0520/1263: training loss 0.806
Epoch 13 iteration 0540/1263: training loss 0.805
Epoch 13 iteration 0560/1263: training loss 0.804
Epoch 13 iteration 0580/1263: training loss 0.803
Epoch 13 iteration 0600/1263: training loss 0.802
Epoch 13 iteration 0620/1263: training loss 0.804
Epoch 13 iteration 0640/1263: training loss 0.805
Epoch 13 iteration 0660/1263: training loss 0.805
Epoch 13 iteration 0680/1263: training loss 0.809
Epoch 13 iteration 0700/1263: training loss 0.811
Epoch 13 iteration 0720/1263: training loss 0.815
Epoch 13 iteration 0740/1263: training loss 0.815
Epoch 13 iteration 0760/1263: training loss 0.817
Epoch 13 iteration 0780/1263: training loss 0.818
Epoch 13 iteration 0800/1263: training loss 0.822
Epoch 13 iteration 0820/1263: training loss 0.825
Epoch 13 iteration 0840/1263: training loss 0.826
Epoch 13 iteration 0860/1263: training loss 0.826
Epoch 13 iteration 0880/1263: training loss 0.825
Epoch 13 iteration 0900/1263: training loss 0.827
Epoch 13 iteration 0920/1263: training loss 0.830
Epoch 13 iteration 0940/1263: training loss 0.831
Epoch 13 iteration 0960/1263: training loss 0.833
Epoch 13 iteration 0980/1263: training loss 0.832
Epoch 13 iteration 1000/1263: training loss 0.832
Epoch 13 iteration 1020/1263: training loss 0.833
Epoch 13 iteration 1040/1263: training loss 0.833
Epoch 13 iteration 1060/1263: training loss 0.833
Epoch 13 iteration 1080/1263: training loss 0.835
Epoch 13 iteration 1100/1263: training loss 0.835
Epoch 13 iteration 1120/1263: training loss 0.834
Epoch 13 iteration 1140/1263: training loss 0.835
Epoch 13 iteration 1160/1263: training loss 0.834
Epoch 13 iteration 1180/1263: training loss 0.835
Epoch 13 iteration 1200/1263: training loss 0.834
Epoch 13 iteration 1220/1263: training loss 0.834
Epoch 13 iteration 1240/1263: training loss 0.834
Epoch 13 iteration 1260/1263: training loss 0.834
Epoch 13 validation pixAcc: 0.759, mIoU: 0.368
Epoch 14 iteration 0020/1263: training loss 0.799
Epoch 14 iteration 0040/1263: training loss 0.854
Epoch 14 iteration 0060/1263: training loss 0.868
Epoch 14 iteration 0080/1263: training loss 0.849
Epoch 14 iteration 0100/1263: training loss 0.838
Epoch 14 iteration 0120/1263: training loss 0.830
Epoch 14 iteration 0140/1263: training loss 0.823
Epoch 14 iteration 0160/1263: training loss 0.813
Epoch 14 iteration 0180/1263: training loss 0.811
Epoch 14 iteration 0200/1263: training loss 0.808
Epoch 14 iteration 0220/1263: training loss 0.809
Epoch 14 iteration 0240/1263: training loss 0.802
Epoch 14 iteration 0260/1263: training loss 0.800
Epoch 14 iteration 0280/1263: training loss 0.799
Epoch 14 iteration 0300/1263: training loss 0.803
Epoch 14 iteration 0320/1263: training loss 0.799
Epoch 14 iteration 0340/1263: training loss 0.795
Epoch 14 iteration 0360/1263: training loss 0.801
Epoch 14 iteration 0380/1263: training loss 0.799
Epoch 14 iteration 0400/1263: training loss 0.798
Epoch 14 iteration 0420/1263: training loss 0.800
Epoch 14 iteration 0440/1263: training loss 0.799
Epoch 14 iteration 0460/1263: training loss 0.799
Epoch 14 iteration 0480/1263: training loss 0.796
Epoch 14 iteration 0500/1263: training loss 0.796
Epoch 14 iteration 0520/1263: training loss 0.797
Epoch 14 iteration 0540/1263: training loss 0.796
Epoch 14 iteration 0560/1263: training loss 0.796
Epoch 14 iteration 0580/1263: training loss 0.797
Epoch 14 iteration 0600/1263: training loss 0.795
Epoch 14 iteration 0620/1263: training loss 0.798
Epoch 14 iteration 0640/1263: training loss 0.799
Epoch 14 iteration 0660/1263: training loss 0.800
Epoch 14 iteration 0680/1263: training loss 0.800
Epoch 14 iteration 0700/1263: training loss 0.798
Epoch 14 iteration 0720/1263: training loss 0.797
Epoch 14 iteration 0740/1263: training loss 0.796
Epoch 14 iteration 0760/1263: training loss 0.795
Epoch 14 iteration 0780/1263: training loss 0.794
Epoch 14 iteration 0800/1263: training loss 0.796
Epoch 14 iteration 0820/1263: training loss 0.795
Epoch 14 iteration 0840/1263: training loss 0.796
Epoch 14 iteration 0860/1263: training loss 0.794
Epoch 14 iteration 0880/1263: training loss 0.793
Epoch 14 iteration 0900/1263: training loss 0.794
Epoch 14 iteration 0920/1263: training loss 0.794
Epoch 14 iteration 0940/1263: training loss 0.795
Epoch 14 iteration 0960/1263: training loss 0.796
Epoch 14 iteration 0980/1263: training loss 0.795
Epoch 14 iteration 1000/1263: training loss 0.793
Epoch 14 iteration 1020/1263: training loss 0.794
Epoch 14 iteration 1040/1263: training loss 0.793
Epoch 14 iteration 1060/1263: training loss 0.794
Epoch 14 iteration 1080/1263: training loss 0.794
Epoch 14 iteration 1100/1263: training loss 0.794
Epoch 14 iteration 1120/1263: training loss 0.793
Epoch 14 iteration 1140/1263: training loss 0.795
Epoch 14 iteration 1160/1263: training loss 0.799
Epoch 14 iteration 1180/1264: training loss 0.799
Epoch 14 iteration 1200/1264: training loss 0.799
Epoch 14 iteration 1220/1264: training loss 0.799
Epoch 14 iteration 1240/1264: training loss 0.798
Epoch 14 iteration 1260/1264: training loss 0.799
Epoch 14 validation pixAcc: 0.761, mIoU: 0.380
Epoch 15 iteration 0020/1263: training loss 0.771
Epoch 15 iteration 0040/1263: training loss 0.769
Epoch 15 iteration 0060/1263: training loss 0.771
Epoch 15 iteration 0080/1263: training loss 0.754
Epoch 15 iteration 0100/1263: training loss 0.773
Epoch 15 iteration 0120/1263: training loss 0.780
Epoch 15 iteration 0140/1263: training loss 0.783
Epoch 15 iteration 0160/1263: training loss 0.786
Epoch 15 iteration 0180/1263: training loss 0.785
Epoch 15 iteration 0200/1263: training loss 0.785
Epoch 15 iteration 0220/1263: training loss 0.795
Epoch 15 iteration 0240/1263: training loss 0.794
Epoch 15 iteration 0260/1263: training loss 0.791
Epoch 15 iteration 0280/1263: training loss 0.790
Epoch 15 iteration 0300/1263: training loss 0.788
Epoch 15 iteration 0320/1263: training loss 0.788
Epoch 15 iteration 0340/1263: training loss 0.789
Epoch 15 iteration 0360/1263: training loss 0.789
Epoch 15 iteration 0380/1263: training loss 0.787
Epoch 15 iteration 0400/1263: training loss 0.787
Epoch 15 iteration 0420/1263: training loss 0.785
Epoch 15 iteration 0440/1263: training loss 0.786
Epoch 15 iteration 0460/1263: training loss 0.785
Epoch 15 iteration 0480/1263: training loss 0.790
Epoch 15 iteration 0500/1263: training loss 0.792
Epoch 15 iteration 0520/1263: training loss 0.790
Epoch 15 iteration 0540/1263: training loss 0.789
Epoch 15 iteration 0560/1263: training loss 0.789
Epoch 15 iteration 0580/1263: training loss 0.791
Epoch 15 iteration 0600/1263: training loss 0.795
Epoch 15 iteration 0620/1263: training loss 0.797
Epoch 15 iteration 0640/1263: training loss 0.797
Epoch 15 iteration 0660/1263: training loss 0.797
Epoch 15 iteration 0680/1263: training loss 0.797
Epoch 15 iteration 0700/1263: training loss 0.797
Epoch 15 iteration 0720/1263: training loss 0.795
Epoch 15 iteration 0740/1263: training loss 0.794
Epoch 15 iteration 0760/1263: training loss 0.794
Epoch 15 iteration 0780/1263: training loss 0.792
Epoch 15 iteration 0800/1263: training loss 0.793
Epoch 15 iteration 0820/1263: training loss 0.793
Epoch 15 iteration 0840/1263: training loss 0.791
Epoch 15 iteration 0860/1263: training loss 0.790
Epoch 15 iteration 0880/1263: training loss 0.789
Epoch 15 iteration 0900/1263: training loss 0.788
Epoch 15 iteration 0920/1263: training loss 0.790
Epoch 15 iteration 0940/1263: training loss 0.790
Epoch 15 iteration 0960/1263: training loss 0.789
Epoch 15 iteration 0980/1263: training loss 0.789
Epoch 15 iteration 1000/1263: training loss 0.788
Epoch 15 iteration 1020/1263: training loss 0.788
Epoch 15 iteration 1040/1263: training loss 0.788
Epoch 15 iteration 1060/1263: training loss 0.788
Epoch 15 iteration 1080/1263: training loss 0.789
Epoch 15 iteration 1100/1263: training loss 0.788
Epoch 15 iteration 1120/1263: training loss 0.790
Epoch 15 iteration 1140/1263: training loss 0.791
Epoch 15 iteration 1160/1263: training loss 0.792
Epoch 15 iteration 1180/1263: training loss 0.792
Epoch 15 iteration 1200/1263: training loss 0.792
Epoch 15 iteration 1220/1263: training loss 0.792
Epoch 15 iteration 1240/1263: training loss 0.795
Epoch 15 iteration 1260/1263: training loss 0.796
Epoch 15 validation pixAcc: 0.761, mIoU: 0.359
Epoch 16 iteration 0020/1263: training loss 0.696
Epoch 16 iteration 0040/1263: training loss 0.690
Epoch 16 iteration 0060/1263: training loss 0.717
Epoch 16 iteration 0080/1263: training loss 0.722
Epoch 16 iteration 0100/1263: training loss 0.711
Epoch 16 iteration 0120/1263: training loss 0.719
Epoch 16 iteration 0140/1263: training loss 0.724
Epoch 16 iteration 0160/1263: training loss 0.730
Epoch 16 iteration 0180/1263: training loss 0.732
Epoch 16 iteration 0200/1263: training loss 0.740
Epoch 16 iteration 0220/1263: training loss 0.737
Epoch 16 iteration 0240/1263: training loss 0.739
Epoch 16 iteration 0260/1263: training loss 0.737
Epoch 16 iteration 0280/1263: training loss 0.736
Epoch 16 iteration 0300/1263: training loss 0.736
Epoch 16 iteration 0320/1263: training loss 0.736
Epoch 16 iteration 0340/1263: training loss 0.738
Epoch 16 iteration 0360/1263: training loss 0.738
Epoch 16 iteration 0380/1263: training loss 0.741
Epoch 16 iteration 0400/1263: training loss 0.739
Epoch 16 iteration 0420/1263: training loss 0.743
Epoch 16 iteration 0440/1263: training loss 0.745
Epoch 16 iteration 0460/1263: training loss 0.743
Epoch 16 iteration 0480/1263: training loss 0.746
Epoch 16 iteration 0500/1263: training loss 0.745
Epoch 16 iteration 0520/1263: training loss 0.746
Epoch 16 iteration 0540/1263: training loss 0.746
Epoch 16 iteration 0560/1263: training loss 0.749
Epoch 16 iteration 0580/1263: training loss 0.749
Epoch 16 iteration 0600/1263: training loss 0.748
Epoch 16 iteration 0620/1263: training loss 0.746
Epoch 16 iteration 0640/1263: training loss 0.747
Epoch 16 iteration 0660/1263: training loss 0.748
Epoch 16 iteration 0680/1263: training loss 0.751
Epoch 16 iteration 0700/1263: training loss 0.751
Epoch 16 iteration 0720/1263: training loss 0.752
Epoch 16 iteration 0740/1263: training loss 0.752
Epoch 16 iteration 0760/1263: training loss 0.752
Epoch 16 iteration 0780/1263: training loss 0.750
Epoch 16 iteration 0800/1263: training loss 0.750
Epoch 16 iteration 0820/1263: training loss 0.750
Epoch 16 iteration 0840/1263: training loss 0.749
Epoch 16 iteration 0860/1263: training loss 0.750
Epoch 16 iteration 0880/1263: training loss 0.750
Epoch 16 iteration 0900/1263: training loss 0.749
Epoch 16 iteration 0920/1263: training loss 0.749
Epoch 16 iteration 0940/1263: training loss 0.749
Epoch 16 iteration 0960/1263: training loss 0.750
Epoch 16 iteration 0980/1263: training loss 0.750
Epoch 16 iteration 1000/1263: training loss 0.751
Epoch 16 iteration 1020/1263: training loss 0.750
Epoch 16 iteration 1040/1263: training loss 0.749
Epoch 16 iteration 1060/1263: training loss 0.749
Epoch 16 iteration 1080/1263: training loss 0.750
Epoch 16 iteration 1100/1263: training loss 0.752
Epoch 16 iteration 1120/1263: training loss 0.753
Epoch 16 iteration 1140/1263: training loss 0.753
Epoch 16 iteration 1160/1263: training loss 0.754
Epoch 16 iteration 1180/1263: training loss 0.757
Epoch 16 iteration 1200/1263: training loss 0.759
Epoch 16 iteration 1220/1263: training loss 0.760
Epoch 16 iteration 1240/1263: training loss 0.760
Epoch 16 iteration 1260/1263: training loss 0.761
Epoch 16 validation pixAcc: 0.764, mIoU: 0.386
Epoch 17 iteration 0020/1263: training loss 0.766
Epoch 17 iteration 0040/1263: training loss 0.752
Epoch 17 iteration 0060/1263: training loss 0.742
Epoch 17 iteration 0080/1263: training loss 0.728
Epoch 17 iteration 0100/1263: training loss 0.732
Epoch 17 iteration 0120/1263: training loss 0.735
Epoch 17 iteration 0140/1263: training loss 0.726
Epoch 17 iteration 0160/1263: training loss 0.722
Epoch 17 iteration 0180/1263: training loss 0.719
Epoch 17 iteration 0200/1263: training loss 0.718
Epoch 17 iteration 0220/1263: training loss 0.719
Epoch 17 iteration 0240/1263: training loss 0.720
Epoch 17 iteration 0260/1263: training loss 0.722
Epoch 17 iteration 0280/1263: training loss 0.723
Epoch 17 iteration 0300/1263: training loss 0.721
Epoch 17 iteration 0320/1263: training loss 0.720
Epoch 17 iteration 0340/1263: training loss 0.720
Epoch 17 iteration 0360/1263: training loss 0.721
Epoch 17 iteration 0380/1263: training loss 0.721
Epoch 17 iteration 0400/1263: training loss 0.722
Epoch 17 iteration 0420/1263: training loss 0.720
Epoch 17 iteration 0440/1263: training loss 0.719
Epoch 17 iteration 0460/1263: training loss 0.721
Epoch 17 iteration 0480/1263: training loss 0.721
Epoch 17 iteration 0500/1263: training loss 0.720
Epoch 17 iteration 0520/1263: training loss 0.721
Epoch 17 iteration 0540/1263: training loss 0.723
Epoch 17 iteration 0560/1263: training loss 0.724
Epoch 17 iteration 0580/1263: training loss 0.724
Epoch 17 iteration 0600/1263: training loss 0.724
Epoch 17 iteration 0620/1263: training loss 0.724
Epoch 17 iteration 0640/1263: training loss 0.725
Epoch 17 iteration 0660/1263: training loss 0.725
Epoch 17 iteration 0680/1263: training loss 0.727
Epoch 17 iteration 0700/1263: training loss 0.729
Epoch 17 iteration 0720/1263: training loss 0.729
Epoch 17 iteration 0740/1263: training loss 0.730
Epoch 17 iteration 0760/1263: training loss 0.731
Epoch 17 iteration 0780/1263: training loss 0.733
Epoch 17 iteration 0800/1263: training loss 0.734
Epoch 17 iteration 0820/1263: training loss 0.736
Epoch 17 iteration 0840/1263: training loss 0.736
Epoch 17 iteration 0860/1263: training loss 0.735
Epoch 17 iteration 0880/1263: training loss 0.734
Epoch 17 iteration 0900/1263: training loss 0.735
Epoch 17 iteration 0920/1263: training loss 0.734
Epoch 17 iteration 0940/1263: training loss 0.733
Epoch 17 iteration 0960/1263: training loss 0.734
Epoch 17 iteration 0980/1263: training loss 0.734
Epoch 17 iteration 1000/1263: training loss 0.735
Epoch 17 iteration 1020/1263: training loss 0.734
Epoch 17 iteration 1040/1263: training loss 0.735
Epoch 17 iteration 1060/1263: training loss 0.735
Epoch 17 iteration 1080/1263: training loss 0.735
Epoch 17 iteration 1100/1263: training loss 0.736
Epoch 17 iteration 1120/1263: training loss 0.735
Epoch 17 iteration 1140/1263: training loss 0.735
Epoch 17 iteration 1160/1263: training loss 0.736
Epoch 17 iteration 1180/1263: training loss 0.736
Epoch 17 iteration 1200/1263: training loss 0.737
Epoch 17 iteration 1220/1263: training loss 0.738
Epoch 17 iteration 1240/1263: training loss 0.737
Epoch 17 iteration 1260/1263: training loss 0.737
Epoch 17 validation pixAcc: 0.773, mIoU: 0.404
Epoch 18 iteration 0020/1263: training loss 0.684
Epoch 18 iteration 0040/1263: training loss 0.700
Epoch 18 iteration 0060/1263: training loss 0.713
Epoch 18 iteration 0080/1263: training loss 0.707
Epoch 18 iteration 0100/1263: training loss 0.698
Epoch 18 iteration 0120/1263: training loss 0.689
Epoch 18 iteration 0140/1263: training loss 0.684
Epoch 18 iteration 0160/1263: training loss 0.691
Epoch 18 iteration 0180/1263: training loss 0.696
Epoch 18 iteration 0200/1263: training loss 0.702
Epoch 18 iteration 0220/1263: training loss 0.701
Epoch 18 iteration 0240/1263: training loss 0.701
Epoch 18 iteration 0260/1263: training loss 0.704
Epoch 18 iteration 0280/1263: training loss 0.705
Epoch 18 iteration 0300/1263: training loss 0.703
Epoch 18 iteration 0320/1263: training loss 0.705
Epoch 18 iteration 0340/1263: training loss 0.709
Epoch 18 iteration 0360/1263: training loss 0.709
Epoch 18 iteration 0380/1263: training loss 0.709
Epoch 18 iteration 0400/1263: training loss 0.710
Epoch 18 iteration 0420/1263: training loss 0.708
Epoch 18 iteration 0440/1263: training loss 0.708
Epoch 18 iteration 0460/1263: training loss 0.709
Epoch 18 iteration 0480/1263: training loss 0.711
Epoch 18 iteration 0500/1263: training loss 0.709
Epoch 18 iteration 0520/1263: training loss 0.709
Epoch 18 iteration 0540/1263: training loss 0.710
Epoch 18 iteration 0560/1263: training loss 0.711
Epoch 18 iteration 0580/1263: training loss 0.712
Epoch 18 iteration 0600/1263: training loss 0.711
Epoch 18 iteration 0620/1263: training loss 0.711
Epoch 18 iteration 0640/1263: training loss 0.710
Epoch 18 iteration 0660/1263: training loss 0.710
Epoch 18 iteration 0680/1263: training loss 0.709
Epoch 18 iteration 0700/1263: training loss 0.709
Epoch 18 iteration 0720/1263: training loss 0.710
Epoch 18 iteration 0740/1263: training loss 0.712
Epoch 18 iteration 0760/1263: training loss 0.712
Epoch 18 iteration 0780/1263: training loss 0.712
Epoch 18 iteration 0800/1263: training loss 0.713
Epoch 18 iteration 0820/1263: training loss 0.716
Epoch 18 iteration 0840/1263: training loss 0.717
Epoch 18 iteration 0860/1263: training loss 0.717
Epoch 18 iteration 0880/1263: training loss 0.717
Epoch 18 iteration 0900/1263: training loss 0.718
Epoch 18 iteration 0920/1263: training loss 0.718
Epoch 18 iteration 0940/1263: training loss 0.718
Epoch 18 iteration 0960/1263: training loss 0.720
Epoch 18 iteration 0980/1263: training loss 0.721
Epoch 18 iteration 1000/1263: training loss 0.721
Epoch 18 iteration 1020/1263: training loss 0.721
Epoch 18 iteration 1040/1263: training loss 0.721
Epoch 18 iteration 1060/1263: training loss 0.721
Epoch 18 iteration 1080/1263: training loss 0.723
Epoch 18 iteration 1100/1263: training loss 0.724
Epoch 18 iteration 1120/1263: training loss 0.724
Epoch 18 iteration 1140/1263: training loss 0.725
Epoch 18 iteration 1160/1263: training loss 0.725
Epoch 18 iteration 1180/1263: training loss 0.724
Epoch 18 iteration 1200/1263: training loss 0.725
Epoch 18 iteration 1220/1263: training loss 0.725
Epoch 18 iteration 1240/1263: training loss 0.726
Epoch 18 iteration 1260/1263: training loss 0.727
Epoch 18 validation pixAcc: 0.777, mIoU: 0.396
Epoch 19 iteration 0020/1263: training loss 0.763
Epoch 19 iteration 0040/1263: training loss 0.752
Epoch 19 iteration 0060/1263: training loss 0.750
Epoch 19 iteration 0080/1263: training loss 0.752
Epoch 19 iteration 0100/1263: training loss 0.745
Epoch 19 iteration 0120/1263: training loss 0.736
Epoch 19 iteration 0140/1263: training loss 0.720
Epoch 19 iteration 0160/1263: training loss 0.719
Epoch 19 iteration 0180/1263: training loss 0.718
Epoch 19 iteration 0200/1263: training loss 0.711
Epoch 19 iteration 0220/1263: training loss 0.705
Epoch 19 iteration 0240/1263: training loss 0.703
Epoch 19 iteration 0260/1263: training loss 0.705
Epoch 19 iteration 0280/1263: training loss 0.704
Epoch 19 iteration 0300/1263: training loss 0.700
Epoch 19 iteration 0320/1263: training loss 0.697
Epoch 19 iteration 0340/1263: training loss 0.697
Epoch 19 iteration 0360/1263: training loss 0.701
Epoch 19 iteration 0380/1263: training loss 0.701
Epoch 19 iteration 0400/1263: training loss 0.701
Epoch 19 iteration 0420/1263: training loss 0.704
Epoch 19 iteration 0440/1263: training loss 0.706
Epoch 19 iteration 0460/1263: training loss 0.710
Epoch 19 iteration 0480/1263: training loss 0.710
Epoch 19 iteration 0500/1263: training loss 0.710
Epoch 19 iteration 0520/1263: training loss 0.713
Epoch 19 iteration 0540/1263: training loss 0.713
Epoch 19 iteration 0560/1263: training loss 0.711
Epoch 19 iteration 0580/1263: training loss 0.713
Epoch 19 iteration 0600/1263: training loss 0.711
Epoch 19 iteration 0620/1263: training loss 0.711
Epoch 19 iteration 0640/1263: training loss 0.710
Epoch 19 iteration 0660/1263: training loss 0.710
Epoch 19 iteration 0680/1263: training loss 0.709
Epoch 19 iteration 0700/1263: training loss 0.710
Epoch 19 iteration 0720/1263: training loss 0.710
Epoch 19 iteration 0740/1263: training loss 0.709
Epoch 19 iteration 0760/1263: training loss 0.710
Epoch 19 iteration 0780/1263: training loss 0.713
Epoch 19 iteration 0800/1263: training loss 0.716
Epoch 19 iteration 0820/1263: training loss 0.719
Epoch 19 iteration 0840/1263: training loss 0.719
Epoch 19 iteration 0860/1263: training loss 0.721
Epoch 19 iteration 0880/1263: training loss 0.722
Epoch 19 iteration 0900/1263: training loss 0.723
Epoch 19 iteration 0920/1263: training loss 0.726
Epoch 19 iteration 0940/1263: training loss 0.727
Epoch 19 iteration 0960/1263: training loss 0.727
Epoch 19 iteration 0980/1263: training loss 0.727
Epoch 19 iteration 1000/1263: training loss 0.727
Epoch 19 iteration 1020/1263: training loss 0.727
Epoch 19 iteration 1040/1263: training loss 0.728
Epoch 19 iteration 1060/1263: training loss 0.729
Epoch 19 iteration 1080/1263: training loss 0.730
Epoch 19 iteration 1100/1263: training loss 0.729
Epoch 19 iteration 1120/1263: training loss 0.728
Epoch 19 iteration 1140/1263: training loss 0.728
Epoch 19 iteration 1160/1263: training loss 0.727
Epoch 19 iteration 1180/1263: training loss 0.727
Epoch 19 iteration 1200/1263: training loss 0.726
Epoch 19 iteration 1220/1263: training loss 0.725
Epoch 19 iteration 1240/1263: training loss 0.726
Epoch 19 iteration 1260/1263: training loss 0.726
Epoch 19 validation pixAcc: 0.770, mIoU: 0.387
Epoch 20 iteration 0020/1263: training loss 0.641
Epoch 20 iteration 0040/1263: training loss 0.652
Epoch 20 iteration 0060/1263: training loss 0.656
Epoch 20 iteration 0080/1263: training loss 0.667
Epoch 20 iteration 0100/1263: training loss 0.681
Epoch 20 iteration 0120/1263: training loss 0.690
Epoch 20 iteration 0140/1263: training loss 0.693
Epoch 20 iteration 0160/1263: training loss 0.700
Epoch 20 iteration 0180/1263: training loss 0.706
Epoch 20 iteration 0200/1263: training loss 0.710
Epoch 20 iteration 0220/1263: training loss 0.707
Epoch 20 iteration 0240/1263: training loss 0.709
Epoch 20 iteration 0260/1263: training loss 0.717
Epoch 20 iteration 0280/1263: training loss 0.719
Epoch 20 iteration 0300/1263: training loss 0.719
Epoch 20 iteration 0320/1263: training loss 0.721
Epoch 20 iteration 0340/1263: training loss 0.721
Epoch 20 iteration 0360/1263: training loss 0.723
Epoch 20 iteration 0380/1263: training loss 0.718
Epoch 20 iteration 0400/1263: training loss 0.717
Epoch 20 iteration 0420/1263: training loss 0.715
Epoch 20 iteration 0440/1263: training loss 0.714
Epoch 20 iteration 0460/1263: training loss 0.714
Epoch 20 iteration 0480/1263: training loss 0.712
Epoch 20 iteration 0500/1263: training loss 0.711
Epoch 20 iteration 0520/1263: training loss 0.708
Epoch 20 iteration 0540/1263: training loss 0.710
Epoch 20 iteration 0560/1263: training loss 0.708
Epoch 20 iteration 0580/1263: training loss 0.709
Epoch 20 iteration 0600/1263: training loss 0.708
Epoch 20 iteration 0620/1263: training loss 0.707
Epoch 20 iteration 0640/1263: training loss 0.710
Epoch 20 iteration 0660/1263: training loss 0.712
Epoch 20 iteration 0680/1263: training loss 0.713
Epoch 20 iteration 0700/1263: training loss 0.714
Epoch 20 iteration 0720/1263: training loss 0.714
Epoch 20 iteration 0740/1263: training loss 0.713
Epoch 20 iteration 0760/1263: training loss 0.711
Epoch 20 iteration 0780/1263: training loss 0.712
Epoch 20 iteration 0800/1263: training loss 0.713
Epoch 20 iteration 0820/1263: training loss 0.715
Epoch 20 iteration 0840/1263: training loss 0.714
Epoch 20 iteration 0860/1263: training loss 0.715
Epoch 20 iteration 0880/1263: training loss 0.715
Epoch 20 iteration 0900/1263: training loss 0.715
Epoch 20 iteration 0920/1263: training loss 0.716
Epoch 20 iteration 0940/1263: training loss 0.717
Epoch 20 iteration 0960/1263: training loss 0.718
Epoch 20 iteration 0980/1263: training loss 0.718
Epoch 20 iteration 1000/1263: training loss 0.718
Epoch 20 iteration 1020/1263: training loss 0.719
Epoch 20 iteration 1040/1263: training loss 0.718
Epoch 20 iteration 1060/1263: training loss 0.718
Epoch 20 iteration 1080/1263: training loss 0.717
Epoch 20 iteration 1100/1263: training loss 0.716
Epoch 20 iteration 1120/1263: training loss 0.716
Epoch 20 iteration 1140/1263: training loss 0.717
Epoch 20 iteration 1160/1263: training loss 0.718
Epoch 20 iteration 1180/1263: training loss 0.718
Epoch 20 iteration 1200/1263: training loss 0.718
Epoch 20 iteration 1220/1263: training loss 0.717
Epoch 20 iteration 1240/1263: training loss 0.718
Epoch 20 iteration 1260/1263: training loss 0.719
Epoch 20 validation pixAcc: 0.770, mIoU: 0.390
Epoch 21 iteration 0020/1263: training loss 0.718
Epoch 21 iteration 0040/1263: training loss 0.716
Epoch 21 iteration 0060/1263: training loss 0.702
Epoch 21 iteration 0080/1263: training loss 0.696
Epoch 21 iteration 0100/1263: training loss 0.697
Epoch 21 iteration 0120/1263: training loss 0.689
Epoch 21 iteration 0140/1263: training loss 0.680
Epoch 21 iteration 0160/1263: training loss 0.676
Epoch 21 iteration 0180/1263: training loss 0.669
Epoch 21 iteration 0200/1263: training loss 0.668
Epoch 21 iteration 0220/1263: training loss 0.666
Epoch 21 iteration 0240/1263: training loss 0.672
Epoch 21 iteration 0260/1263: training loss 0.671
Epoch 21 iteration 0280/1263: training loss 0.670
Epoch 21 iteration 0300/1263: training loss 0.669
Epoch 21 iteration 0320/1263: training loss 0.668
Epoch 21 iteration 0340/1263: training loss 0.672
Epoch 21 iteration 0360/1263: training loss 0.673
Epoch 21 iteration 0380/1263: training loss 0.673
Epoch 21 iteration 0400/1263: training loss 0.673
Epoch 21 iteration 0420/1263: training loss 0.672
Epoch 21 iteration 0440/1263: training loss 0.673
Epoch 21 iteration 0460/1263: training loss 0.672
Epoch 21 iteration 0480/1263: training loss 0.672
Epoch 21 iteration 0500/1263: training loss 0.675
Epoch 21 iteration 0520/1263: training loss 0.677
Epoch 21 iteration 0540/1263: training loss 0.678
Epoch 21 iteration 0560/1263: training loss 0.679
Epoch 21 iteration 0580/1263: training loss 0.677
Epoch 21 iteration 0600/1263: training loss 0.676
Epoch 21 iteration 0620/1263: training loss 0.677
Epoch 21 iteration 0640/1263: training loss 0.677
Epoch 21 iteration 0660/1263: training loss 0.677
Epoch 21 iteration 0680/1263: training loss 0.677
Epoch 21 iteration 0700/1263: training loss 0.679
Epoch 21 iteration 0720/1263: training loss 0.678
Epoch 21 iteration 0740/1263: training loss 0.678
Epoch 21 iteration 0760/1263: training loss 0.678
Epoch 21 iteration 0780/1263: training loss 0.678
Epoch 21 iteration 0800/1263: training loss 0.678
Epoch 21 iteration 0820/1263: training loss 0.678
Epoch 21 iteration 0840/1263: training loss 0.680
Epoch 21 iteration 0860/1263: training loss 0.680
Epoch 21 iteration 0880/1263: training loss 0.679
Epoch 21 iteration 0900/1263: training loss 0.678
Epoch 21 iteration 0920/1263: training loss 0.678
Epoch 21 iteration 0940/1263: training loss 0.679
Epoch 21 iteration 0960/1263: training loss 0.680
Epoch 21 iteration 0980/1263: training loss 0.681
Epoch 21 iteration 1000/1263: training loss 0.681
Epoch 21 iteration 1020/1263: training loss 0.682
Epoch 21 iteration 1040/1263: training loss 0.681
Epoch 21 iteration 1060/1263: training loss 0.681
Epoch 21 iteration 1080/1263: training loss 0.682
Epoch 21 iteration 1100/1263: training loss 0.683
Epoch 21 iteration 1120/1263: training loss 0.684
Epoch 21 iteration 1140/1263: training loss 0.684
Epoch 21 iteration 1160/1263: training loss 0.685
Epoch 21 iteration 1180/1263: training loss 0.685
Epoch 21 iteration 1200/1263: training loss 0.685
Epoch 21 iteration 1220/1263: training loss 0.685
Epoch 21 iteration 1240/1263: training loss 0.685
Epoch 21 iteration 1260/1263: training loss 0.686
Epoch 21 validation pixAcc: 0.774, mIoU: 0.397
Epoch 22 iteration 0020/1263: training loss 0.626
Epoch 22 iteration 0040/1263: training loss 0.604
Epoch 22 iteration 0060/1263: training loss 0.588
Epoch 22 iteration 0080/1263: training loss 0.602
Epoch 22 iteration 0100/1263: training loss 0.603
Epoch 22 iteration 0120/1263: training loss 0.619
Epoch 22 iteration 0140/1263: training loss 0.623
Epoch 22 iteration 0160/1263: training loss 0.619
Epoch 22 iteration 0180/1263: training loss 0.624
Epoch 22 iteration 0200/1263: training loss 0.622
Epoch 22 iteration 0220/1263: training loss 0.625
Epoch 22 iteration 0240/1263: training loss 0.629
Epoch 22 iteration 0260/1263: training loss 0.634
Epoch 22 iteration 0280/1263: training loss 0.641
Epoch 22 iteration 0300/1263: training loss 0.641
Epoch 22 iteration 0320/1263: training loss 0.639
Epoch 22 iteration 0340/1263: training loss 0.641
Epoch 22 iteration 0360/1263: training loss 0.643
Epoch 22 iteration 0380/1263: training loss 0.647
Epoch 22 iteration 0400/1263: training loss 0.647
Epoch 22 iteration 0420/1263: training loss 0.648
Epoch 22 iteration 0440/1263: training loss 0.650
Epoch 22 iteration 0460/1263: training loss 0.652
Epoch 22 iteration 0480/1263: training loss 0.652
Epoch 22 iteration 0500/1263: training loss 0.653
Epoch 22 iteration 0520/1263: training loss 0.654
Epoch 22 iteration 0540/1263: training loss 0.654
Epoch 22 iteration 0560/1263: training loss 0.657
Epoch 22 iteration 0580/1263: training loss 0.659
Epoch 22 iteration 0600/1263: training loss 0.659
Epoch 22 iteration 0620/1263: training loss 0.659
Epoch 22 iteration 0640/1263: training loss 0.657
Epoch 22 iteration 0660/1263: training loss 0.656
Epoch 22 iteration 0680/1263: training loss 0.657
Epoch 22 iteration 0700/1263: training loss 0.658
Epoch 22 iteration 0720/1263: training loss 0.658
Epoch 22 iteration 0740/1263: training loss 0.657
Epoch 22 iteration 0760/1263: training loss 0.656
Epoch 22 iteration 0780/1263: training loss 0.658
Epoch 22 iteration 0800/1263: training loss 0.658
Epoch 22 iteration 0820/1263: training loss 0.657
Epoch 22 iteration 0840/1263: training loss 0.658
Epoch 22 iteration 0860/1263: training loss 0.658
Epoch 22 iteration 0880/1263: training loss 0.658
Epoch 22 iteration 0900/1263: training loss 0.657
Epoch 22 iteration 0920/1263: training loss 0.657
Epoch 22 iteration 0940/1263: training loss 0.660
Epoch 22 iteration 0960/1263: training loss 0.662
Epoch 22 iteration 0980/1263: training loss 0.663
Epoch 22 iteration 1000/1263: training loss 0.662
Epoch 22 iteration 1020/1263: training loss 0.662
Epoch 22 iteration 1040/1263: training loss 0.663
Epoch 22 iteration 1060/1263: training loss 0.664
Epoch 22 iteration 1080/1263: training loss 0.663
Epoch 22 iteration 1100/1263: training loss 0.662
Epoch 22 iteration 1120/1263: training loss 0.662
Epoch 22 iteration 1140/1263: training loss 0.663
Epoch 22 iteration 1160/1263: training loss 0.665
Epoch 22 iteration 1180/1264: training loss 0.666
Epoch 22 iteration 1200/1264: training loss 0.666
Epoch 22 iteration 1220/1264: training loss 0.667
Epoch 22 iteration 1240/1264: training loss 0.669
Epoch 22 iteration 1260/1264: training loss 0.669
Epoch 22 validation pixAcc: 0.764, mIoU: 0.389
Epoch 23 iteration 0020/1263: training loss 0.671
Epoch 23 iteration 0040/1263: training loss 0.670
Epoch 23 iteration 0060/1263: training loss 0.671
Epoch 23 iteration 0080/1263: training loss 0.670
Epoch 23 iteration 0100/1263: training loss 0.665
Epoch 23 iteration 0120/1263: training loss 0.655
Epoch 23 iteration 0140/1263: training loss 0.659
Epoch 23 iteration 0160/1263: training loss 0.655
Epoch 23 iteration 0180/1263: training loss 0.653
Epoch 23 iteration 0200/1263: training loss 0.651
Epoch 23 iteration 0220/1263: training loss 0.656
Epoch 23 iteration 0240/1263: training loss 0.665
Epoch 23 iteration 0260/1263: training loss 0.663
Epoch 23 iteration 0280/1263: training loss 0.660
Epoch 23 iteration 0300/1263: training loss 0.659
Epoch 23 iteration 0320/1263: training loss 0.659
Epoch 23 iteration 0340/1263: training loss 0.659
Epoch 23 iteration 0360/1263: training loss 0.658
Epoch 23 iteration 0380/1263: training loss 0.655
Epoch 23 iteration 0400/1263: training loss 0.661
Epoch 23 iteration 0420/1263: training loss 0.668
Epoch 23 iteration 0440/1263: training loss 0.669
Epoch 23 iteration 0460/1263: training loss 0.668
Epoch 23 iteration 0480/1263: training loss 0.667
Epoch 23 iteration 0500/1263: training loss 0.663
Epoch 23 iteration 0520/1263: training loss 0.662
Epoch 23 iteration 0540/1263: training loss 0.661
Epoch 23 iteration 0560/1263: training loss 0.657
Epoch 23 iteration 0580/1263: training loss 0.659
Epoch 23 iteration 0600/1263: training loss 0.661
Epoch 23 iteration 0620/1263: training loss 0.660
Epoch 23 iteration 0640/1263: training loss 0.660
Epoch 23 iteration 0660/1263: training loss 0.659
Epoch 23 iteration 0680/1263: training loss 0.658
Epoch 23 iteration 0700/1263: training loss 0.658
Epoch 23 iteration 0720/1263: training loss 0.660
Epoch 23 iteration 0740/1263: training loss 0.661
Epoch 23 iteration 0760/1263: training loss 0.663
Epoch 23 iteration 0780/1263: training loss 0.663
Epoch 23 iteration 0800/1263: training loss 0.664
Epoch 23 iteration 0820/1263: training loss 0.664
Epoch 23 iteration 0840/1263: training loss 0.664
Epoch 23 iteration 0860/1263: training loss 0.662
Epoch 23 iteration 0880/1263: training loss 0.662
Epoch 23 iteration 0900/1263: training loss 0.663
Epoch 23 iteration 0920/1263: training loss 0.663
Epoch 23 iteration 0940/1263: training loss 0.664
Epoch 23 iteration 0960/1263: training loss 0.664
Epoch 23 iteration 0980/1263: training loss 0.665
Epoch 23 iteration 1000/1263: training loss 0.665
Epoch 23 iteration 1020/1263: training loss 0.664
Epoch 23 iteration 1040/1263: training loss 0.665
Epoch 23 iteration 1060/1263: training loss 0.665
Epoch 23 iteration 1080/1263: training loss 0.665
Epoch 23 iteration 1100/1263: training loss 0.665
Epoch 23 iteration 1120/1263: training loss 0.665
Epoch 23 iteration 1140/1263: training loss 0.665
Epoch 23 iteration 1160/1263: training loss 0.667
Epoch 23 iteration 1180/1263: training loss 0.668
Epoch 23 iteration 1200/1263: training loss 0.668
Epoch 23 iteration 1220/1263: training loss 0.667
Epoch 23 iteration 1240/1263: training loss 0.667
Epoch 23 iteration 1260/1263: training loss 0.667
Epoch 23 validation pixAcc: 0.779, mIoU: 0.407
Epoch 24 iteration 0020/1263: training loss 0.639
Epoch 24 iteration 0040/1263: training loss 0.605
Epoch 24 iteration 0060/1263: training loss 0.605
Epoch 24 iteration 0080/1263: training loss 0.610
Epoch 24 iteration 0100/1263: training loss 0.607
Epoch 24 iteration 0120/1263: training loss 0.601
Epoch 24 iteration 0140/1263: training loss 0.593
Epoch 24 iteration 0160/1263: training loss 0.593
Epoch 24 iteration 0180/1263: training loss 0.602
Epoch 24 iteration 0200/1263: training loss 0.604
Epoch 24 iteration 0220/1263: training loss 0.610
Epoch 24 iteration 0240/1263: training loss 0.614
Epoch 24 iteration 0260/1263: training loss 0.611
Epoch 24 iteration 0280/1263: training loss 0.612
Epoch 24 iteration 0300/1263: training loss 0.611
Epoch 24 iteration 0320/1263: training loss 0.609
Epoch 24 iteration 0340/1263: training loss 0.617
Epoch 24 iteration 0360/1263: training loss 0.619
Epoch 24 iteration 0380/1263: training loss 0.621
Epoch 24 iteration 0400/1263: training loss 0.627
Epoch 24 iteration 0420/1263: training loss 0.626
Epoch 24 iteration 0440/1263: training loss 0.630
Epoch 24 iteration 0460/1263: training loss 0.632
Epoch 24 iteration 0480/1263: training loss 0.631
Epoch 24 iteration 0500/1263: training loss 0.633
Epoch 24 iteration 0520/1263: training loss 0.633
Epoch 24 iteration 0540/1263: training loss 0.634
Epoch 24 iteration 0560/1263: training loss 0.636
Epoch 24 iteration 0580/1263: training loss 0.635
Epoch 24 iteration 0600/1263: training loss 0.636
Epoch 24 iteration 0620/1263: training loss 0.637
Epoch 24 iteration 0640/1263: training loss 0.638
Epoch 24 iteration 0660/1263: training loss 0.637
Epoch 24 iteration 0680/1263: training loss 0.637
Epoch 24 iteration 0700/1263: training loss 0.637
Epoch 24 iteration 0720/1263: training loss 0.638
Epoch 24 iteration 0740/1263: training loss 0.639
Epoch 24 iteration 0760/1263: training loss 0.641
Epoch 24 iteration 0780/1263: training loss 0.643
Epoch 24 iteration 0800/1263: training loss 0.642
Epoch 24 iteration 0820/1263: training loss 0.643
Epoch 24 iteration 0840/1263: training loss 0.646
Epoch 24 iteration 0860/1263: training loss 0.644
Epoch 24 iteration 0880/1263: training loss 0.645
Epoch 24 iteration 0900/1263: training loss 0.645
Epoch 24 iteration 0920/1263: training loss 0.646
Epoch 24 iteration 0940/1263: training loss 0.646
Epoch 24 iteration 0960/1263: training loss 0.647
Epoch 24 iteration 0980/1263: training loss 0.648
Epoch 24 iteration 1000/1263: training loss 0.650
Epoch 24 iteration 1020/1263: training loss 0.651
Epoch 24 iteration 1040/1263: training loss 0.651
Epoch 24 iteration 1060/1263: training loss 0.650
Epoch 24 iteration 1080/1263: training loss 0.651
Epoch 24 iteration 1100/1263: training loss 0.651
Epoch 24 iteration 1120/1263: training loss 0.652
Epoch 24 iteration 1140/1263: training loss 0.652
Epoch 24 iteration 1160/1263: training loss 0.653
Epoch 24 iteration 1180/1263: training loss 0.654
Epoch 24 iteration 1200/1263: training loss 0.655
Epoch 24 iteration 1220/1263: training loss 0.656
Epoch 24 iteration 1240/1263: training loss 0.656
Epoch 24 iteration 1260/1263: training loss 0.657
Epoch 24 validation pixAcc: 0.777, mIoU: 0.411
Epoch 25 iteration 0020/1263: training loss 0.660
Epoch 25 iteration 0040/1263: training loss 0.648
Epoch 25 iteration 0060/1263: training loss 0.646
Epoch 25 iteration 0080/1263: training loss 0.634
Epoch 25 iteration 0100/1263: training loss 0.626
Epoch 25 iteration 0120/1263: training loss 0.618
Epoch 25 iteration 0140/1263: training loss 0.626
Epoch 25 iteration 0160/1263: training loss 0.621
Epoch 25 iteration 0180/1263: training loss 0.617
Epoch 25 iteration 0200/1263: training loss 0.616
Epoch 25 iteration 0220/1263: training loss 0.613
Epoch 25 iteration 0240/1263: training loss 0.617
Epoch 25 iteration 0260/1263: training loss 0.621
Epoch 25 iteration 0280/1263: training loss 0.625
Epoch 25 iteration 0300/1263: training loss 0.628
Epoch 25 iteration 0320/1263: training loss 0.631
Epoch 25 iteration 0340/1263: training loss 0.633
Epoch 25 iteration 0360/1263: training loss 0.637
Epoch 25 iteration 0380/1263: training loss 0.640
Epoch 25 iteration 0400/1263: training loss 0.641
Epoch 25 iteration 0420/1263: training loss 0.640
Epoch 25 iteration 0440/1263: training loss 0.642
Epoch 25 iteration 0460/1263: training loss 0.642
Epoch 25 iteration 0480/1263: training loss 0.642
Epoch 25 iteration 0500/1263: training loss 0.640
Epoch 25 iteration 0520/1263: training loss 0.639
Epoch 25 iteration 0540/1263: training loss 0.638
Epoch 25 iteration 0560/1263: training loss 0.641
Epoch 25 iteration 0580/1263: training loss 0.641
Epoch 25 iteration 0600/1263: training loss 0.639
Epoch 25 iteration 0620/1263: training loss 0.638
Epoch 25 iteration 0640/1263: training loss 0.637
Epoch 25 iteration 0660/1263: training loss 0.637
Epoch 25 iteration 0680/1263: training loss 0.637
Epoch 25 iteration 0700/1263: training loss 0.637
Epoch 25 iteration 0720/1263: training loss 0.637
Epoch 25 iteration 0740/1263: training loss 0.637
Epoch 25 iteration 0760/1263: training loss 0.637
Epoch 25 iteration 0780/1263: training loss 0.637
Epoch 25 iteration 0800/1263: training loss 0.635
Epoch 25 iteration 0820/1263: training loss 0.634
Epoch 25 iteration 0840/1263: training loss 0.633
Epoch 25 iteration 0860/1263: training loss 0.634
Epoch 25 iteration 0880/1263: training loss 0.633
Epoch 25 iteration 0900/1263: training loss 0.633
Epoch 25 iteration 0920/1263: training loss 0.632
Epoch 25 iteration 0940/1263: training loss 0.633
Epoch 25 iteration 0960/1263: training loss 0.633
Epoch 25 iteration 0980/1263: training loss 0.634
Epoch 25 iteration 1000/1263: training loss 0.634
Epoch 25 iteration 1020/1263: training loss 0.634
Epoch 25 iteration 1040/1263: training loss 0.635
Epoch 25 iteration 1060/1263: training loss 0.635
Epoch 25 iteration 1080/1263: training loss 0.635
Epoch 25 iteration 1100/1263: training loss 0.636
Epoch 25 iteration 1120/1263: training loss 0.636
Epoch 25 iteration 1140/1263: training loss 0.637
Epoch 25 iteration 1160/1263: training loss 0.636
Epoch 25 iteration 1180/1263: training loss 0.637
Epoch 25 iteration 1200/1263: training loss 0.637
Epoch 25 iteration 1220/1263: training loss 0.637
Epoch 25 iteration 1240/1263: training loss 0.638
Epoch 25 iteration 1260/1263: training loss 0.638
Epoch 25 validation pixAcc: 0.778, mIoU: 0.410
Epoch 26 iteration 0020/1263: training loss 0.622
Epoch 26 iteration 0040/1263: training loss 0.616
Epoch 26 iteration 0060/1263: training loss 0.604
Epoch 26 iteration 0080/1263: training loss 0.606
Epoch 26 iteration 0100/1263: training loss 0.595
Epoch 26 iteration 0120/1263: training loss 0.594
Epoch 26 iteration 0140/1263: training loss 0.596
Epoch 26 iteration 0160/1263: training loss 0.601
Epoch 26 iteration 0180/1263: training loss 0.599
Epoch 26 iteration 0200/1263: training loss 0.601
Epoch 26 iteration 0220/1263: training loss 0.603
Epoch 26 iteration 0240/1263: training loss 0.603
Epoch 26 iteration 0260/1263: training loss 0.603
Epoch 26 iteration 0280/1263: training loss 0.606
Epoch 26 iteration 0300/1263: training loss 0.606
Epoch 26 iteration 0320/1263: training loss 0.610
Epoch 26 iteration 0340/1263: training loss 0.612
Epoch 26 iteration 0360/1263: training loss 0.613
Epoch 26 iteration 0380/1263: training loss 0.613
Epoch 26 iteration 0400/1263: training loss 0.624
Epoch 26 iteration 0420/1263: training loss 0.629
Epoch 26 iteration 0440/1263: training loss 0.631
Epoch 26 iteration 0460/1263: training loss 0.634
Epoch 26 iteration 0480/1263: training loss 0.635
Epoch 26 iteration 0500/1263: training loss 0.637
Epoch 26 iteration 0520/1263: training loss 0.641
Epoch 26 iteration 0540/1263: training loss 0.646
Epoch 26 iteration 0560/1263: training loss 0.646
Epoch 26 iteration 0580/1263: training loss 0.647
Epoch 26 iteration 0600/1263: training loss 0.648
Epoch 26 iteration 0620/1263: training loss 0.648
Epoch 26 iteration 0640/1263: training loss 0.647
Epoch 26 iteration 0660/1263: training loss 0.646
Epoch 26 iteration 0680/1263: training loss 0.644
Epoch 26 iteration 0700/1263: training loss 0.646
Epoch 26 iteration 0720/1263: training loss 0.645
Epoch 26 iteration 0740/1263: training loss 0.645
Epoch 26 iteration 0760/1263: training loss 0.644
Epoch 26 iteration 0780/1263: training loss 0.644
Epoch 26 iteration 0800/1263: training loss 0.644
Epoch 26 iteration 0820/1263: training loss 0.644
Epoch 26 iteration 0840/1263: training loss 0.644
Epoch 26 iteration 0860/1263: training loss 0.644
Epoch 26 iteration 0880/1263: training loss 0.644
Epoch 26 iteration 0900/1263: training loss 0.643
Epoch 26 iteration 0920/1263: training loss 0.644
Epoch 26 iteration 0940/1263: training loss 0.644
Epoch 26 iteration 0960/1263: training loss 0.643
Epoch 26 iteration 0980/1263: training loss 0.644
Epoch 26 iteration 1000/1263: training loss 0.645
Epoch 26 iteration 1020/1263: training loss 0.644
Epoch 26 iteration 1040/1263: training loss 0.645
Epoch 26 iteration 1060/1263: training loss 0.645
Epoch 26 iteration 1080/1263: training loss 0.646
Epoch 26 iteration 1100/1263: training loss 0.647
Epoch 26 iteration 1120/1263: training loss 0.646
Epoch 26 iteration 1140/1263: training loss 0.646
Epoch 26 iteration 1160/1263: training loss 0.646
Epoch 26 iteration 1180/1263: training loss 0.645
Epoch 26 iteration 1200/1263: training loss 0.646
Epoch 26 iteration 1220/1263: training loss 0.646
Epoch 26 iteration 1240/1263: training loss 0.646
Epoch 26 iteration 1260/1263: training loss 0.645
Epoch 26 validation pixAcc: 0.779, mIoU: 0.404
Epoch 27 iteration 0020/1263: training loss 0.583
Epoch 27 iteration 0040/1263: training loss 0.571
Epoch 27 iteration 0060/1263: training loss 0.571
Epoch 27 iteration 0080/1263: training loss 0.574
Epoch 27 iteration 0100/1263: training loss 0.576
Epoch 27 iteration 0120/1263: training loss 0.575
Epoch 27 iteration 0140/1263: training loss 0.582
Epoch 27 iteration 0160/1263: training loss 0.589
Epoch 27 iteration 0180/1263: training loss 0.594
Epoch 27 iteration 0200/1263: training loss 0.595
Epoch 27 iteration 0220/1263: training loss 0.598
Epoch 27 iteration 0240/1263: training loss 0.603
Epoch 27 iteration 0260/1263: training loss 0.604
Epoch 27 iteration 0280/1263: training loss 0.606
Epoch 27 iteration 0300/1263: training loss 0.606
Epoch 27 iteration 0320/1263: training loss 0.608
Epoch 27 iteration 0340/1263: training loss 0.610
Epoch 27 iteration 0360/1263: training loss 0.608
Epoch 27 iteration 0380/1263: training loss 0.607
Epoch 27 iteration 0400/1263: training loss 0.605
Epoch 27 iteration 0420/1263: training loss 0.604
Epoch 27 iteration 0440/1263: training loss 0.604
Epoch 27 iteration 0460/1263: training loss 0.603
Epoch 27 iteration 0480/1263: training loss 0.602
Epoch 27 iteration 0500/1263: training loss 0.603
Epoch 27 iteration 0520/1263: training loss 0.605
Epoch 27 iteration 0540/1263: training loss 0.605
Epoch 27 iteration 0560/1263: training loss 0.605
Epoch 27 iteration 0580/1263: training loss 0.608
Epoch 27 iteration 0600/1263: training loss 0.610
Epoch 27 iteration 0620/1263: training loss 0.613
Epoch 27 iteration 0640/1263: training loss 0.616
Epoch 27 iteration 0660/1263: training loss 0.617
Epoch 27 iteration 0680/1263: training loss 0.617
Epoch 27 iteration 0700/1263: training loss 0.619
Epoch 27 iteration 0720/1263: training loss 0.621
Epoch 27 iteration 0740/1263: training loss 0.620
Epoch 27 iteration 0760/1263: training loss 0.620
Epoch 27 iteration 0780/1263: training loss 0.618
Epoch 27 iteration 0800/1263: training loss 0.618
Epoch 27 iteration 0820/1263: training loss 0.619
Epoch 27 iteration 0840/1263: training loss 0.619
Epoch 27 iteration 0860/1263: training loss 0.619
Epoch 27 iteration 0880/1263: training loss 0.621
Epoch 27 iteration 0900/1263: training loss 0.620
Epoch 27 iteration 0920/1263: training loss 0.621
Epoch 27 iteration 0940/1263: training loss 0.621
Epoch 27 iteration 0960/1263: training loss 0.621
Epoch 27 iteration 0980/1263: training loss 0.622
Epoch 27 iteration 1000/1263: training loss 0.621
Epoch 27 iteration 1020/1263: training loss 0.622
Epoch 27 iteration 1040/1263: training loss 0.623
Epoch 27 iteration 1060/1263: training loss 0.624
Epoch 27 iteration 1080/1263: training loss 0.624
Epoch 27 iteration 1100/1263: training loss 0.626
Epoch 27 iteration 1120/1263: training loss 0.626
Epoch 27 iteration 1140/1263: training loss 0.627
Epoch 27 iteration 1160/1263: training loss 0.627
Epoch 27 iteration 1180/1263: training loss 0.628
Epoch 27 iteration 1200/1263: training loss 0.628
Epoch 27 iteration 1220/1263: training loss 0.629
Epoch 27 iteration 1240/1263: training loss 0.629
Epoch 27 iteration 1260/1263: training loss 0.629
Epoch 27 validation pixAcc: 0.775, mIoU: 0.399
Epoch 28 iteration 0020/1263: training loss 0.602
Epoch 28 iteration 0040/1263: training loss 0.623
Epoch 28 iteration 0060/1263: training loss 0.607
Epoch 28 iteration 0080/1263: training loss 0.608
Epoch 28 iteration 0100/1263: training loss 0.616
Epoch 28 iteration 0120/1263: training loss 0.611
Epoch 28 iteration 0140/1263: training loss 0.609
Epoch 28 iteration 0160/1263: training loss 0.604
Epoch 28 iteration 0180/1263: training loss 0.597
Epoch 28 iteration 0200/1263: training loss 0.596
Epoch 28 iteration 0220/1263: training loss 0.596
Epoch 28 iteration 0240/1263: training loss 0.593
Epoch 28 iteration 0260/1263: training loss 0.591
Epoch 28 iteration 0280/1263: training loss 0.593
Epoch 28 iteration 0300/1263: training loss 0.587
Epoch 28 iteration 0320/1263: training loss 0.589
Epoch 28 iteration 0340/1263: training loss 0.590
Epoch 28 iteration 0360/1263: training loss 0.592
Epoch 28 iteration 0380/1263: training loss 0.593
Epoch 28 iteration 0400/1263: training loss 0.594
Epoch 28 iteration 0420/1263: training loss 0.594
Epoch 28 iteration 0440/1263: training loss 0.593
Epoch 28 iteration 0460/1263: training loss 0.594
Epoch 28 iteration 0480/1263: training loss 0.596
Epoch 28 iteration 0500/1263: training loss 0.597
Epoch 28 iteration 0520/1263: training loss 0.595
Epoch 28 iteration 0540/1263: training loss 0.598
Epoch 28 iteration 0560/1263: training loss 0.600
Epoch 28 iteration 0580/1263: training loss 0.600
Epoch 28 iteration 0600/1263: training loss 0.602
Epoch 28 iteration 0620/1263: training loss 0.603
Epoch 28 iteration 0640/1263: training loss 0.604
Epoch 28 iteration 0660/1263: training loss 0.604
Epoch 28 iteration 0680/1263: training loss 0.605
Epoch 28 iteration 0700/1263: training loss 0.606
Epoch 28 iteration 0720/1263: training loss 0.607
Epoch 28 iteration 0740/1263: training loss 0.608
Epoch 28 iteration 0760/1263: training loss 0.608
Epoch 28 iteration 0780/1263: training loss 0.608
Epoch 28 iteration 0800/1263: training loss 0.607
Epoch 28 iteration 0820/1263: training loss 0.608
Epoch 28 iteration 0840/1263: training loss 0.608
Epoch 28 iteration 0860/1263: training loss 0.607
Epoch 28 iteration 0880/1263: training loss 0.607
Epoch 28 iteration 0900/1263: training loss 0.606
Epoch 28 iteration 0920/1263: training loss 0.607
Epoch 28 iteration 0940/1263: training loss 0.609
Epoch 28 iteration 0960/1263: training loss 0.611
Epoch 28 iteration 0980/1263: training loss 0.610
Epoch 28 iteration 1000/1263: training loss 0.610
Epoch 28 iteration 1020/1263: training loss 0.612
Epoch 28 iteration 1040/1263: training loss 0.612
Epoch 28 iteration 1060/1263: training loss 0.612
Epoch 28 iteration 1080/1263: training loss 0.613
Epoch 28 iteration 1100/1263: training loss 0.613
Epoch 28 iteration 1120/1263: training loss 0.613
Epoch 28 iteration 1140/1263: training loss 0.615
Epoch 28 iteration 1160/1263: training loss 0.616
Epoch 28 iteration 1180/1263: training loss 0.617
Epoch 28 iteration 1200/1263: training loss 0.618
Epoch 28 iteration 1220/1263: training loss 0.619
Epoch 28 iteration 1240/1263: training loss 0.619
Epoch 28 iteration 1260/1263: training loss 0.618
Epoch 28 validation pixAcc: 0.776, mIoU: 0.408
Epoch 29 iteration 0020/1263: training loss 0.595
Epoch 29 iteration 0040/1263: training loss 0.579
Epoch 29 iteration 0060/1263: training loss 0.567
Epoch 29 iteration 0080/1263: training loss 0.567
Epoch 29 iteration 0100/1263: training loss 0.556
Epoch 29 iteration 0120/1263: training loss 0.563
Epoch 29 iteration 0140/1263: training loss 0.555
Epoch 29 iteration 0160/1263: training loss 0.560
Epoch 29 iteration 0180/1263: training loss 0.561
Epoch 29 iteration 0200/1263: training loss 0.568
Epoch 29 iteration 0220/1263: training loss 0.570
Epoch 29 iteration 0240/1263: training loss 0.575
Epoch 29 iteration 0260/1263: training loss 0.581
Epoch 29 iteration 0280/1263: training loss 0.583
Epoch 29 iteration 0300/1263: training loss 0.585
Epoch 29 iteration 0320/1263: training loss 0.587
Epoch 29 iteration 0340/1263: training loss 0.586
Epoch 29 iteration 0360/1263: training loss 0.589
Epoch 29 iteration 0380/1263: training loss 0.591
Epoch 29 iteration 0400/1263: training loss 0.593
Epoch 29 iteration 0420/1263: training loss 0.592
Epoch 29 iteration 0440/1263: training loss 0.589
Epoch 29 iteration 0460/1263: training loss 0.591
Epoch 29 iteration 0480/1263: training loss 0.591
Epoch 29 iteration 0500/1263: training loss 0.593
Epoch 29 iteration 0520/1263: training loss 0.592
Epoch 29 iteration 0540/1263: training loss 0.595
Epoch 29 iteration 0560/1263: training loss 0.598
Epoch 29 iteration 0580/1263: training loss 0.597
Epoch 29 iteration 0600/1263: training loss 0.599
Epoch 29 iteration 0620/1263: training loss 0.600
Epoch 29 iteration 0640/1263: training loss 0.600
Epoch 29 iteration 0660/1263: training loss 0.601
Epoch 29 iteration 0680/1263: training loss 0.601
Epoch 29 iteration 0700/1263: training loss 0.600
Epoch 29 iteration 0720/1263: training loss 0.598
Epoch 29 iteration 0740/1263: training loss 0.598
Epoch 29 iteration 0760/1263: training loss 0.599
Epoch 29 iteration 0780/1263: training loss 0.598
Epoch 29 iteration 0800/1263: training loss 0.597
Epoch 29 iteration 0820/1263: training loss 0.597
Epoch 29 iteration 0840/1263: training loss 0.596
Epoch 29 iteration 0860/1263: training loss 0.595
Epoch 29 iteration 0880/1263: training loss 0.595
Epoch 29 iteration 0900/1263: training loss 0.594
Epoch 29 iteration 0920/1263: training loss 0.594
Epoch 29 iteration 0940/1263: training loss 0.594
Epoch 29 iteration 0960/1263: training loss 0.593
Epoch 29 iteration 0980/1263: training loss 0.593
Epoch 29 iteration 1000/1263: training loss 0.592
Epoch 29 iteration 1020/1263: training loss 0.593
Epoch 29 iteration 1040/1263: training loss 0.594
Epoch 29 iteration 1060/1263: training loss 0.595
Epoch 29 iteration 1080/1263: training loss 0.596
Epoch 29 iteration 1100/1263: training loss 0.596
Epoch 29 iteration 1120/1263: training loss 0.596
Epoch 29 iteration 1140/1263: training loss 0.597
Epoch 29 iteration 1160/1263: training loss 0.598
Epoch 29 iteration 1180/1263: training loss 0.599
Epoch 29 iteration 1200/1263: training loss 0.601
Epoch 29 iteration 1220/1263: training loss 0.603
Epoch 29 iteration 1240/1263: training loss 0.605
Epoch 29 iteration 1260/1263: training loss 0.606
Epoch 29 validation pixAcc: 0.769, mIoU: 0.399
Epoch 30 iteration 0020/1263: training loss 0.645
Epoch 30 iteration 0040/1263: training loss 0.638
Epoch 30 iteration 0060/1263: training loss 0.628
Epoch 30 iteration 0080/1263: training loss 0.623
Epoch 30 iteration 0100/1263: training loss 0.620
Epoch 30 iteration 0120/1263: training loss 0.610
Epoch 30 iteration 0140/1263: training loss 0.609
Epoch 30 iteration 0160/1263: training loss 0.604
Epoch 30 iteration 0180/1263: training loss 0.603
Epoch 30 iteration 0200/1263: training loss 0.598
Epoch 30 iteration 0220/1263: training loss 0.602
Epoch 30 iteration 0240/1263: training loss 0.598
Epoch 30 iteration 0260/1263: training loss 0.596
Epoch 30 iteration 0280/1263: training loss 0.598
Epoch 30 iteration 0300/1263: training loss 0.595
Epoch 30 iteration 0320/1263: training loss 0.597
Epoch 30 iteration 0340/1263: training loss 0.599
Epoch 30 iteration 0360/1263: training loss 0.599
Epoch 30 iteration 0380/1263: training loss 0.601
Epoch 30 iteration 0400/1263: training loss 0.602
Epoch 30 iteration 0420/1263: training loss 0.602
Epoch 30 iteration 0440/1263: training loss 0.602
Epoch 30 iteration 0460/1263: training loss 0.601
Epoch 30 iteration 0480/1263: training loss 0.598
Epoch 30 iteration 0500/1263: training loss 0.597
Epoch 30 iteration 0520/1263: training loss 0.597
Epoch 30 iteration 0540/1263: training loss 0.597
Epoch 30 iteration 0560/1263: training loss 0.597
Epoch 30 iteration 0580/1263: training loss 0.597
Epoch 30 iteration 0600/1263: training loss 0.597
Epoch 30 iteration 0620/1263: training loss 0.597
Epoch 30 iteration 0640/1263: training loss 0.598
Epoch 30 iteration 0660/1263: training loss 0.598
Epoch 30 iteration 0680/1263: training loss 0.596
Epoch 30 iteration 0700/1263: training loss 0.596
Epoch 30 iteration 0720/1263: training loss 0.596
Epoch 30 iteration 0740/1263: training loss 0.595
Epoch 30 iteration 0760/1263: training loss 0.592
Epoch 30 iteration 0780/1263: training loss 0.592
Epoch 30 iteration 0800/1263: training loss 0.593
Epoch 30 iteration 0820/1263: training loss 0.592
Epoch 30 iteration 0840/1263: training loss 0.592
Epoch 30 iteration 0860/1263: training loss 0.593
Epoch 30 iteration 0880/1263: training loss 0.595
Epoch 30 iteration 0900/1263: training loss 0.596
Epoch 30 iteration 0920/1263: training loss 0.597
Epoch 30 iteration 0940/1263: training loss 0.596
Epoch 30 iteration 0960/1263: training loss 0.595
Epoch 30 iteration 0980/1263: training loss 0.598
Epoch 30 iteration 1000/1263: training loss 0.599
Epoch 30 iteration 1020/1263: training loss 0.599
Epoch 30 iteration 1040/1263: training loss 0.600
Epoch 30 iteration 1060/1263: training loss 0.601
Epoch 30 iteration 1080/1263: training loss 0.601
Epoch 30 iteration 1100/1263: training loss 0.602
Epoch 30 iteration 1120/1263: training loss 0.604
Epoch 30 iteration 1140/1263: training loss 0.605
Epoch 30 iteration 1160/1263: training loss 0.608
Epoch 30 iteration 1180/1264: training loss 0.609
Epoch 30 iteration 1200/1264: training loss 0.609
Epoch 30 iteration 1220/1264: training loss 0.610
Epoch 30 iteration 1240/1264: training loss 0.610
Epoch 30 iteration 1260/1264: training loss 0.609
Epoch 30 validation pixAcc: 0.776, mIoU: 0.397
Epoch 31 iteration 0020/1263: training loss 0.597
Epoch 31 iteration 0040/1263: training loss 0.594
Epoch 31 iteration 0060/1263: training loss 0.626
Epoch 31 iteration 0080/1263: training loss 0.607
Epoch 31 iteration 0100/1263: training loss 0.585
Epoch 31 iteration 0120/1263: training loss 0.579
Epoch 31 iteration 0140/1263: training loss 0.574
Epoch 31 iteration 0160/1263: training loss 0.575
Epoch 31 iteration 0180/1263: training loss 0.572
Epoch 31 iteration 0200/1263: training loss 0.570
Epoch 31 iteration 0220/1263: training loss 0.571
Epoch 31 iteration 0240/1263: training loss 0.571
Epoch 31 iteration 0260/1263: training loss 0.569
Epoch 31 iteration 0280/1263: training loss 0.566
Epoch 31 iteration 0300/1263: training loss 0.566
Epoch 31 iteration 0320/1263: training loss 0.566
Epoch 31 iteration 0340/1263: training loss 0.566
Epoch 31 iteration 0360/1263: training loss 0.569
Epoch 31 iteration 0380/1263: training loss 0.568
Epoch 31 iteration 0400/1263: training loss 0.570
Epoch 31 iteration 0420/1263: training loss 0.576
Epoch 31 iteration 0440/1263: training loss 0.579
Epoch 31 iteration 0460/1263: training loss 0.580
Epoch 31 iteration 0480/1263: training loss 0.582
Epoch 31 iteration 0500/1263: training loss 0.582
Epoch 31 iteration 0520/1263: training loss 0.580
Epoch 31 iteration 0540/1263: training loss 0.579
Epoch 31 iteration 0560/1263: training loss 0.577
Epoch 31 iteration 0580/1263: training loss 0.577
Epoch 31 iteration 0600/1263: training loss 0.577
Epoch 31 iteration 0620/1263: training loss 0.576
Epoch 31 iteration 0640/1263: training loss 0.579
Epoch 31 iteration 0660/1263: training loss 0.580
Epoch 31 iteration 0680/1263: training loss 0.580
Epoch 31 iteration 0700/1263: training loss 0.581
Epoch 31 iteration 0720/1263: training loss 0.581
Epoch 31 iteration 0740/1263: training loss 0.580
Epoch 31 iteration 0760/1263: training loss 0.580
Epoch 31 iteration 0780/1263: training loss 0.580
Epoch 31 iteration 0800/1263: training loss 0.580
Epoch 31 iteration 0820/1263: training loss 0.581
Epoch 31 iteration 0840/1263: training loss 0.583
Epoch 31 iteration 0860/1263: training loss 0.585
Epoch 31 iteration 0880/1263: training loss 0.587
Epoch 31 iteration 0900/1263: training loss 0.588
Epoch 31 iteration 0920/1263: training loss 0.588
Epoch 31 iteration 0940/1263: training loss 0.589
Epoch 31 iteration 0960/1263: training loss 0.590
Epoch 31 iteration 0980/1263: training loss 0.589
Epoch 31 iteration 1000/1263: training loss 0.590
Epoch 31 iteration 1020/1263: training loss 0.589
Epoch 31 iteration 1040/1263: training loss 0.588
Epoch 31 iteration 1060/1263: training loss 0.589
Epoch 31 iteration 1080/1263: training loss 0.590
Epoch 31 iteration 1100/1263: training loss 0.590
Epoch 31 iteration 1120/1263: training loss 0.591
Epoch 31 iteration 1140/1263: training loss 0.592
Epoch 31 iteration 1160/1263: training loss 0.593
Epoch 31 iteration 1180/1263: training loss 0.594
Epoch 31 iteration 1200/1263: training loss 0.597
Epoch 31 iteration 1220/1263: training loss 0.597
Epoch 31 iteration 1240/1263: training loss 0.597
Epoch 31 iteration 1260/1263: training loss 0.598
Epoch 31 validation pixAcc: 0.769, mIoU: 0.390
Epoch 32 iteration 0020/1263: training loss 0.575
Epoch 32 iteration 0040/1263: training loss 0.574
Epoch 32 iteration 0060/1263: training loss 0.596
Epoch 32 iteration 0080/1263: training loss 0.606
Epoch 32 iteration 0100/1263: training loss 0.612
Epoch 32 iteration 0120/1263: training loss 0.613
Epoch 32 iteration 0140/1263: training loss 0.622
Epoch 32 iteration 0160/1263: training loss 0.612
Epoch 32 iteration 0180/1263: training loss 0.604
Epoch 32 iteration 0200/1263: training loss 0.598
Epoch 32 iteration 0220/1263: training loss 0.595
Epoch 32 iteration 0240/1263: training loss 0.592
Epoch 32 iteration 0260/1263: training loss 0.593
Epoch 32 iteration 0280/1263: training loss 0.594
Epoch 32 iteration 0300/1263: training loss 0.594
Epoch 32 iteration 0320/1263: training loss 0.592
Epoch 32 iteration 0340/1263: training loss 0.597
Epoch 32 iteration 0360/1263: training loss 0.594
Epoch 32 iteration 0380/1263: training loss 0.593
Epoch 32 iteration 0400/1263: training loss 0.593
Epoch 32 iteration 0420/1263: training loss 0.593
Epoch 32 iteration 0440/1263: training loss 0.594
Epoch 32 iteration 0460/1263: training loss 0.595
Epoch 32 iteration 0480/1263: training loss 0.595
Epoch 32 iteration 0500/1263: training loss 0.595
Epoch 32 iteration 0520/1263: training loss 0.593
Epoch 32 iteration 0540/1263: training loss 0.592
Epoch 32 iteration 0560/1263: training loss 0.592
Epoch 32 iteration 0580/1263: training loss 0.590
Epoch 32 iteration 0600/1263: training loss 0.589
Epoch 32 iteration 0620/1263: training loss 0.587
Epoch 32 iteration 0640/1263: training loss 0.587
Epoch 32 iteration 0660/1263: training loss 0.587
Epoch 32 iteration 0680/1263: training loss 0.586
Epoch 32 iteration 0700/1263: training loss 0.588
Epoch 32 iteration 0720/1263: training loss 0.587
Epoch 32 iteration 0740/1263: training loss 0.585
Epoch 32 iteration 0760/1263: training loss 0.586
Epoch 32 iteration 0780/1263: training loss 0.587
Epoch 32 iteration 0800/1263: training loss 0.587
Epoch 32 iteration 0820/1263: training loss 0.586
Epoch 32 iteration 0840/1263: training loss 0.585
Epoch 32 iteration 0860/1263: training loss 0.584
Epoch 32 iteration 0880/1263: training loss 0.583
Epoch 32 iteration 0900/1263: training loss 0.583
Epoch 32 iteration 0920/1263: training loss 0.582
Epoch 32 iteration 0940/1263: training loss 0.582
Epoch 32 iteration 0960/1263: training loss 0.582
Epoch 32 iteration 0980/1263: training loss 0.582
Epoch 32 iteration 1000/1263: training loss 0.582
Epoch 32 iteration 1020/1263: training loss 0.582
Epoch 32 iteration 1040/1263: training loss 0.582
Epoch 32 iteration 1060/1263: training loss 0.582
Epoch 32 iteration 1080/1263: training loss 0.583
Epoch 32 iteration 1100/1263: training loss 0.583
Epoch 32 iteration 1120/1263: training loss 0.583
Epoch 32 iteration 1140/1263: training loss 0.583
Epoch 32 iteration 1160/1263: training loss 0.584
Epoch 32 iteration 1180/1263: training loss 0.586
Epoch 32 iteration 1200/1263: training loss 0.586
Epoch 32 iteration 1220/1263: training loss 0.585
Epoch 32 iteration 1240/1263: training loss 0.585
Epoch 32 iteration 1260/1263: training loss 0.585
Epoch 32 validation pixAcc: 0.780, mIoU: 0.413
Epoch 33 iteration 0020/1263: training loss 0.533
Epoch 33 iteration 0040/1263: training loss 0.534
Epoch 33 iteration 0060/1263: training loss 0.520
Epoch 33 iteration 0080/1263: training loss 0.522
Epoch 33 iteration 0100/1263: training loss 0.526
Epoch 33 iteration 0120/1263: training loss 0.537
Epoch 33 iteration 0140/1263: training loss 0.544
Epoch 33 iteration 0160/1263: training loss 0.539
Epoch 33 iteration 0180/1263: training loss 0.543
Epoch 33 iteration 0200/1263: training loss 0.551
Epoch 33 iteration 0220/1263: training loss 0.550
Epoch 33 iteration 0240/1263: training loss 0.546
Epoch 33 iteration 0260/1263: training loss 0.543
Epoch 33 iteration 0280/1263: training loss 0.542
Epoch 33 iteration 0300/1263: training loss 0.540
Epoch 33 iteration 0320/1263: training loss 0.540
Epoch 33 iteration 0340/1263: training loss 0.541
Epoch 33 iteration 0360/1263: training loss 0.540
Epoch 33 iteration 0380/1263: training loss 0.539
Epoch 33 iteration 0400/1263: training loss 0.541
Epoch 33 iteration 0420/1263: training loss 0.541
Epoch 33 iteration 0440/1263: training loss 0.543
Epoch 33 iteration 0460/1263: training loss 0.545
Epoch 33 iteration 0480/1263: training loss 0.546
Epoch 33 iteration 0500/1263: training loss 0.549
Epoch 33 iteration 0520/1263: training loss 0.550
Epoch 33 iteration 0540/1263: training loss 0.553
Epoch 33 iteration 0560/1263: training loss 0.552
Epoch 33 iteration 0580/1263: training loss 0.552
Epoch 33 iteration 0600/1263: training loss 0.553
Epoch 33 iteration 0620/1263: training loss 0.554
Epoch 33 iteration 0640/1263: training loss 0.553
Epoch 33 iteration 0660/1263: training loss 0.553
Epoch 33 iteration 0680/1263: training loss 0.555
Epoch 33 iteration 0700/1263: training loss 0.555
Epoch 33 iteration 0720/1263: training loss 0.556
Epoch 33 iteration 0740/1263: training loss 0.555
Epoch 33 iteration 0760/1263: training loss 0.557
Epoch 33 iteration 0780/1263: training loss 0.557
Epoch 33 iteration 0800/1263: training loss 0.557
Epoch 33 iteration 0820/1263: training loss 0.557
Epoch 33 iteration 0840/1263: training loss 0.559
Epoch 33 iteration 0860/1263: training loss 0.559
Epoch 33 iteration 0880/1263: training loss 0.558
Epoch 33 iteration 0900/1263: training loss 0.556
Epoch 33 iteration 0920/1263: training loss 0.557
Epoch 33 iteration 0940/1263: training loss 0.558
Epoch 33 iteration 0960/1263: training loss 0.559
Epoch 33 iteration 0980/1263: training loss 0.559
Epoch 33 iteration 1000/1263: training loss 0.561
Epoch 33 iteration 1020/1263: training loss 0.561
Epoch 33 iteration 1040/1263: training loss 0.561
Epoch 33 iteration 1060/1263: training loss 0.560
Epoch 33 iteration 1080/1263: training loss 0.560
Epoch 33 iteration 1100/1263: training loss 0.562
Epoch 33 iteration 1120/1263: training loss 0.563
Epoch 33 iteration 1140/1263: training loss 0.564
Epoch 33 iteration 1160/1263: training loss 0.563
Epoch 33 iteration 1180/1263: training loss 0.563
Epoch 33 iteration 1200/1263: training loss 0.563
Epoch 33 iteration 1220/1263: training loss 0.563
Epoch 33 iteration 1240/1263: training loss 0.563
Epoch 33 iteration 1260/1263: training loss 0.563
Epoch 33 validation pixAcc: 0.779, mIoU: 0.407
Epoch 34 iteration 0020/1263: training loss 0.545
Epoch 34 iteration 0040/1263: training loss 0.542
Epoch 34 iteration 0060/1263: training loss 0.548
Epoch 34 iteration 0080/1263: training loss 0.557
Epoch 34 iteration 0100/1263: training loss 0.555
Epoch 34 iteration 0120/1263: training loss 0.553
Epoch 34 iteration 0140/1263: training loss 0.549
Epoch 34 iteration 0160/1263: training loss 0.547
Epoch 34 iteration 0180/1263: training loss 0.549
Epoch 34 iteration 0200/1263: training loss 0.549
Epoch 34 iteration 0220/1263: training loss 0.550
Epoch 34 iteration 0240/1263: training loss 0.553
Epoch 34 iteration 0260/1263: training loss 0.555
Epoch 34 iteration 0280/1263: training loss 0.552
Epoch 34 iteration 0300/1263: training loss 0.553
Epoch 34 iteration 0320/1263: training loss 0.553
Epoch 34 iteration 0340/1263: training loss 0.554
Epoch 34 iteration 0360/1263: training loss 0.555
Epoch 34 iteration 0380/1263: training loss 0.555
Epoch 34 iteration 0400/1263: training loss 0.555
Epoch 34 iteration 0420/1263: training loss 0.555
Epoch 34 iteration 0440/1263: training loss 0.555
Epoch 34 iteration 0460/1263: training loss 0.550
Epoch 34 iteration 0480/1263: training loss 0.549
Epoch 34 iteration 0500/1263: training loss 0.546
Epoch 34 iteration 0520/1263: training loss 0.547
Epoch 34 iteration 0540/1263: training loss 0.548
Epoch 34 iteration 0560/1263: training loss 0.550
Epoch 34 iteration 0580/1263: training loss 0.550
Epoch 34 iteration 0600/1263: training loss 0.549
Epoch 34 iteration 0620/1263: training loss 0.550
Epoch 34 iteration 0640/1263: training loss 0.551
Epoch 34 iteration 0660/1263: training loss 0.552
Epoch 34 iteration 0680/1263: training loss 0.552
Epoch 34 iteration 0700/1263: training loss 0.553
Epoch 34 iteration 0720/1263: training loss 0.553
Epoch 34 iteration 0740/1263: training loss 0.553
Epoch 34 iteration 0760/1263: training loss 0.552
Epoch 34 iteration 0780/1263: training loss 0.552
Epoch 34 iteration 0800/1263: training loss 0.553
Epoch 34 iteration 0820/1263: training loss 0.552
Epoch 34 iteration 0840/1263: training loss 0.552
Epoch 34 iteration 0860/1263: training loss 0.551
Epoch 34 iteration 0880/1263: training loss 0.551
Epoch 34 iteration 0900/1263: training loss 0.551
Epoch 34 iteration 0920/1263: training loss 0.551
Epoch 34 iteration 0940/1263: training loss 0.551
Epoch 34 iteration 0960/1263: training loss 0.550
Epoch 34 iteration 0980/1263: training loss 0.551
Epoch 34 iteration 1000/1263: training loss 0.552
Epoch 34 iteration 1020/1263: training loss 0.554
Epoch 34 iteration 1040/1263: training loss 0.555
Epoch 34 iteration 1060/1263: training loss 0.556
Epoch 34 iteration 1080/1263: training loss 0.556
Epoch 34 iteration 1100/1263: training loss 0.557
Epoch 34 iteration 1120/1263: training loss 0.558
Epoch 34 iteration 1140/1263: training loss 0.558
Epoch 34 iteration 1160/1263: training loss 0.557
Epoch 34 iteration 1180/1263: training loss 0.557
Epoch 34 iteration 1200/1263: training loss 0.556
Epoch 34 iteration 1220/1263: training loss 0.558
Epoch 34 iteration 1240/1263: training loss 0.559
Epoch 34 iteration 1260/1263: training loss 0.559
Epoch 34 validation pixAcc: 0.769, mIoU: 0.393
Epoch 35 iteration 0020/1263: training loss 0.539
Epoch 35 iteration 0040/1263: training loss 0.558
Epoch 35 iteration 0060/1263: training loss 0.539
Epoch 35 iteration 0080/1263: training loss 0.550
Epoch 35 iteration 0100/1263: training loss 0.545
Epoch 35 iteration 0120/1263: training loss 0.541
Epoch 35 iteration 0140/1263: training loss 0.539
Epoch 35 iteration 0160/1263: training loss 0.544
Epoch 35 iteration 0180/1263: training loss 0.544
Epoch 35 iteration 0200/1263: training loss 0.542
Epoch 35 iteration 0220/1263: training loss 0.544
Epoch 35 iteration 0240/1263: training loss 0.544
Epoch 35 iteration 0260/1263: training loss 0.548
Epoch 35 iteration 0280/1263: training loss 0.550
Epoch 35 iteration 0300/1263: training loss 0.552
Epoch 35 iteration 0320/1263: training loss 0.555
Epoch 35 iteration 0340/1263: training loss 0.555
Epoch 35 iteration 0360/1263: training loss 0.555
Epoch 35 iteration 0380/1263: training loss 0.555
Epoch 35 iteration 0400/1263: training loss 0.555
Epoch 35 iteration 0420/1263: training loss 0.554
Epoch 35 iteration 0440/1263: training loss 0.555
Epoch 35 iteration 0460/1263: training loss 0.554
Epoch 35 iteration 0480/1263: training loss 0.555
Epoch 35 iteration 0500/1263: training loss 0.555
Epoch 35 iteration 0520/1263: training loss 0.555
Epoch 35 iteration 0540/1263: training loss 0.555
Epoch 35 iteration 0560/1263: training loss 0.552
Epoch 35 iteration 0580/1263: training loss 0.551
Epoch 35 iteration 0600/1263: training loss 0.550
Epoch 35 iteration 0620/1263: training loss 0.548
Epoch 35 iteration 0640/1263: training loss 0.548
Epoch 35 iteration 0660/1263: training loss 0.547
Epoch 35 iteration 0680/1263: training loss 0.547
Epoch 35 iteration 0700/1263: training loss 0.547
Epoch 35 iteration 0720/1263: training loss 0.547
Epoch 35 iteration 0740/1263: training loss 0.546
Epoch 35 iteration 0760/1263: training loss 0.546
Epoch 35 iteration 0780/1263: training loss 0.545
Epoch 35 iteration 0800/1263: training loss 0.546
Epoch 35 iteration 0820/1263: training loss 0.546
Epoch 35 iteration 0840/1263: training loss 0.546
Epoch 35 iteration 0860/1263: training loss 0.547
Epoch 35 iteration 0880/1263: training loss 0.547
Epoch 35 iteration 0900/1263: training loss 0.548
Epoch 35 iteration 0920/1263: training loss 0.550
Epoch 35 iteration 0940/1263: training loss 0.551
Epoch 35 iteration 0960/1263: training loss 0.552
Epoch 35 iteration 0980/1263: training loss 0.552
Epoch 35 iteration 1000/1263: training loss 0.554
Epoch 35 iteration 1020/1263: training loss 0.555
Epoch 35 iteration 1040/1263: training loss 0.555
Epoch 35 iteration 1060/1263: training loss 0.555
Epoch 35 iteration 1080/1263: training loss 0.556
Epoch 35 iteration 1100/1263: training loss 0.556
Epoch 35 iteration 1120/1263: training loss 0.555
Epoch 35 iteration 1140/1263: training loss 0.555
Epoch 35 iteration 1160/1263: training loss 0.555
Epoch 35 iteration 1180/1263: training loss 0.556
Epoch 35 iteration 1200/1263: training loss 0.555
Epoch 35 iteration 1220/1263: training loss 0.555
Epoch 35 iteration 1240/1263: training loss 0.556
Epoch 35 iteration 1260/1263: training loss 0.556
Epoch 35 validation pixAcc: 0.780, mIoU: 0.409
Epoch 36 iteration 0020/1263: training loss 0.512
Epoch 36 iteration 0040/1263: training loss 0.526
Epoch 36 iteration 0060/1263: training loss 0.536
Epoch 36 iteration 0080/1263: training loss 0.543
Epoch 36 iteration 0100/1263: training loss 0.531
Epoch 36 iteration 0120/1263: training loss 0.533
Epoch 36 iteration 0140/1263: training loss 0.529
Epoch 36 iteration 0160/1263: training loss 0.522
Epoch 36 iteration 0180/1263: training loss 0.518
Epoch 36 iteration 0200/1263: training loss 0.513
Epoch 36 iteration 0220/1263: training loss 0.512
Epoch 36 iteration 0240/1263: training loss 0.515
Epoch 36 iteration 0260/1263: training loss 0.516
Epoch 36 iteration 0280/1263: training loss 0.516
Epoch 36 iteration 0300/1263: training loss 0.519
Epoch 36 iteration 0320/1263: training loss 0.519
Epoch 36 iteration 0340/1263: training loss 0.521
Epoch 36 iteration 0360/1263: training loss 0.521
Epoch 36 iteration 0380/1263: training loss 0.520
Epoch 36 iteration 0400/1263: training loss 0.519
Epoch 36 iteration 0420/1263: training loss 0.522
Epoch 36 iteration 0440/1263: training loss 0.521
Epoch 36 iteration 0460/1263: training loss 0.519
Epoch 36 iteration 0480/1263: training loss 0.519
Epoch 36 iteration 0500/1263: training loss 0.519
Epoch 36 iteration 0520/1263: training loss 0.519
Epoch 36 iteration 0540/1263: training loss 0.520
Epoch 36 iteration 0560/1263: training loss 0.522
Epoch 36 iteration 0580/1263: training loss 0.525
Epoch 36 iteration 0600/1263: training loss 0.526
Epoch 36 iteration 0620/1263: training loss 0.529
Epoch 36 iteration 0640/1263: training loss 0.533
Epoch 36 iteration 0660/1263: training loss 0.536
Epoch 36 iteration 0680/1263: training loss 0.538
Epoch 36 iteration 0700/1263: training loss 0.539
Epoch 36 iteration 0720/1263: training loss 0.540
Epoch 36 iteration 0740/1263: training loss 0.540
Epoch 36 iteration 0760/1263: training loss 0.540
Epoch 36 iteration 0780/1263: training loss 0.541
Epoch 36 iteration 0800/1263: training loss 0.542
Epoch 36 iteration 0820/1263: training loss 0.544
Epoch 36 iteration 0840/1263: training loss 0.544
Epoch 36 iteration 0860/1263: training loss 0.545
Epoch 36 iteration 0880/1263: training loss 0.546
Epoch 36 iteration 0900/1263: training loss 0.545
Epoch 36 iteration 0920/1263: training loss 0.545
Epoch 36 iteration 0940/1263: training loss 0.545
Epoch 36 iteration 0960/1263: training loss 0.545
Epoch 36 iteration 0980/1263: training loss 0.545
Epoch 36 iteration 1000/1263: training loss 0.546
Epoch 36 iteration 1020/1263: training loss 0.546
Epoch 36 iteration 1040/1263: training loss 0.546
Epoch 36 iteration 1060/1263: training loss 0.545
Epoch 36 iteration 1080/1263: training loss 0.544
Epoch 36 iteration 1100/1263: training loss 0.547
Epoch 36 iteration 1120/1263: training loss 0.548
Epoch 36 iteration 1140/1263: training loss 0.549
Epoch 36 iteration 1160/1263: training loss 0.549
Epoch 36 iteration 1180/1263: training loss 0.550
Epoch 36 iteration 1200/1263: training loss 0.551
Epoch 36 iteration 1220/1263: training loss 0.550
Epoch 36 iteration 1240/1263: training loss 0.550
Epoch 36 iteration 1260/1263: training loss 0.551
Epoch 36 validation pixAcc: 0.781, mIoU: 0.402
Epoch 37 iteration 0020/1263: training loss 0.564
Epoch 37 iteration 0040/1263: training loss 0.561
Epoch 37 iteration 0060/1263: training loss 0.556
Epoch 37 iteration 0080/1263: training loss 0.550
Epoch 37 iteration 0100/1263: training loss 0.549
Epoch 37 iteration 0120/1263: training loss 0.544
Epoch 37 iteration 0140/1263: training loss 0.537
Epoch 37 iteration 0160/1263: training loss 0.534
Epoch 37 iteration 0180/1263: training loss 0.534
Epoch 37 iteration 0200/1263: training loss 0.536
Epoch 37 iteration 0220/1263: training loss 0.532
Epoch 37 iteration 0240/1263: training loss 0.534
Epoch 37 iteration 0260/1263: training loss 0.532
Epoch 37 iteration 0280/1263: training loss 0.532
Epoch 37 iteration 0300/1263: training loss 0.527
Epoch 37 iteration 0320/1263: training loss 0.527
Epoch 37 iteration 0340/1263: training loss 0.526
Epoch 37 iteration 0360/1263: training loss 0.524
Epoch 37 iteration 0380/1263: training loss 0.522
Epoch 37 iteration 0400/1263: training loss 0.524
Epoch 37 iteration 0420/1263: training loss 0.522
Epoch 37 iteration 0440/1263: training loss 0.524
Epoch 37 iteration 0460/1263: training loss 0.523
Epoch 37 iteration 0480/1263: training loss 0.524
Epoch 37 iteration 0500/1263: training loss 0.524
Epoch 37 iteration 0520/1263: training loss 0.523
Epoch 37 iteration 0540/1263: training loss 0.521
Epoch 37 iteration 0560/1263: training loss 0.520
Epoch 37 iteration 0580/1263: training loss 0.520
Epoch 37 iteration 0600/1263: training loss 0.519
Epoch 37 iteration 0620/1263: training loss 0.519
Epoch 37 iteration 0640/1263: training loss 0.520
Epoch 37 iteration 0660/1263: training loss 0.521
Epoch 37 iteration 0680/1263: training loss 0.520
Epoch 37 iteration 0700/1263: training loss 0.519
Epoch 37 iteration 0720/1263: training loss 0.518
Epoch 37 iteration 0740/1263: training loss 0.517
Epoch 37 iteration 0760/1263: training loss 0.518
Epoch 37 iteration 0780/1263: training loss 0.518
Epoch 37 iteration 0800/1263: training loss 0.519
Epoch 37 iteration 0820/1263: training loss 0.519
Epoch 37 iteration 0840/1263: training loss 0.519
Epoch 37 iteration 0860/1263: training loss 0.520
Epoch 37 iteration 0880/1263: training loss 0.521
Epoch 37 iteration 0900/1263: training loss 0.521
Epoch 37 iteration 0920/1263: training loss 0.521
Epoch 37 iteration 0940/1263: training loss 0.523
Epoch 37 iteration 0960/1263: training loss 0.525
Epoch 37 iteration 0980/1263: training loss 0.526
Epoch 37 iteration 1000/1263: training loss 0.526
Epoch 37 iteration 1020/1263: training loss 0.527
Epoch 37 iteration 1040/1263: training loss 0.528
Epoch 37 iteration 1060/1263: training loss 0.528
Epoch 37 iteration 1080/1263: training loss 0.528
Epoch 37 iteration 1100/1263: training loss 0.529
Epoch 37 iteration 1120/1263: training loss 0.529
Epoch 37 iteration 1140/1263: training loss 0.529
Epoch 37 iteration 1160/1263: training loss 0.529
Epoch 37 iteration 1180/1263: training loss 0.529
Epoch 37 iteration 1200/1263: training loss 0.530
Epoch 37 iteration 1220/1263: training loss 0.531
Epoch 37 iteration 1240/1263: training loss 0.534
Epoch 37 iteration 1260/1263: training loss 0.535
Epoch 37 validation pixAcc: 0.773, mIoU: 0.401
Epoch 38 iteration 0020/1263: training loss 0.567
Epoch 38 iteration 0040/1263: training loss 0.550
Epoch 38 iteration 0060/1263: training loss 0.521
Epoch 38 iteration 0080/1263: training loss 0.512
Epoch 38 iteration 0100/1263: training loss 0.520
Epoch 38 iteration 0120/1263: training loss 0.535
Epoch 38 iteration 0140/1263: training loss 0.547
Epoch 38 iteration 0160/1263: training loss 0.554
Epoch 38 iteration 0180/1263: training loss 0.553
Epoch 38 iteration 0200/1263: training loss 0.553
Epoch 38 iteration 0220/1263: training loss 0.553
Epoch 38 iteration 0240/1263: training loss 0.554
Epoch 38 iteration 0260/1263: training loss 0.558
Epoch 38 iteration 0280/1263: training loss 0.557
Epoch 38 iteration 0300/1263: training loss 0.559
Epoch 38 iteration 0320/1263: training loss 0.561
Epoch 38 iteration 0340/1263: training loss 0.561
Epoch 38 iteration 0360/1263: training loss 0.560
Epoch 38 iteration 0380/1263: training loss 0.557
Epoch 38 iteration 0400/1263: training loss 0.556
Epoch 38 iteration 0420/1263: training loss 0.556
Epoch 38 iteration 0440/1263: training loss 0.554
Epoch 38 iteration 0460/1263: training loss 0.555
Epoch 38 iteration 0480/1263: training loss 0.554
Epoch 38 iteration 0500/1263: training loss 0.553
Epoch 38 iteration 0520/1263: training loss 0.552
Epoch 38 iteration 0540/1263: training loss 0.551
Epoch 38 iteration 0560/1263: training loss 0.550
Epoch 38 iteration 0580/1263: training loss 0.550
Epoch 38 iteration 0600/1263: training loss 0.548
Epoch 38 iteration 0620/1263: training loss 0.549
Epoch 38 iteration 0640/1263: training loss 0.548
Epoch 38 iteration 0660/1263: training loss 0.548
Epoch 38 iteration 0680/1263: training loss 0.546
Epoch 38 iteration 0700/1263: training loss 0.547
Epoch 38 iteration 0720/1263: training loss 0.546
Epoch 38 iteration 0740/1263: training loss 0.547
Epoch 38 iteration 0760/1263: training loss 0.546
Epoch 38 iteration 0780/1263: training loss 0.545
Epoch 38 iteration 0800/1263: training loss 0.544
Epoch 38 iteration 0820/1263: training loss 0.544
Epoch 38 iteration 0840/1263: training loss 0.545
Epoch 38 iteration 0860/1263: training loss 0.545
Epoch 38 iteration 0880/1263: training loss 0.545
Epoch 38 iteration 0900/1263: training loss 0.545
Epoch 38 iteration 0920/1263: training loss 0.544
Epoch 38 iteration 0940/1263: training loss 0.544
Epoch 38 iteration 0960/1263: training loss 0.543
Epoch 38 iteration 0980/1263: training loss 0.543
Epoch 38 iteration 1000/1263: training loss 0.543
Epoch 38 iteration 1020/1263: training loss 0.543
Epoch 38 iteration 1040/1263: training loss 0.542
Epoch 38 iteration 1060/1263: training loss 0.542
Epoch 38 iteration 1080/1263: training loss 0.541
Epoch 38 iteration 1100/1263: training loss 0.541
Epoch 38 iteration 1120/1263: training loss 0.542
Epoch 38 iteration 1140/1263: training loss 0.543
Epoch 38 iteration 1160/1263: training loss 0.543
Epoch 38 iteration 1180/1264: training loss 0.542
Epoch 38 iteration 1200/1264: training loss 0.542
Epoch 38 iteration 1220/1264: training loss 0.542
Epoch 38 iteration 1240/1264: training loss 0.542
Epoch 38 iteration 1260/1264: training loss 0.542
Namespace(aux=True, aux_weight=0.5, backbone='resnet50', base_size=520, batch_size=16, checkname='deeplab_resnest269_ade', crop_size=480, ctx=[gpu(0), gpu(1), gpu(2), gpu(3), gpu(4), gpu(5), gpu(6), gpu(7)], dataset='ade20k', dtype='float32', epochs=120, eval=False, kvstore='device', log_interval=20, logging_file='train.log', lr=0.01, mode=None, model='fcn', model_zoo='deeplab_resnet269_ade', momentum=0.9, ngpus=8, no_cuda=False, no_val=False, no_wd=False, norm_kwargs={'num_devices': 8}, norm_layer=<class 'mxnet.gluon.contrib.nn.basic_layers.SyncBatchNorm'>, optimizer='sgd', pretrained=False, resume=None, save_dir='runs/ade20k/fcn/resnet50/', start_epoch=0, syncbn=True, test_batch_size=16, train_split='train', warmup_epochs=0, weight_decay=0.0001, workers=48)
Namespace(aux=True, aux_weight=0.5, backbone='resnet50', base_size=520, batch_size=16, checkname='deeplab_resnest269_ade', crop_size=480, ctx=[gpu(0), gpu(1), gpu(2), gpu(3), gpu(4), gpu(5), gpu(6), gpu(7)], dataset='ade20k', dtype='float32', epochs=120, eval=False, kvstore='device', log_interval=20, logging_file='train.log', lr=0.01, mode=None, model='fcn', model_zoo='deeplab_resnest269_ade', momentum=0.9, ngpus=8, no_cuda=False, no_val=False, no_wd=False, norm_kwargs={'num_devices': 8}, norm_layer=<class 'mxnet.gluon.contrib.nn.basic_layers.SyncBatchNorm'>, optimizer='sgd', pretrained=False, resume=None, save_dir='runs/ade20k/fcn/resnet50/', start_epoch=0, syncbn=True, test_batch_size=16, train_split='train', warmup_epochs=0, weight_decay=0.0001, workers=48)
Model file not found. Downloading.
DeepLabV3(
  (conv1): HybridSequential(
    (0): Conv2D(3 -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_syncbatchnorm0_', in_channels=64)
    (2): Activation(relu)
    (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (4): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_syncbatchnorm1_', in_channels=64)
    (5): Activation(relu)
    (6): Conv2D(64 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_syncbatchnorm2_', in_channels=128)
  (relu): Activation(relu)
  (maxpool): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)
  (layer1): HybridSequential(
    (0): Bottleneck(
      (conv1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm0_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(32 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm1_', in_channels=128)
        (relu): Activation(relu)
        (fc1): Conv2D(64 -> 32, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm2_', in_channels=32)
        (relu1): Activation(relu)
        (fc2): Conv2D(32 -> 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm3_', in_channels=256)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(128 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down1_syncbatchnorm0_', in_channels=256)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm4_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(32 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm5_', in_channels=128)
        (relu): Activation(relu)
        (fc1): Conv2D(64 -> 32, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm6_', in_channels=32)
        (relu1): Activation(relu)
        (fc2): Conv2D(32 -> 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm7_', in_channels=256)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm8_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(32 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm9_', in_channels=128)
        (relu): Activation(relu)
        (fc1): Conv2D(64 -> 32, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm10_', in_channels=32)
        (relu1): Activation(relu)
        (fc2): Conv2D(32 -> 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers1_syncbatchnorm11_', in_channels=256)
      (relu3): Activation(relu)
    )
  )
  (layer2): HybridSequential(
    (0): Bottleneck(
      (conv1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm0_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm1_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm2_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm3_', in_channels=512)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down2_syncbatchnorm0_', in_channels=512)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm4_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm5_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm6_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm7_', in_channels=512)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm8_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm9_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm10_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm11_', in_channels=512)
      (relu3): Activation(relu)
    )
    (3): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm12_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm13_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm14_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm15_', in_channels=512)
      (relu3): Activation(relu)
    )
    (4): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm16_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm17_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm18_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm19_', in_channels=512)
      (relu3): Activation(relu)
    )
    (5): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm20_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm21_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm22_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm23_', in_channels=512)
      (relu3): Activation(relu)
    )
    (6): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm24_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm25_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm26_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm27_', in_channels=512)
      (relu3): Activation(relu)
    )
    (7): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm28_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm29_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm30_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm31_', in_channels=512)
      (relu3): Activation(relu)
    )
    (8): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm32_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm33_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm34_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm35_', in_channels=512)
      (relu3): Activation(relu)
    )
    (9): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm36_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm37_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm38_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm39_', in_channels=512)
      (relu3): Activation(relu)
    )
    (10): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm40_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm41_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm42_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm43_', in_channels=512)
      (relu3): Activation(relu)
    )
    (11): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm44_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm45_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm46_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm47_', in_channels=512)
      (relu3): Activation(relu)
    )
    (12): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm48_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm49_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm50_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm51_', in_channels=512)
      (relu3): Activation(relu)
    )
    (13): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm52_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm53_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm54_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm55_', in_channels=512)
      (relu3): Activation(relu)
    )
    (14): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm56_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm57_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm58_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm59_', in_channels=512)
      (relu3): Activation(relu)
    )
    (15): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm60_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm61_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm62_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm63_', in_channels=512)
      (relu3): Activation(relu)
    )
    (16): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm64_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm65_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm66_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm67_', in_channels=512)
      (relu3): Activation(relu)
    )
    (17): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm68_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm69_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm70_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm71_', in_channels=512)
      (relu3): Activation(relu)
    )
    (18): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm72_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm73_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm74_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm75_', in_channels=512)
      (relu3): Activation(relu)
    )
    (19): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm76_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm77_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm78_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm79_', in_channels=512)
      (relu3): Activation(relu)
    )
    (20): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm80_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm81_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm82_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm83_', in_channels=512)
      (relu3): Activation(relu)
    )
    (21): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm84_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm85_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm86_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm87_', in_channels=512)
      (relu3): Activation(relu)
    )
    (22): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm88_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm89_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm90_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm91_', in_channels=512)
      (relu3): Activation(relu)
    )
    (23): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm92_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm93_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm94_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm95_', in_channels=512)
      (relu3): Activation(relu)
    )
    (24): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm96_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm97_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm98_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm99_', in_channels=512)
      (relu3): Activation(relu)
    )
    (25): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm100_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm101_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm102_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm103_', in_channels=512)
      (relu3): Activation(relu)
    )
    (26): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm104_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm105_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm106_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm107_', in_channels=512)
      (relu3): Activation(relu)
    )
    (27): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm108_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm109_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm110_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm111_', in_channels=512)
      (relu3): Activation(relu)
    )
    (28): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm112_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm113_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm114_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm115_', in_channels=512)
      (relu3): Activation(relu)
    )
    (29): Bottleneck(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm116_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm117_', in_channels=256)
        (relu): Activation(relu)
        (fc1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm118_', in_channels=64)
        (relu1): Activation(relu)
        (fc2): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers2_syncbatchnorm119_', in_channels=512)
      (relu3): Activation(relu)
    )
  )
  (layer3): HybridSequential(
    (0): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm0_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm1_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm2_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm3_', in_channels=1024)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(512 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down3_syncbatchnorm0_', in_channels=1024)
      )
    )
    (1): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm4_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm5_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm6_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm7_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm8_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm9_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm10_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm11_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (3): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm12_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm13_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm14_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm15_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (4): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm16_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm17_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm18_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm19_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (5): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm20_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm21_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm22_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm23_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (6): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm24_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm25_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm26_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm27_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (7): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm28_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm29_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm30_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm31_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (8): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm32_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm33_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm34_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm35_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (9): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm36_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm37_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm38_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm39_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (10): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm40_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm41_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm42_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm43_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (11): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm44_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm45_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm46_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm47_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (12): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm48_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm49_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm50_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm51_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (13): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm52_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm53_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm54_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm55_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (14): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm56_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm57_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm58_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm59_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (15): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm60_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm61_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm62_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm63_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (16): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm64_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm65_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm66_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm67_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (17): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm68_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm69_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm70_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm71_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (18): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm72_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm73_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm74_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm75_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (19): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm76_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm77_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm78_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm79_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (20): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm80_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm81_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm82_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm83_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (21): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm84_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm85_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm86_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm87_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (22): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm88_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm89_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm90_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm91_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (23): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm92_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm93_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm94_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm95_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (24): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm96_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm97_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm98_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm99_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (25): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm100_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm101_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm102_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm103_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (26): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm104_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm105_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm106_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm107_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (27): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm108_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm109_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm110_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm111_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (28): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm112_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm113_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm114_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm115_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (29): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm116_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm117_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm118_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm119_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (30): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm120_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm121_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm122_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm123_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (31): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm124_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm125_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm126_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm127_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (32): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm128_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm129_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm130_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm131_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (33): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm132_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm133_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm134_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm135_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (34): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm136_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm137_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm138_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm139_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (35): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm140_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm141_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm142_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm143_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (36): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm144_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm145_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm146_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm147_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (37): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm148_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm149_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm150_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm151_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (38): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm152_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm153_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm154_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm155_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (39): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm156_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm157_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm158_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm159_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (40): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm160_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm161_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm162_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm163_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (41): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm164_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm165_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm166_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm167_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (42): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm168_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm169_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm170_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm171_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (43): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm172_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm173_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm174_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm175_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (44): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm176_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm177_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm178_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm179_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (45): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm180_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm181_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm182_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm183_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (46): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm184_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm185_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm186_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm187_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (47): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm188_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(128 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm189_', in_channels=512)
        (relu): Activation(relu)
        (fc1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm190_', in_channels=128)
        (relu1): Activation(relu)
        (fc2): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers3_syncbatchnorm191_', in_channels=1024)
      (relu3): Activation(relu)
    )
  )
  (layer4): HybridSequential(
    (0): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(1024 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm0_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm1_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm2_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm3_', in_channels=2048)
      (avd_layer): AvgPool2D(size=(3, 3), stride=(1, 1), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): AvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=False, pool_type=avg, layout=NCHW)
        (1): Conv2D(1024 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_down4_syncbatchnorm0_', in_channels=2048)
      )
    )
    (1): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm4_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm5_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm6_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm7_', in_channels=2048)
      (relu3): Activation(relu)
    )
    (2): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm8_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm9_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm10_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm11_', in_channels=2048)
      (relu3): Activation(relu)
    )
    (3): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm12_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm13_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm14_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm15_', in_channels=2048)
      (relu3): Activation(relu)
    )
    (4): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm16_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm17_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm18_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm19_', in_channels=2048)
      (relu3): Activation(relu)
    )
    (5): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm20_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm21_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm22_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm23_', in_channels=2048)
      (relu3): Activation(relu)
    )
    (6): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm24_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm25_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm26_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm27_', in_channels=2048)
      (relu3): Activation(relu)
    )
    (7): Bottleneck(
      (dropblock1): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock2): DropBlock(drop_prob: 0.0, block_size3)
      (dropblock3): DropBlock(drop_prob: 0.0, block_size3)
      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm28_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): SplitAttentionConv(
        (conv): Conv2D(256 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), groups=2, bias=False)
        (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm29_', in_channels=1024)
        (relu): Activation(relu)
        (fc1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm30_', in_channels=256)
        (relu1): Activation(relu)
        (fc2): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30_resnest_layers4_syncbatchnorm31_', in_channels=2048)
      (relu3): Activation(relu)
    )
  )
  (head): _DeepLabHead(
    (aspp): _ASPP(
      (concurent): HybridConcurrent(
        (0): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential0_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (1): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential1_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (2): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential2_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (3): HybridSequential(
          (0): Conv2D(2048 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)
          (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential3_syncbatchnorm0_', in_channels=256)
          (2): Activation(relu)
        )
        (4): _AsppPooling(
          (gap): HybridSequential(
            (0): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=True, pool_type=avg, layout=NCHW)
            (1): Conv2D(2048 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential4_syncbatchnorm0_', in_channels=256)
            (3): Activation(relu)
          )
        )
      )
      (project): HybridSequential(
        (0): Conv2D(1280 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_hybridsequential5_syncbatchnorm0_', in_channels=256)
        (2): Activation(relu)
        (3): Dropout(p = 0.5, axes=())
      )
    )
    (block): HybridSequential(
      (0): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__deeplabhead0_syncbatchnorm0_', in_channels=256)
      (2): Activation(relu)
      (3): Dropout(p = 0.1, axes=())
      (4): Conv2D(256 -> 150, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (auxlayer): _FCNHead(
    (block): HybridSequential(
      (0): Conv2D(1024 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=8, key='deeplabv30__fcnhead0_hybridsequential0_syncbatchnorm0_', in_channels=256)
      (2): Activation(relu)
      (3): Dropout(p = 0.1, axes=())
      (4): Conv2D(256 -> 150, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
Starting Epoch: 0
Total Epochs: 120
Epoch 0 iteration 0020/1263: training loss 4.315
Epoch 0 iteration 0040/1263: training loss 3.758
Epoch 0 iteration 0060/1263: training loss 3.360
Epoch 0 iteration 0080/1263: training loss 3.104
Epoch 0 iteration 0100/1263: training loss 2.940
Epoch 0 iteration 0120/1263: training loss 2.862
Epoch 0 iteration 0140/1263: training loss 2.780
Epoch 0 iteration 0160/1263: training loss 2.703
Epoch 0 iteration 0180/1263: training loss 2.626
Epoch 0 iteration 0200/1263: training loss 2.570
Epoch 0 iteration 0220/1263: training loss 2.515
Epoch 0 iteration 0240/1263: training loss 2.470
Epoch 0 iteration 0260/1263: training loss 2.433
Epoch 0 iteration 0280/1263: training loss 2.399
Epoch 0 iteration 0300/1263: training loss 2.361
Epoch 0 iteration 0320/1263: training loss 2.325
Epoch 0 iteration 0340/1263: training loss 2.291
Epoch 0 iteration 0360/1263: training loss 2.263
Epoch 0 iteration 0380/1263: training loss 2.247
Epoch 0 iteration 0400/1263: training loss 2.227
Epoch 0 iteration 0420/1263: training loss 2.197
Epoch 0 iteration 0440/1263: training loss 2.177
Epoch 0 iteration 0460/1263: training loss 2.162
Epoch 0 iteration 0480/1263: training loss 2.149
Epoch 0 iteration 0500/1263: training loss 2.135
Epoch 0 iteration 0520/1263: training loss 2.121
Epoch 0 iteration 0540/1263: training loss 2.099
Epoch 0 iteration 0560/1263: training loss 2.081
Epoch 0 iteration 0580/1263: training loss 2.063
Epoch 0 iteration 0600/1263: training loss 2.050
Epoch 0 iteration 0620/1263: training loss 2.037
Epoch 0 iteration 0640/1263: training loss 2.020
Epoch 0 iteration 0660/1263: training loss 2.007
Epoch 0 iteration 0680/1263: training loss 1.998
Epoch 0 iteration 0700/1263: training loss 1.989
Epoch 0 iteration 0720/1263: training loss 1.980
Epoch 0 iteration 0740/1263: training loss 1.966
Epoch 0 iteration 0760/1263: training loss 1.956
Epoch 0 iteration 0780/1263: training loss 1.947
Epoch 0 iteration 0800/1263: training loss 1.939
Epoch 0 iteration 0820/1263: training loss 1.930
Epoch 0 iteration 0840/1263: training loss 1.920
Epoch 0 iteration 0860/1263: training loss 1.914
Epoch 0 iteration 0880/1263: training loss 1.903
Epoch 0 iteration 0900/1263: training loss 1.893
Epoch 0 iteration 0920/1263: training loss 1.886
Epoch 0 iteration 0940/1263: training loss 1.877
Epoch 0 iteration 0960/1263: training loss 1.870
Epoch 0 iteration 0980/1263: training loss 1.862
Epoch 0 iteration 1000/1263: training loss 1.852
Epoch 0 iteration 1020/1263: training loss 1.844
Epoch 0 iteration 1040/1263: training loss 1.836
Epoch 0 iteration 1060/1263: training loss 1.829
Epoch 0 iteration 1080/1263: training loss 1.822
Epoch 0 iteration 1100/1263: training loss 1.817
Epoch 0 iteration 1120/1263: training loss 1.811
Epoch 0 iteration 1140/1263: training loss 1.802
Epoch 0 iteration 1160/1263: training loss 1.794
Epoch 0 iteration 1180/1263: training loss 1.786
Epoch 0 iteration 1200/1263: training loss 1.779
Epoch 0 iteration 1220/1263: training loss 1.774
Epoch 0 iteration 1240/1263: training loss 1.767
Epoch 0 iteration 1260/1263: training loss 1.762
Epoch 0 validation pixAcc: 0.688, mIoU: 0.209
Epoch 1 iteration 0020/1263: training loss 1.313
Epoch 1 iteration 0040/1263: training loss 1.304
Epoch 1 iteration 0060/1263: training loss 1.304
Epoch 1 iteration 0080/1263: training loss 1.321
Epoch 1 iteration 0100/1263: training loss 1.315
Epoch 1 iteration 0120/1263: training loss 1.312
Epoch 1 iteration 0140/1263: training loss 1.314
Epoch 1 iteration 0160/1263: training loss 1.309
Epoch 1 iteration 0180/1263: training loss 1.311
Epoch 1 iteration 0200/1263: training loss 1.314
Epoch 1 iteration 0220/1263: training loss 1.314
Epoch 1 iteration 0240/1263: training loss 1.331
Epoch 1 iteration 0260/1263: training loss 1.328
Epoch 1 iteration 0280/1263: training loss 1.326
Epoch 1 iteration 0300/1263: training loss 1.325
Epoch 1 iteration 0320/1263: training loss 1.331
Epoch 1 iteration 0340/1263: training loss 1.333
Epoch 1 iteration 0360/1263: training loss 1.331
Epoch 1 iteration 0380/1263: training loss 1.331
Epoch 1 iteration 0400/1263: training loss 1.320
Epoch 1 iteration 0420/1263: training loss 1.319
Epoch 1 iteration 0440/1263: training loss 1.316
Epoch 1 iteration 0460/1263: training loss 1.310
Epoch 1 iteration 0480/1263: training loss 1.310
Epoch 1 iteration 0500/1263: training loss 1.306
Epoch 1 iteration 0520/1263: training loss 1.304
Epoch 1 iteration 0540/1263: training loss 1.306
Epoch 1 iteration 0560/1263: training loss 1.306
Epoch 1 iteration 0580/1263: training loss 1.305
Epoch 1 iteration 0600/1263: training loss 1.305
Epoch 1 iteration 0620/1263: training loss 1.305
Epoch 1 iteration 0640/1263: training loss 1.306
Epoch 1 iteration 0660/1263: training loss 1.304
Epoch 1 iteration 0680/1263: training loss 1.305
Epoch 1 iteration 0700/1263: training loss 1.299
Epoch 1 iteration 0720/1263: training loss 1.301
Epoch 1 iteration 0740/1263: training loss 1.298
Epoch 1 iteration 0760/1263: training loss 1.296
Epoch 1 iteration 0780/1263: training loss 1.295
Epoch 1 iteration 0800/1263: training loss 1.295
Epoch 1 iteration 0820/1263: training loss 1.294
Epoch 1 iteration 0840/1263: training loss 1.295
Epoch 1 iteration 0860/1263: training loss 1.294
Epoch 1 iteration 0880/1263: training loss 1.293
Epoch 1 iteration 0900/1263: training loss 1.290
Epoch 1 iteration 0920/1263: training loss 1.292
Epoch 1 iteration 0940/1263: training loss 1.292
Epoch 1 iteration 0960/1263: training loss 1.292
Epoch 1 iteration 0980/1263: training loss 1.292
Epoch 1 iteration 1000/1263: training loss 1.291
Epoch 1 iteration 1020/1263: training loss 1.289
Epoch 1 iteration 1040/1263: training loss 1.286
Epoch 1 iteration 1060/1263: training loss 1.285
Epoch 1 iteration 1080/1263: training loss 1.282
Epoch 1 iteration 1100/1263: training loss 1.281
Epoch 1 iteration 1120/1263: training loss 1.279
Epoch 1 iteration 1140/1263: training loss 1.278
Epoch 1 iteration 1160/1263: training loss 1.277
Epoch 1 iteration 1180/1263: training loss 1.274
Epoch 1 iteration 1200/1263: training loss 1.273
Epoch 1 iteration 1220/1263: training loss 1.271
Epoch 1 iteration 1240/1263: training loss 1.271
Epoch 1 iteration 1260/1263: training loss 1.268
Epoch 1 validation pixAcc: 0.718, mIoU: 0.272
Epoch 2 iteration 0020/1263: training loss 1.081
Epoch 2 iteration 0040/1263: training loss 1.099
Epoch 2 iteration 0060/1263: training loss 1.100
Epoch 2 iteration 0080/1263: training loss 1.106
Epoch 2 iteration 0100/1263: training loss 1.117
Epoch 2 iteration 0120/1263: training loss 1.105
Epoch 2 iteration 0140/1263: training loss 1.097
Epoch 2 iteration 0160/1263: training loss 1.100
Epoch 2 iteration 0180/1263: training loss 1.102
Epoch 2 iteration 0200/1263: training loss 1.108
Epoch 2 iteration 0220/1263: training loss 1.105
Epoch 2 iteration 0240/1263: training loss 1.109
Epoch 2 iteration 0260/1263: training loss 1.108
Epoch 2 iteration 0280/1263: training loss 1.110
Epoch 2 iteration 0300/1263: training loss 1.120
Epoch 2 iteration 0320/1263: training loss 1.122
Epoch 2 iteration 0340/1263: training loss 1.126
Epoch 2 iteration 0360/1263: training loss 1.128
Epoch 2 iteration 0380/1263: training loss 1.136
Epoch 2 iteration 0400/1263: training loss 1.132
Epoch 2 iteration 0420/1263: training loss 1.132
Epoch 2 iteration 0440/1263: training loss 1.134
Epoch 2 iteration 0460/1263: training loss 1.134
Epoch 2 iteration 0480/1263: training loss 1.131
Epoch 2 iteration 0500/1263: training loss 1.131
Epoch 2 iteration 0520/1263: training loss 1.132
Epoch 2 iteration 0540/1263: training loss 1.133
Epoch 2 iteration 0560/1263: training loss 1.132
Epoch 2 iteration 0580/1263: training loss 1.130
Epoch 2 iteration 0600/1263: training loss 1.130
Epoch 2 iteration 0620/1263: training loss 1.131
Epoch 2 iteration 0640/1263: training loss 1.133
Epoch 2 iteration 0660/1263: training loss 1.131
Epoch 2 iteration 0680/1263: training loss 1.134
Epoch 2 iteration 0700/1263: training loss 1.131
Epoch 2 iteration 0720/1263: training loss 1.132
Epoch 2 iteration 0740/1263: training loss 1.132
Epoch 2 iteration 0760/1263: training loss 1.131
Epoch 2 iteration 0780/1263: training loss 1.131
Epoch 2 iteration 0800/1263: training loss 1.133
Epoch 2 iteration 0820/1263: training loss 1.130
Epoch 2 iteration 0840/1263: training loss 1.129
Epoch 2 iteration 0860/1263: training loss 1.129
Epoch 2 iteration 0880/1263: training loss 1.127
Epoch 2 iteration 0900/1263: training loss 1.123
Epoch 2 iteration 0920/1263: training loss 1.122
Epoch 2 iteration 0940/1263: training loss 1.124
Epoch 2 iteration 0960/1263: training loss 1.124
Epoch 2 iteration 0980/1263: training loss 1.127
Epoch 2 iteration 1000/1263: training loss 1.129
Epoch 2 iteration 1020/1263: training loss 1.129
Epoch 2 iteration 1040/1263: training loss 1.128
Epoch 2 iteration 1060/1263: training loss 1.129
Epoch 2 iteration 1080/1263: training loss 1.128
Epoch 2 iteration 1100/1263: training loss 1.127
Epoch 2 iteration 1120/1263: training loss 1.127
Epoch 2 iteration 1140/1263: training loss 1.128
Epoch 2 iteration 1160/1263: training loss 1.128
Epoch 2 iteration 1180/1263: training loss 1.127
Epoch 2 iteration 1200/1263: training loss 1.126
Epoch 2 iteration 1220/1263: training loss 1.128
Epoch 2 iteration 1240/1263: training loss 1.129
Epoch 2 iteration 1260/1263: training loss 1.129
Epoch 2 validation pixAcc: 0.729, mIoU: 0.307
Epoch 3 iteration 0020/1263: training loss 1.101
Epoch 3 iteration 0040/1263: training loss 1.085
Epoch 3 iteration 0060/1263: training loss 1.071
Epoch 3 iteration 0080/1263: training loss 1.046
Epoch 3 iteration 0100/1263: training loss 1.051
Epoch 3 iteration 0120/1263: training loss 1.045
Epoch 3 iteration 0140/1263: training loss 1.032
Epoch 3 iteration 0160/1263: training loss 1.038
Epoch 3 iteration 0180/1263: training loss 1.036
Epoch 3 iteration 0200/1263: training loss 1.035
Epoch 3 iteration 0220/1263: training loss 1.041
Epoch 3 iteration 0240/1263: training loss 1.047
Epoch 3 iteration 0260/1263: training loss 1.043
Epoch 3 iteration 0280/1263: training loss 1.039
Epoch 3 iteration 0300/1263: training loss 1.036
Epoch 3 iteration 0320/1263: training loss 1.034
Epoch 3 iteration 0340/1263: training loss 1.030
Epoch 3 iteration 0360/1263: training loss 1.031
Epoch 3 iteration 0380/1263: training loss 1.032
Epoch 3 iteration 0400/1263: training loss 1.031
Epoch 3 iteration 0420/1263: training loss 1.035
Epoch 3 iteration 0440/1263: training loss 1.035
Epoch 3 iteration 0460/1263: training loss 1.038
Epoch 3 iteration 0480/1263: training loss 1.045
Epoch 3 iteration 0500/1263: training loss 1.045
Epoch 3 iteration 0520/1263: training loss 1.047
Epoch 3 iteration 0540/1263: training loss 1.045
Epoch 3 iteration 0560/1263: training loss 1.043
Epoch 3 iteration 0580/1263: training loss 1.048
Epoch 3 iteration 0600/1263: training loss 1.047
Epoch 3 iteration 0620/1263: training loss 1.046
Epoch 3 iteration 0640/1263: training loss 1.048
Epoch 3 iteration 0660/1263: training loss 1.050
Epoch 3 iteration 0680/1263: training loss 1.050
Epoch 3 iteration 0700/1263: training loss 1.051
Epoch 3 iteration 0720/1263: training loss 1.052
Epoch 3 iteration 0740/1263: training loss 1.051
Epoch 3 iteration 0760/1263: training loss 1.051
Epoch 3 iteration 0780/1263: training loss 1.049
Epoch 3 iteration 0800/1263: training loss 1.048
Epoch 3 iteration 0820/1263: training loss 1.048
Epoch 3 iteration 0840/1263: training loss 1.050
Epoch 3 iteration 0860/1263: training loss 1.050
Epoch 3 iteration 0880/1263: training loss 1.051
Epoch 3 iteration 0900/1263: training loss 1.051
Epoch 3 iteration 0920/1263: training loss 1.052
Epoch 3 iteration 0940/1263: training loss 1.052
Epoch 3 iteration 0960/1263: training loss 1.051
Epoch 3 iteration 0980/1263: training loss 1.050
Epoch 3 iteration 1000/1263: training loss 1.051
Epoch 3 iteration 1020/1263: training loss 1.052
Epoch 3 iteration 1040/1263: training loss 1.053
Epoch 3 iteration 1060/1263: training loss 1.053
Epoch 3 iteration 1080/1263: training loss 1.053
Epoch 3 iteration 1100/1263: training loss 1.055
Epoch 3 iteration 1120/1263: training loss 1.055
Epoch 3 iteration 1140/1263: training loss 1.056
Epoch 3 iteration 1160/1263: training loss 1.055
Epoch 3 iteration 1180/1263: training loss 1.055
Epoch 3 iteration 1200/1263: training loss 1.056
Epoch 3 iteration 1220/1263: training loss 1.056
Epoch 3 iteration 1240/1263: training loss 1.055
Epoch 3 iteration 1260/1263: training loss 1.054
Epoch 3 validation pixAcc: 0.746, mIoU: 0.321
Epoch 4 iteration 0020/1263: training loss 0.874
Epoch 4 iteration 0040/1263: training loss 0.937
Epoch 4 iteration 0060/1263: training loss 0.943
Epoch 4 iteration 0080/1263: training loss 0.975
Epoch 4 iteration 0100/1263: training loss 0.957
Epoch 4 iteration 0120/1263: training loss 0.952
Epoch 4 iteration 0140/1263: training loss 0.954
Epoch 4 iteration 0160/1263: training loss 0.957
Epoch 4 iteration 0180/1263: training loss 0.964
Epoch 4 iteration 0200/1263: training loss 0.960
Epoch 4 iteration 0220/1263: training loss 0.955
Epoch 4 iteration 0240/1263: training loss 0.945
Epoch 4 iteration 0260/1263: training loss 0.944
Epoch 4 iteration 0280/1263: training loss 0.942
Epoch 4 iteration 0300/1263: training loss 0.949
Epoch 4 iteration 0320/1263: training loss 0.948
Epoch 4 iteration 0340/1263: training loss 0.950
Epoch 4 iteration 0360/1263: training loss 0.954
Epoch 4 iteration 0380/1263: training loss 0.957
Epoch 4 iteration 0400/1263: training loss 0.954
Epoch 4 iteration 0420/1263: training loss 0.954
Epoch 4 iteration 0440/1263: training loss 0.954
Epoch 4 iteration 0460/1263: training loss 0.955
Epoch 4 iteration 0480/1263: training loss 0.955
Epoch 4 iteration 0500/1263: training loss 0.956
Epoch 4 iteration 0520/1263: training loss 0.958
Epoch 4 iteration 0540/1263: training loss 0.959
Epoch 4 iteration 0560/1263: training loss 0.959
Epoch 4 iteration 0580/1263: training loss 0.962
Epoch 4 iteration 0600/1263: training loss 0.964
Epoch 4 iteration 0620/1263: training loss 0.964
Epoch 4 iteration 0640/1263: training loss 0.967
Epoch 4 iteration 0660/1263: training loss 0.968
Epoch 4 iteration 0680/1263: training loss 0.972
Epoch 4 iteration 0700/1263: training loss 0.972
Epoch 4 iteration 0720/1263: training loss 0.975
Epoch 4 iteration 0740/1263: training loss 0.978
Epoch 4 iteration 0760/1263: training loss 0.979
Epoch 4 iteration 0780/1263: training loss 0.979
Epoch 4 iteration 0800/1263: training loss 0.981
Epoch 4 iteration 0820/1263: training loss 0.981
Epoch 4 iteration 0840/1263: training loss 0.981
Epoch 4 iteration 0860/1263: training loss 0.981
Epoch 4 iteration 0880/1263: training loss 0.980
Epoch 4 iteration 0900/1263: training loss 0.979
Epoch 4 iteration 0920/1263: training loss 0.979
Epoch 4 iteration 0940/1263: training loss 0.980
Epoch 4 iteration 0960/1263: training loss 0.980
Epoch 4 iteration 0980/1263: training loss 0.979
Epoch 4 iteration 1000/1263: training loss 0.978
Epoch 4 iteration 1020/1263: training loss 0.979
Epoch 4 iteration 1040/1263: training loss 0.979
Epoch 4 iteration 1060/1263: training loss 0.981
Epoch 4 iteration 1080/1263: training loss 0.983
Epoch 4 iteration 1100/1263: training loss 0.984
Epoch 4 iteration 1120/1263: training loss 0.982
Epoch 4 iteration 1140/1263: training loss 0.982
Epoch 4 iteration 1160/1263: training loss 0.983
Epoch 4 iteration 1180/1263: training loss 0.982
Epoch 4 iteration 1200/1263: training loss 0.983
Epoch 4 iteration 1220/1263: training loss 0.982
Epoch 4 iteration 1240/1263: training loss 0.981
Epoch 4 iteration 1260/1263: training loss 0.983
Epoch 4 validation pixAcc: 0.752, mIoU: 0.340
Epoch 5 iteration 0020/1263: training loss 0.947
Epoch 5 iteration 0040/1263: training loss 0.910
Epoch 5 iteration 0060/1263: training loss 0.885
Epoch 5 iteration 0080/1263: training loss 0.902
Epoch 5 iteration 0100/1263: training loss 0.897
Epoch 5 iteration 0120/1263: training loss 0.924
Epoch 5 iteration 0140/1263: training loss 0.925
Epoch 5 iteration 0160/1263: training loss 0.922
Epoch 5 iteration 0180/1263: training loss 0.925
Epoch 5 iteration 0200/1263: training loss 0.923
Epoch 5 iteration 0220/1263: training loss 0.922
Epoch 5 iteration 0240/1263: training loss 0.920
Epoch 5 iteration 0260/1263: training loss 0.924
Epoch 5 iteration 0280/1263: training loss 0.931
Epoch 5 iteration 0300/1263: training loss 0.931
Epoch 5 iteration 0320/1263: training loss 0.933
Epoch 5 iteration 0340/1263: training loss 0.935
Epoch 5 iteration 0360/1263: training loss 0.934
Epoch 5 iteration 0380/1263: training loss 0.936
Epoch 5 iteration 0400/1263: training loss 0.934
Epoch 5 iteration 0420/1263: training loss 0.939
Epoch 5 iteration 0440/1263: training loss 0.939
Epoch 5 iteration 0460/1263: training loss 0.933
Epoch 5 iteration 0480/1263: training loss 0.938
Epoch 5 iteration 0500/1263: training loss 0.937
Epoch 5 iteration 0520/1263: training loss 0.939
Epoch 5 iteration 0540/1263: training loss 0.939
Epoch 5 iteration 0560/1263: training loss 0.938
Epoch 5 iteration 0580/1263: training loss 0.940
Epoch 5 iteration 0600/1263: training loss 0.943
Epoch 5 iteration 0620/1263: training loss 0.941
Epoch 5 iteration 0640/1263: training loss 0.942
Epoch 5 iteration 0660/1263: training loss 0.940
Epoch 5 iteration 0680/1263: training loss 0.940
Epoch 5 iteration 0700/1263: training loss 0.940
Epoch 5 iteration 0720/1263: training loss 0.940
Epoch 5 iteration 0740/1263: training loss 0.941
Epoch 5 iteration 0760/1263: training loss 0.939
Epoch 5 iteration 0780/1263: training loss 0.941
Epoch 5 iteration 0800/1263: training loss 0.940
Epoch 5 iteration 0820/1263: training loss 0.940
Epoch 5 iteration 0840/1263: training loss 0.938
Epoch 5 iteration 0860/1263: training loss 0.937
Epoch 5 iteration 0880/1263: training loss 0.937
Epoch 5 iteration 0900/1263: training loss 0.937
Epoch 5 iteration 0920/1263: training loss 0.937
Epoch 5 iteration 0940/1263: training loss 0.936
Epoch 5 iteration 0960/1263: training loss 0.936
Epoch 5 iteration 0980/1263: training loss 0.935
Epoch 5 iteration 1000/1263: training loss 0.935
Epoch 5 iteration 1020/1263: training loss 0.934
Epoch 5 iteration 1040/1263: training loss 0.935
Epoch 5 iteration 1060/1263: training loss 0.934
Epoch 5 iteration 1080/1263: training loss 0.935
Epoch 5 iteration 1100/1263: training loss 0.936
Epoch 5 iteration 1120/1263: training loss 0.935
Epoch 5 iteration 1140/1263: training loss 0.935
Epoch 5 iteration 1160/1263: training loss 0.935
Epoch 5 iteration 1180/1263: training loss 0.936
Epoch 5 iteration 1200/1263: training loss 0.938
Epoch 5 iteration 1220/1263: training loss 0.937
Epoch 5 iteration 1240/1263: training loss 0.938
Epoch 5 iteration 1260/1263: training loss 0.937
Epoch 5 validation pixAcc: 0.763, mIoU: 0.370
Epoch 6 iteration 0020/1263: training loss 0.918
Epoch 6 iteration 0040/1263: training loss 0.922
Epoch 6 iteration 0060/1263: training loss 0.895
Epoch 6 iteration 0080/1263: training loss 0.881
Epoch 6 iteration 0100/1263: training loss 0.863
Epoch 6 iteration 0120/1263: training loss 0.859
Epoch 6 iteration 0140/1263: training loss 0.854
Epoch 6 iteration 0160/1263: training loss 0.852
Epoch 6 iteration 0180/1263: training loss 0.858
Epoch 6 iteration 0200/1263: training loss 0.867
Epoch 6 iteration 0220/1263: training loss 0.868
Epoch 6 iteration 0240/1263: training loss 0.868
Epoch 6 iteration 0260/1263: training loss 0.865
Epoch 6 iteration 0280/1263: training loss 0.865
Epoch 6 iteration 0300/1263: training loss 0.866
Epoch 6 iteration 0320/1263: training loss 0.863
Epoch 6 iteration 0340/1263: training loss 0.863
Epoch 6 iteration 0360/1263: training loss 0.860
Epoch 6 iteration 0380/1263: training loss 0.863
Epoch 6 iteration 0400/1263: training loss 0.870
Epoch 6 iteration 0420/1263: training loss 0.868
Epoch 6 iteration 0440/1263: training loss 0.868
Epoch 6 iteration 0460/1263: training loss 0.870
Epoch 6 iteration 0480/1263: training loss 0.868
Epoch 6 iteration 0500/1263: training loss 0.870
Epoch 6 iteration 0520/1263: training loss 0.871
Epoch 6 iteration 0540/1263: training loss 0.871
Epoch 6 iteration 0560/1263: training loss 0.873
Epoch 6 iteration 0580/1263: training loss 0.873
Epoch 6 iteration 0600/1263: training loss 0.871
Epoch 6 iteration 0620/1263: training loss 0.868
Epoch 6 iteration 0640/1263: training loss 0.870
Epoch 6 iteration 0660/1263: training loss 0.870
Epoch 6 iteration 0680/1263: training loss 0.869
Epoch 6 iteration 0700/1263: training loss 0.868
Epoch 6 iteration 0720/1263: training loss 0.870
Epoch 6 iteration 0740/1263: training loss 0.872
Epoch 6 iteration 0760/1263: training loss 0.872
Epoch 6 iteration 0780/1263: training loss 0.872
Epoch 6 iteration 0800/1263: training loss 0.876
Epoch 6 iteration 0820/1263: training loss 0.875
Epoch 6 iteration 0840/1263: training loss 0.878
Epoch 6 iteration 0860/1263: training loss 0.878
Epoch 6 iteration 0880/1263: training loss 0.879
Epoch 6 iteration 0900/1263: training loss 0.878
Epoch 6 iteration 0920/1263: training loss 0.879
Epoch 6 iteration 0940/1263: training loss 0.880
Epoch 6 iteration 0960/1263: training loss 0.879
Epoch 6 iteration 0980/1263: training loss 0.879
Epoch 6 iteration 1000/1263: training loss 0.880
Epoch 6 iteration 1020/1263: training loss 0.878
Epoch 6 iteration 1040/1263: training loss 0.878
Epoch 6 iteration 1060/1263: training loss 0.877
Epoch 6 iteration 1080/1263: training loss 0.876
Epoch 6 iteration 1100/1263: training loss 0.876
Epoch 6 iteration 1120/1263: training loss 0.876
Epoch 6 iteration 1140/1263: training loss 0.875
Epoch 6 iteration 1160/1263: training loss 0.876
Epoch 6 iteration 1180/1264: training loss 0.877
Epoch 6 iteration 1200/1264: training loss 0.877
Epoch 6 iteration 1220/1264: training loss 0.877
Epoch 6 iteration 1240/1264: training loss 0.877
Epoch 6 iteration 1260/1264: training loss 0.878
Epoch 6 validation pixAcc: 0.769, mIoU: 0.376
Epoch 7 iteration 0020/1263: training loss 0.748
Epoch 7 iteration 0040/1263: training loss 0.779
Epoch 7 iteration 0060/1263: training loss 0.821
Epoch 7 iteration 0080/1263: training loss 0.813
Epoch 7 iteration 0100/1263: training loss 0.825
Epoch 7 iteration 0120/1263: training loss 0.840
Epoch 7 iteration 0140/1263: training loss 0.835
Epoch 7 iteration 0160/1263: training loss 0.835
Epoch 7 iteration 0180/1263: training loss 0.837
Epoch 7 iteration 0200/1263: training loss 0.834
Epoch 7 iteration 0220/1263: training loss 0.828
Epoch 7 iteration 0240/1263: training loss 0.831
Epoch 7 iteration 0260/1263: training loss 0.835
Epoch 7 iteration 0280/1263: training loss 0.840
Epoch 7 iteration 0300/1263: training loss 0.845
Epoch 7 iteration 0320/1263: training loss 0.849
Epoch 7 iteration 0340/1263: training loss 0.851
Epoch 7 iteration 0360/1263: training loss 0.852
Epoch 7 iteration 0380/1263: training loss 0.853
Epoch 7 iteration 0400/1263: training loss 0.852
Epoch 7 iteration 0420/1263: training loss 0.849
Epoch 7 iteration 0440/1263: training loss 0.849
Epoch 7 iteration 0460/1263: training loss 0.851
Epoch 7 iteration 0480/1263: training loss 0.856
Epoch 7 iteration 0500/1263: training loss 0.857
Epoch 7 iteration 0520/1263: training loss 0.856
Epoch 7 iteration 0540/1263: training loss 0.858
Epoch 7 iteration 0560/1263: training loss 0.859
Epoch 7 iteration 0580/1263: training loss 0.859
Epoch 7 iteration 0600/1263: training loss 0.857
Epoch 7 iteration 0620/1263: training loss 0.857
Epoch 7 iteration 0640/1263: training loss 0.855
Epoch 7 iteration 0660/1263: training loss 0.856
Epoch 7 iteration 0680/1263: training loss 0.855
Epoch 7 iteration 0700/1263: training loss 0.856
Epoch 7 iteration 0720/1263: training loss 0.857
Epoch 7 iteration 0740/1263: training loss 0.859
Epoch 7 iteration 0760/1263: training loss 0.859
Epoch 7 iteration 0780/1263: training loss 0.860
Epoch 7 iteration 0800/1263: training loss 0.861
Epoch 7 iteration 0820/1263: training loss 0.863
Epoch 7 iteration 0840/1263: training loss 0.864
Epoch 7 iteration 0860/1263: training loss 0.863
Epoch 7 iteration 0880/1263: training loss 0.863
Epoch 7 iteration 0900/1263: training loss 0.863
Epoch 7 iteration 0920/1263: training loss 0.864
Epoch 7 iteration 0940/1263: training loss 0.864
Epoch 7 iteration 0960/1263: training loss 0.864
Epoch 7 iteration 0980/1263: training loss 0.863
Epoch 7 iteration 1000/1263: training loss 0.861
Epoch 7 iteration 1020/1263: training loss 0.862
Epoch 7 iteration 1040/1263: training loss 0.863
Epoch 7 iteration 1060/1263: training loss 0.863
Epoch 7 iteration 1080/1263: training loss 0.863
Epoch 7 iteration 1100/1263: training loss 0.863
Epoch 7 iteration 1120/1263: training loss 0.863
Epoch 7 iteration 1140/1263: training loss 0.863
Epoch 7 iteration 1160/1263: training loss 0.862
Epoch 7 iteration 1180/1263: training loss 0.861
Epoch 7 iteration 1200/1263: training loss 0.862
Epoch 7 iteration 1220/1263: training loss 0.861
Epoch 7 iteration 1240/1263: training loss 0.862
Epoch 7 iteration 1260/1263: training loss 0.862
Epoch 7 validation pixAcc: 0.762, mIoU: 0.377
Epoch 8 iteration 0020/1263: training loss 0.733
Epoch 8 iteration 0040/1263: training loss 0.745
Epoch 8 iteration 0060/1263: training loss 0.751
Epoch 8 iteration 0080/1263: training loss 0.772
Epoch 8 iteration 0100/1263: training loss 0.778
Epoch 8 iteration 0120/1263: training loss 0.774
Epoch 8 iteration 0140/1263: training loss 0.771
Epoch 8 iteration 0160/1263: training loss 0.767
Epoch 8 iteration 0180/1263: training loss 0.774
Epoch 8 iteration 0200/1263: training loss 0.779
Epoch 8 iteration 0220/1263: training loss 0.779
Epoch 8 iteration 0240/1263: training loss 0.787
Epoch 8 iteration 0260/1263: training loss 0.787
Epoch 8 iteration 0280/1263: training loss 0.783
Epoch 8 iteration 0300/1263: training loss 0.778
Epoch 8 iteration 0320/1263: training loss 0.778
Epoch 8 iteration 0340/1263: training loss 0.783
Epoch 8 iteration 0360/1263: training loss 0.789
Epoch 8 iteration 0380/1263: training loss 0.792
Epoch 8 iteration 0400/1263: training loss 0.791
Epoch 8 iteration 0420/1263: training loss 0.790
Epoch 8 iteration 0440/1263: training loss 0.790
Epoch 8 iteration 0460/1263: training loss 0.793
Epoch 8 iteration 0480/1263: training loss 0.791
Epoch 8 iteration 0500/1263: training loss 0.792
Epoch 8 iteration 0520/1263: training loss 0.792
Epoch 8 iteration 0540/1263: training loss 0.794
Epoch 8 iteration 0560/1263: training loss 0.794
Epoch 8 iteration 0580/1263: training loss 0.794
Epoch 8 iteration 0600/1263: training loss 0.795
Epoch 8 iteration 0620/1263: training loss 0.796
Epoch 8 iteration 0640/1263: training loss 0.798
Epoch 8 iteration 0660/1263: training loss 0.801
Epoch 8 iteration 0680/1263: training loss 0.805
Epoch 8 iteration 0700/1263: training loss 0.805
Epoch 8 iteration 0720/1263: training loss 0.807
Epoch 8 iteration 0740/1263: training loss 0.809
Epoch 8 iteration 0760/1263: training loss 0.813
Epoch 8 iteration 0780/1263: training loss 0.814
Epoch 8 iteration 0800/1263: training loss 0.813
Epoch 8 iteration 0820/1263: training loss 0.815
Epoch 8 iteration 0840/1263: training loss 0.813
Epoch 8 iteration 0860/1263: training loss 0.812
Epoch 8 iteration 0880/1263: training loss 0.812
Epoch 8 iteration 0900/1263: training loss 0.813
Epoch 8 iteration 0920/1263: training loss 0.811
Epoch 8 iteration 0940/1263: training loss 0.812
Epoch 8 iteration 0960/1263: training loss 0.811
Epoch 8 iteration 0980/1263: training loss 0.811
Epoch 8 iteration 1000/1263: training loss 0.811
Epoch 8 iteration 1020/1263: training loss 0.812
Epoch 8 iteration 1040/1263: training loss 0.813
Epoch 8 iteration 1060/1263: training loss 0.813
Epoch 8 iteration 1080/1263: training loss 0.812
Epoch 8 iteration 1100/1263: training loss 0.811
Epoch 8 iteration 1120/1263: training loss 0.812
Epoch 8 iteration 1140/1263: training loss 0.813
Epoch 8 iteration 1160/1263: training loss 0.813
Epoch 8 iteration 1180/1263: training loss 0.814
Epoch 8 iteration 1200/1263: training loss 0.813
Epoch 8 iteration 1220/1263: training loss 0.812
Epoch 8 iteration 1240/1263: training loss 0.812
Epoch 8 iteration 1260/1263: training loss 0.812
Epoch 8 validation pixAcc: 0.772, mIoU: 0.388
Epoch 9 iteration 0020/1263: training loss 0.790
Epoch 9 iteration 0040/1263: training loss 0.743
Epoch 9 iteration 0060/1263: training loss 0.738
Epoch 9 iteration 0080/1263: training loss 0.738
Epoch 9 iteration 0100/1263: training loss 0.757
Epoch 9 iteration 0120/1263: training loss 0.767
Epoch 9 iteration 0140/1263: training loss 0.766
Epoch 9 iteration 0160/1263: training loss 0.763
Epoch 9 iteration 0180/1263: training loss 0.764
Epoch 9 iteration 0200/1263: training loss 0.763
Epoch 9 iteration 0220/1263: training loss 0.775
Epoch 9 iteration 0240/1263: training loss 0.777
Epoch 9 iteration 0260/1263: training loss 0.785
Epoch 9 iteration 0280/1263: training loss 0.787
Epoch 9 iteration 0300/1263: training loss 0.790
Epoch 9 iteration 0320/1263: training loss 0.794
Epoch 9 iteration 0340/1263: training loss 0.795
Epoch 9 iteration 0360/1263: training loss 0.794
Epoch 9 iteration 0380/1263: training loss 0.798
Epoch 9 iteration 0400/1263: training loss 0.796
Epoch 9 iteration 0420/1263: training loss 0.799
Epoch 9 iteration 0440/1263: training loss 0.796
Epoch 9 iteration 0460/1263: training loss 0.798
Epoch 9 iteration 0480/1263: training loss 0.797
Epoch 9 iteration 0500/1263: training loss 0.795
Epoch 9 iteration 0520/1263: training loss 0.799
Epoch 9 iteration 0540/1263: training loss 0.797
Epoch 9 iteration 0560/1263: training loss 0.794
Epoch 9 iteration 0580/1263: training loss 0.795
Epoch 9 iteration 0600/1263: training loss 0.797
Epoch 9 iteration 0620/1263: training loss 0.796
Epoch 9 iteration 0640/1263: training loss 0.794
Epoch 9 iteration 0660/1263: training loss 0.793
Epoch 9 iteration 0680/1263: training loss 0.796
Epoch 9 iteration 0700/1263: training loss 0.797
Epoch 9 iteration 0720/1263: training loss 0.798
Epoch 9 iteration 0740/1263: training loss 0.797
Epoch 9 iteration 0760/1263: training loss 0.799
Epoch 9 iteration 0780/1263: training loss 0.803
Epoch 9 iteration 0800/1263: training loss 0.802
Epoch 9 iteration 0820/1263: training loss 0.802
Epoch 9 iteration 0840/1263: training loss 0.803
Epoch 9 iteration 0860/1263: training loss 0.804
Epoch 9 iteration 0880/1263: training loss 0.805
Epoch 9 iteration 0900/1263: training loss 0.804
Epoch 9 iteration 0920/1263: training loss 0.802
Epoch 9 iteration 0940/1263: training loss 0.801
Epoch 9 iteration 0960/1263: training loss 0.799
Epoch 9 iteration 0980/1263: training loss 0.799
Epoch 9 iteration 1000/1263: training loss 0.800
Epoch 9 iteration 1020/1263: training loss 0.799
Epoch 9 iteration 1040/1263: training loss 0.800
Epoch 9 iteration 1060/1263: training loss 0.799
Epoch 9 iteration 1080/1263: training loss 0.799
Epoch 9 iteration 1100/1263: training loss 0.798
Epoch 9 iteration 1120/1263: training loss 0.798
Epoch 9 iteration 1140/1263: training loss 0.797
Epoch 9 iteration 1160/1263: training loss 0.799
Epoch 9 iteration 1180/1263: training loss 0.798
Epoch 9 iteration 1200/1263: training loss 0.800
Epoch 9 iteration 1220/1263: training loss 0.801
Epoch 9 iteration 1240/1263: training loss 0.802
Epoch 9 iteration 1260/1263: training loss 0.803
Epoch 9 validation pixAcc: 0.768, mIoU: 0.382
Epoch 10 iteration 0020/1263: training loss 0.829
Epoch 10 iteration 0040/1263: training loss 0.783
Epoch 10 iteration 0060/1263: training loss 0.779
Epoch 10 iteration 0080/1263: training loss 0.767
Epoch 10 iteration 0100/1263: training loss 0.772
Epoch 10 iteration 0120/1263: training loss 0.781
Epoch 10 iteration 0140/1263: training loss 0.776
Epoch 10 iteration 0160/1263: training loss 0.778
Epoch 10 iteration 0180/1263: training loss 0.774
Epoch 10 iteration 0200/1263: training loss 0.762
Epoch 10 iteration 0220/1263: training loss 0.755
Epoch 10 iteration 0240/1263: training loss 0.762
Epoch 10 iteration 0260/1263: training loss 0.759
Epoch 10 iteration 0280/1263: training loss 0.760
Epoch 10 iteration 0300/1263: training loss 0.757
Epoch 10 iteration 0320/1263: training loss 0.760
Epoch 10 iteration 0340/1263: training loss 0.760
Epoch 10 iteration 0360/1263: training loss 0.761
Epoch 10 iteration 0380/1263: training loss 0.766
Epoch 10 iteration 0400/1263: training loss 0.768
Epoch 10 iteration 0420/1263: training loss 0.766
Epoch 10 iteration 0440/1263: training loss 0.764
Epoch 10 iteration 0460/1263: training loss 0.762
Epoch 10 iteration 0480/1263: training loss 0.760
Epoch 10 iteration 0500/1263: training loss 0.758
Epoch 10 iteration 0520/1263: training loss 0.758
Epoch 10 iteration 0540/1263: training loss 0.757
Epoch 10 iteration 0560/1263: training loss 0.756
Epoch 10 iteration 0580/1263: training loss 0.757
Epoch 10 iteration 0600/1263: training loss 0.758
Epoch 10 iteration 0620/1263: training loss 0.759
Epoch 10 iteration 0640/1263: training loss 0.760
Epoch 10 iteration 0660/1263: training loss 0.760
Epoch 10 iteration 0680/1263: training loss 0.759
Epoch 10 iteration 0700/1263: training loss 0.757
Epoch 10 iteration 0720/1263: training loss 0.755
Epoch 10 iteration 0740/1263: training loss 0.756
Epoch 10 iteration 0760/1263: training loss 0.755
Epoch 10 iteration 0780/1263: training loss 0.757
Epoch 10 iteration 0800/1263: training loss 0.757
Epoch 10 iteration 0820/1263: training loss 0.757
Epoch 10 iteration 0840/1263: training loss 0.757
Epoch 10 iteration 0860/1263: training loss 0.758
Epoch 10 iteration 0880/1263: training loss 0.757
Epoch 10 iteration 0900/1263: training loss 0.758
Epoch 10 iteration 0920/1263: training loss 0.760
Epoch 10 iteration 0940/1263: training loss 0.760
Epoch 10 iteration 0960/1263: training loss 0.761
Epoch 10 iteration 0980/1263: training loss 0.762
Epoch 10 iteration 1000/1263: training loss 0.762
Epoch 10 iteration 1020/1263: training loss 0.761
Epoch 10 iteration 1040/1263: training loss 0.760
Epoch 10 iteration 1060/1263: training loss 0.758
Epoch 10 iteration 1080/1263: training loss 0.759
Epoch 10 iteration 1100/1263: training loss 0.761
Epoch 10 iteration 1120/1263: training loss 0.762
Epoch 10 iteration 1140/1263: training loss 0.764
Epoch 10 iteration 1160/1263: training loss 0.764
Epoch 10 iteration 1180/1263: training loss 0.763
Epoch 10 iteration 1200/1263: training loss 0.763
Epoch 10 iteration 1220/1263: training loss 0.764
Epoch 10 iteration 1240/1263: training loss 0.765
Epoch 10 iteration 1260/1263: training loss 0.765
Epoch 10 validation pixAcc: 0.781, mIoU: 0.402
Epoch 11 iteration 0020/1263: training loss 0.747
Epoch 11 iteration 0040/1263: training loss 0.712
Epoch 11 iteration 0060/1263: training loss 0.704
Epoch 11 iteration 0080/1263: training loss 0.712
Epoch 11 iteration 0100/1263: training loss 0.715
Epoch 11 iteration 0120/1263: training loss 0.710
Epoch 11 iteration 0140/1263: training loss 0.710
Epoch 11 iteration 0160/1263: training loss 0.715
Epoch 11 iteration 0180/1263: training loss 0.715
Epoch 11 iteration 0200/1263: training loss 0.721
Epoch 11 iteration 0220/1263: training loss 0.725
Epoch 11 iteration 0240/1263: training loss 0.724
Epoch 11 iteration 0260/1263: training loss 0.722
Epoch 11 iteration 0280/1263: training loss 0.722
Epoch 11 iteration 0300/1263: training loss 0.725
Epoch 11 iteration 0320/1263: training loss 0.725
Epoch 11 iteration 0340/1263: training loss 0.729
Epoch 11 iteration 0360/1263: training loss 0.726
Epoch 11 iteration 0380/1263: training loss 0.726
Epoch 11 iteration 0400/1263: training loss 0.726
Epoch 11 iteration 0420/1263: training loss 0.723
Epoch 11 iteration 0440/1263: training loss 0.723
Epoch 11 iteration 0460/1263: training loss 0.723
Epoch 11 iteration 0480/1263: training loss 0.724
Epoch 11 iteration 0500/1263: training loss 0.727
Epoch 11 iteration 0520/1263: training loss 0.728
Epoch 11 iteration 0540/1263: training loss 0.732
Epoch 11 iteration 0560/1263: training loss 0.732
Epoch 11 iteration 0580/1263: training loss 0.734
Epoch 11 iteration 0600/1263: training loss 0.737
Epoch 11 iteration 0620/1263: training loss 0.738
Epoch 11 iteration 0640/1263: training loss 0.738
Epoch 11 iteration 0660/1263: training loss 0.741
Epoch 11 iteration 0680/1263: training loss 0.745
Epoch 11 iteration 0700/1263: training loss 0.746
Epoch 11 iteration 0720/1263: training loss 0.747
Epoch 11 iteration 0740/1263: training loss 0.749
Epoch 11 iteration 0760/1263: training loss 0.751
Epoch 11 iteration 0780/1263: training loss 0.751
Epoch 11 iteration 0800/1263: training loss 0.750
Epoch 11 iteration 0820/1263: training loss 0.749
Epoch 11 iteration 0840/1263: training loss 0.748
Epoch 11 iteration 0860/1263: training loss 0.750
Epoch 11 iteration 0880/1263: training loss 0.751
Epoch 11 iteration 0900/1263: training loss 0.751
Epoch 11 iteration 0920/1263: training loss 0.750
Epoch 11 iteration 0940/1263: training loss 0.751
Epoch 11 iteration 0960/1263: training loss 0.751
Epoch 11 iteration 0980/1263: training loss 0.753
Epoch 11 iteration 1000/1263: training loss 0.754
Epoch 11 iteration 1020/1263: training loss 0.754
Epoch 11 iteration 1040/1263: training loss 0.755
Epoch 11 iteration 1060/1263: training loss 0.755
Epoch 11 iteration 1080/1263: training loss 0.754
Epoch 11 iteration 1100/1263: training loss 0.753
Epoch 11 iteration 1120/1263: training loss 0.753
Epoch 11 iteration 1140/1263: training loss 0.754
Epoch 11 iteration 1160/1263: training loss 0.754
Epoch 11 iteration 1180/1263: training loss 0.754
Epoch 11 iteration 1200/1263: training loss 0.754
Epoch 11 iteration 1220/1263: training loss 0.756
Epoch 11 iteration 1240/1263: training loss 0.757
Epoch 11 iteration 1260/1263: training loss 0.758
Epoch 11 validation pixAcc: 0.756, mIoU: 0.373
Epoch 12 iteration 0020/1263: training loss 0.812
Epoch 12 iteration 0040/1263: training loss 0.774
Epoch 12 iteration 0060/1263: training loss 0.773
Epoch 12 iteration 0080/1263: training loss 0.753
Epoch 12 iteration 0100/1263: training loss 0.746
Epoch 12 iteration 0120/1263: training loss 0.740
Epoch 12 iteration 0140/1263: training loss 0.731
Epoch 12 iteration 0160/1263: training loss 0.734
Epoch 12 iteration 0180/1263: training loss 0.724
Epoch 12 iteration 0200/1263: training loss 0.720
Epoch 12 iteration 0220/1263: training loss 0.720
Epoch 12 iteration 0240/1263: training loss 0.716
Epoch 12 iteration 0260/1263: training loss 0.715
Epoch 12 iteration 0280/1263: training loss 0.713
Epoch 12 iteration 0300/1263: training loss 0.715
Epoch 12 iteration 0320/1263: training loss 0.714
Epoch 12 iteration 0340/1263: training loss 0.723
Epoch 12 iteration 0360/1263: training loss 0.727
Epoch 12 iteration 0380/1263: training loss 0.727
Epoch 12 iteration 0400/1263: training loss 0.727
Epoch 12 iteration 0420/1263: training loss 0.728
Epoch 12 iteration 0440/1263: training loss 0.725
Epoch 12 iteration 0460/1263: training loss 0.723
Epoch 12 iteration 0480/1263: training loss 0.722
Epoch 12 iteration 0500/1263: training loss 0.721
Epoch 12 iteration 0520/1263: training loss 0.723
Epoch 12 iteration 0540/1263: training loss 0.723
Epoch 12 iteration 0560/1263: training loss 0.723
Epoch 12 iteration 0580/1263: training loss 0.723
Epoch 12 iteration 0600/1263: training loss 0.725
Epoch 12 iteration 0620/1263: training loss 0.727
Epoch 12 iteration 0640/1263: training loss 0.727
Epoch 12 iteration 0660/1263: training loss 0.728
Epoch 12 iteration 0680/1263: training loss 0.727
Epoch 12 iteration 0700/1263: training loss 0.726
Epoch 12 iteration 0720/1263: training loss 0.724
Epoch 12 iteration 0740/1263: training loss 0.726
Epoch 12 iteration 0760/1263: training loss 0.727
Epoch 12 iteration 0780/1263: training loss 0.729
Epoch 12 iteration 0800/1263: training loss 0.731
Epoch 12 iteration 0820/1263: training loss 0.734
Epoch 12 iteration 0840/1263: training loss 0.735
Epoch 12 iteration 0860/1263: training loss 0.737
Epoch 12 iteration 0880/1263: training loss 0.737
Epoch 12 iteration 0900/1263: training loss 0.736
Epoch 12 iteration 0920/1263: training loss 0.739
Epoch 12 iteration 0940/1263: training loss 0.740
Epoch 12 iteration 0960/1263: training loss 0.740
Epoch 12 iteration 0980/1263: training loss 0.740
Epoch 12 iteration 1000/1263: training loss 0.742
Epoch 12 iteration 1020/1263: training loss 0.742
Epoch 12 iteration 1040/1263: training loss 0.741
Epoch 12 iteration 1060/1263: training loss 0.741
Epoch 12 iteration 1080/1263: training loss 0.742
Epoch 12 iteration 1100/1263: training loss 0.742
Epoch 12 iteration 1120/1263: training loss 0.742
Epoch 12 iteration 1140/1263: training loss 0.741
Epoch 12 iteration 1160/1263: training loss 0.742
Epoch 12 iteration 1180/1263: training loss 0.742
Epoch 12 iteration 1200/1263: training loss 0.742
Epoch 12 iteration 1220/1263: training loss 0.741
Epoch 12 iteration 1240/1263: training loss 0.741
Epoch 12 iteration 1260/1263: training loss 0.741
Epoch 12 validation pixAcc: 0.778, mIoU: 0.402
Epoch 13 iteration 0020/1263: training loss 0.625
Epoch 13 iteration 0040/1263: training loss 0.635
Epoch 13 iteration 0060/1263: training loss 0.641
Epoch 13 iteration 0080/1263: training loss 0.638
Epoch 13 iteration 0100/1263: training loss 0.652
Epoch 13 iteration 0120/1263: training loss 0.664
Epoch 13 iteration 0140/1263: training loss 0.664
Epoch 13 iteration 0160/1263: training loss 0.660
Epoch 13 iteration 0180/1263: training loss 0.662
Epoch 13 iteration 0200/1263: training loss 0.662
Epoch 13 iteration 0220/1263: training loss 0.667
Epoch 13 iteration 0240/1263: training loss 0.668
Epoch 13 iteration 0260/1263: training loss 0.669
Epoch 13 iteration 0280/1263: training loss 0.673
Epoch 13 iteration 0300/1263: training loss 0.673
Epoch 13 iteration 0320/1263: training loss 0.675
Epoch 13 iteration 0340/1263: training loss 0.677
Epoch 13 iteration 0360/1263: training loss 0.679
Epoch 13 iteration 0380/1263: training loss 0.679
Epoch 13 iteration 0400/1263: training loss 0.678
Epoch 13 iteration 0420/1263: training loss 0.677
Epoch 13 iteration 0440/1263: training loss 0.677
Epoch 13 iteration 0460/1263: training loss 0.677
Epoch 13 iteration 0480/1263: training loss 0.676
Epoch 13 iteration 0500/1263: training loss 0.678
Epoch 13 iteration 0520/1263: training loss 0.678
Epoch 13 iteration 0540/1263: training loss 0.679
Epoch 13 iteration 0560/1263: training loss 0.681
Epoch 13 iteration 0580/1263: training loss 0.682
Epoch 13 iteration 0600/1263: training loss 0.682
Epoch 13 iteration 0620/1263: training loss 0.685
Epoch 13 iteration 0640/1263: training loss 0.686
Epoch 13 iteration 0660/1263: training loss 0.687
Epoch 13 iteration 0680/1263: training loss 0.689
Epoch 13 iteration 0700/1263: training loss 0.692
Epoch 13 iteration 0720/1263: training loss 0.693
Epoch 13 iteration 0740/1263: training loss 0.692
Epoch 13 iteration 0760/1263: training loss 0.692
Epoch 13 iteration 0780/1263: training loss 0.692
Epoch 13 iteration 0800/1263: training loss 0.694
Epoch 13 iteration 0820/1263: training loss 0.696
Epoch 13 iteration 0840/1263: training loss 0.696
Epoch 13 iteration 0860/1263: training loss 0.696
Epoch 13 iteration 0880/1263: training loss 0.698
Epoch 13 iteration 0900/1263: training loss 0.699
Epoch 13 iteration 0920/1263: training loss 0.700
Epoch 13 iteration 0940/1263: training loss 0.701
Epoch 13 iteration 0960/1263: training loss 0.702
Epoch 13 iteration 0980/1263: training loss 0.701
Epoch 13 iteration 1000/1263: training loss 0.702
Epoch 13 iteration 1020/1263: training loss 0.702
Epoch 13 iteration 1040/1263: training loss 0.702
Epoch 13 iteration 1060/1263: training loss 0.703
Epoch 13 iteration 1080/1263: training loss 0.703
Epoch 13 iteration 1100/1263: training loss 0.704
Epoch 13 iteration 1120/1263: training loss 0.704
Epoch 13 iteration 1140/1263: training loss 0.705
Epoch 13 iteration 1160/1263: training loss 0.704
Epoch 13 iteration 1180/1263: training loss 0.703
Epoch 13 iteration 1200/1263: training loss 0.704
Epoch 13 iteration 1220/1263: training loss 0.705
Epoch 13 iteration 1240/1263: training loss 0.705
Epoch 13 iteration 1260/1263: training loss 0.707
Epoch 13 validation pixAcc: 0.777, mIoU: 0.388
Epoch 14 iteration 0020/1263: training loss 0.663
Epoch 14 iteration 0040/1263: training loss 0.641
Epoch 14 iteration 0060/1263: training loss 0.667
Epoch 14 iteration 0080/1263: training loss 0.675
Epoch 14 iteration 0100/1263: training loss 0.667
Epoch 14 iteration 0120/1263: training loss 0.669
Epoch 14 iteration 0140/1263: training loss 0.673
Epoch 14 iteration 0160/1263: training loss 0.671
Epoch 14 iteration 0180/1263: training loss 0.682
Epoch 14 iteration 0200/1263: training loss 0.680
Epoch 14 iteration 0220/1263: training loss 0.677
Epoch 14 iteration 0240/1263: training loss 0.686
Epoch 14 iteration 0260/1263: training loss 0.689
Epoch 14 iteration 0280/1263: training loss 0.687
Epoch 14 iteration 0300/1263: training loss 0.684
Epoch 14 iteration 0320/1263: training loss 0.681
Epoch 14 iteration 0340/1263: training loss 0.681
Epoch 14 iteration 0360/1263: training loss 0.680
Epoch 14 iteration 0380/1263: training loss 0.678
Epoch 14 iteration 0400/1263: training loss 0.678
Epoch 14 iteration 0420/1263: training loss 0.680
Epoch 14 iteration 0440/1263: training loss 0.681
Epoch 14 iteration 0460/1263: training loss 0.680
Epoch 14 iteration 0480/1263: training loss 0.679
Epoch 14 iteration 0500/1263: training loss 0.682
Epoch 14 iteration 0520/1263: training loss 0.681
Epoch 14 iteration 0540/1263: training loss 0.682
Epoch 14 iteration 0560/1263: training loss 0.684
Epoch 14 iteration 0580/1263: training loss 0.684
Epoch 14 iteration 0600/1263: training loss 0.684
Epoch 14 iteration 0620/1263: training loss 0.686
Epoch 14 iteration 0640/1263: training loss 0.689
Epoch 14 iteration 0660/1263: training loss 0.690
Epoch 14 iteration 0680/1263: training loss 0.691
Epoch 14 iteration 0700/1263: training loss 0.691
Epoch 14 iteration 0720/1263: training loss 0.692
Epoch 14 iteration 0740/1263: training loss 0.691
Epoch 14 iteration 0760/1263: training loss 0.691
Epoch 14 iteration 0780/1263: training loss 0.689
Epoch 14 iteration 0800/1263: training loss 0.692
Epoch 14 iteration 0820/1263: training loss 0.691
Epoch 14 iteration 0840/1263: training loss 0.692
Epoch 14 iteration 0860/1263: training loss 0.691
Epoch 14 iteration 0880/1263: training loss 0.692
Epoch 14 iteration 0900/1263: training loss 0.693
Epoch 14 iteration 0920/1263: training loss 0.692
Epoch 14 iteration 0940/1263: training loss 0.692
Epoch 14 iteration 0960/1263: training loss 0.691
Epoch 14 iteration 0980/1263: training loss 0.691
Epoch 14 iteration 1000/1263: training loss 0.690
Epoch 14 iteration 1020/1263: training loss 0.691
Epoch 14 iteration 1040/1263: training loss 0.691
Epoch 14 iteration 1060/1263: training loss 0.692
Epoch 14 iteration 1080/1263: training loss 0.693
Epoch 14 iteration 1100/1263: training loss 0.693
Epoch 14 iteration 1120/1263: training loss 0.693
Epoch 14 iteration 1140/1263: training loss 0.694
Epoch 14 iteration 1160/1263: training loss 0.694
Epoch 14 iteration 1180/1264: training loss 0.694
Epoch 14 iteration 1200/1264: training loss 0.694
Epoch 14 iteration 1220/1264: training loss 0.693
Epoch 14 iteration 1240/1264: training loss 0.695
Epoch 14 iteration 1260/1264: training loss 0.694
Epoch 14 validation pixAcc: 0.777, mIoU: 0.397
Epoch 15 iteration 0020/1263: training loss 0.660
Epoch 15 iteration 0040/1263: training loss 0.664
Epoch 15 iteration 0060/1263: training loss 0.665
Epoch 15 iteration 0080/1263: training loss 0.654
Epoch 15 iteration 0100/1263: training loss 0.655
Epoch 15 iteration 0120/1263: training loss 0.655
Epoch 15 iteration 0140/1263: training loss 0.655
Epoch 15 iteration 0160/1263: training loss 0.650
Epoch 15 iteration 0180/1263: training loss 0.647
Epoch 15 iteration 0200/1263: training loss 0.653
Epoch 15 iteration 0220/1263: training loss 0.653
Epoch 15 iteration 0240/1263: training loss 0.649
Epoch 15 iteration 0260/1263: training loss 0.652
Epoch 15 iteration 0280/1263: training loss 0.652
Epoch 15 iteration 0300/1263: training loss 0.654
Epoch 15 iteration 0320/1263: training loss 0.657
Epoch 15 iteration 0340/1263: training loss 0.658
Epoch 15 iteration 0360/1263: training loss 0.660
Epoch 15 iteration 0380/1263: training loss 0.663
Epoch 15 iteration 0400/1263: training loss 0.662
Epoch 15 iteration 0420/1263: training loss 0.665
Epoch 15 iteration 0440/1263: training loss 0.663
Epoch 15 iteration 0460/1263: training loss 0.663
Epoch 15 iteration 0480/1263: training loss 0.661
Epoch 15 iteration 0500/1263: training loss 0.658
Epoch 15 iteration 0520/1263: training loss 0.656
Epoch 15 iteration 0540/1263: training loss 0.655
Epoch 15 iteration 0560/1263: training loss 0.656
Epoch 15 iteration 0580/1263: training loss 0.655
Epoch 15 iteration 0600/1263: training loss 0.654
Epoch 15 iteration 0620/1263: training loss 0.652
Epoch 15 iteration 0640/1263: training loss 0.655
Epoch 15 iteration 0660/1263: training loss 0.655
Epoch 15 iteration 0680/1263: training loss 0.654
Epoch 15 iteration 0700/1263: training loss 0.655
Epoch 15 iteration 0720/1263: training loss 0.655
Epoch 15 iteration 0740/1263: training loss 0.656
Epoch 15 iteration 0760/1263: training loss 0.657
Epoch 15 iteration 0780/1263: training loss 0.656
Epoch 15 iteration 0800/1263: training loss 0.657
Epoch 15 iteration 0820/1263: training loss 0.659
Epoch 15 iteration 0840/1263: training loss 0.658
Epoch 15 iteration 0860/1263: training loss 0.658
Epoch 15 iteration 0880/1263: training loss 0.658
Epoch 15 iteration 0900/1263: training loss 0.658
Epoch 15 iteration 0920/1263: training loss 0.658
Epoch 15 iteration 0940/1263: training loss 0.657
Epoch 15 iteration 0960/1263: training loss 0.657
Epoch 15 iteration 0980/1263: training loss 0.657
Epoch 15 iteration 1000/1263: training loss 0.657
Epoch 15 iteration 1020/1263: training loss 0.658
Epoch 15 iteration 1040/1263: training loss 0.661
Epoch 15 iteration 1060/1263: training loss 0.663
Epoch 15 iteration 1080/1263: training loss 0.665
Epoch 15 iteration 1100/1263: training loss 0.666
Epoch 15 iteration 1120/1263: training loss 0.665
Epoch 15 iteration 1140/1263: training loss 0.666
Epoch 15 iteration 1160/1263: training loss 0.666
Epoch 15 iteration 1180/1263: training loss 0.666
Epoch 15 iteration 1200/1263: training loss 0.665
Epoch 15 iteration 1220/1263: training loss 0.667
Epoch 15 iteration 1240/1263: training loss 0.667
Epoch 15 iteration 1260/1263: training loss 0.667
Epoch 15 validation pixAcc: 0.780, mIoU: 0.406
Epoch 16 iteration 0020/1263: training loss 0.697
Epoch 16 iteration 0040/1263: training loss 0.647
Epoch 16 iteration 0060/1263: training loss 0.643
Epoch 16 iteration 0080/1263: training loss 0.628
Epoch 16 iteration 0100/1263: training loss 0.628
Epoch 16 iteration 0120/1263: training loss 0.622
Epoch 16 iteration 0140/1263: training loss 0.621
Epoch 16 iteration 0160/1263: training loss 0.622
Epoch 16 iteration 0180/1263: training loss 0.619
Epoch 16 iteration 0200/1263: training loss 0.617
Epoch 16 iteration 0220/1263: training loss 0.616
Epoch 16 iteration 0240/1263: training loss 0.618
Epoch 16 iteration 0260/1263: training loss 0.618
Epoch 16 iteration 0280/1263: training loss 0.619
Epoch 16 iteration 0300/1263: training loss 0.624
Epoch 16 iteration 0320/1263: training loss 0.626
Epoch 16 iteration 0340/1263: training loss 0.628
Epoch 16 iteration 0360/1263: training loss 0.628
Epoch 16 iteration 0380/1263: training loss 0.629
Epoch 16 iteration 0400/1263: training loss 0.630
Epoch 16 iteration 0420/1263: training loss 0.631
Epoch 16 iteration 0440/1263: training loss 0.631
Epoch 16 iteration 0460/1263: training loss 0.633
Epoch 16 iteration 0480/1263: training loss 0.636
Epoch 16 iteration 0500/1263: training loss 0.637
Epoch 16 iteration 0520/1263: training loss 0.639
Epoch 16 iteration 0540/1263: training loss 0.640
Epoch 16 iteration 0560/1263: training loss 0.643
Epoch 16 iteration 0580/1263: training loss 0.642
Epoch 16 iteration 0600/1263: training loss 0.643
Epoch 16 iteration 0620/1263: training loss 0.643
Epoch 16 iteration 0640/1263: training loss 0.643
Epoch 16 iteration 0660/1263: training loss 0.642
Epoch 16 iteration 0680/1263: training loss 0.643
Epoch 16 iteration 0700/1263: training loss 0.643
Epoch 16 iteration 0720/1263: training loss 0.643
Epoch 16 iteration 0740/1263: training loss 0.642
Epoch 16 iteration 0760/1263: training loss 0.642
Epoch 16 iteration 0780/1263: training loss 0.643
Epoch 16 iteration 0800/1263: training loss 0.644
Epoch 16 iteration 0820/1263: training loss 0.644
Epoch 16 iteration 0840/1263: training loss 0.644
Epoch 16 iteration 0860/1263: training loss 0.645
Epoch 16 iteration 0880/1263: training loss 0.646
Epoch 16 iteration 0900/1263: training loss 0.646
Epoch 16 iteration 0920/1263: training loss 0.646
Epoch 16 iteration 0940/1263: training loss 0.645
Epoch 16 iteration 0960/1263: training loss 0.646
Epoch 16 iteration 0980/1263: training loss 0.646
Epoch 16 iteration 1000/1263: training loss 0.646
Epoch 16 iteration 1020/1263: training loss 0.646
Epoch 16 iteration 1040/1263: training loss 0.646
Epoch 16 iteration 1060/1263: training loss 0.646
Epoch 16 iteration 1080/1263: training loss 0.647
Epoch 16 iteration 1100/1263: training loss 0.647
Epoch 16 iteration 1120/1263: training loss 0.648
Epoch 16 iteration 1140/1263: training loss 0.649
Epoch 16 iteration 1160/1263: training loss 0.650
Epoch 16 iteration 1180/1263: training loss 0.650
Epoch 16 iteration 1200/1263: training loss 0.651
Epoch 16 iteration 1220/1263: training loss 0.650
Epoch 16 iteration 1240/1263: training loss 0.651
Epoch 16 iteration 1260/1263: training loss 0.652
Epoch 16 validation pixAcc: 0.778, mIoU: 0.409
Epoch 17 iteration 0020/1263: training loss 0.661
Epoch 17 iteration 0040/1263: training loss 0.646
Epoch 17 iteration 0060/1263: training loss 0.632
Epoch 17 iteration 0080/1263: training loss 0.622
Epoch 17 iteration 0100/1263: training loss 0.619
Epoch 17 iteration 0120/1263: training loss 0.612
Epoch 17 iteration 0140/1263: training loss 0.617
Epoch 17 iteration 0160/1263: training loss 0.613
Epoch 17 iteration 0180/1263: training loss 0.617
Epoch 17 iteration 0200/1263: training loss 0.614
Epoch 17 iteration 0220/1263: training loss 0.617
Epoch 17 iteration 0240/1263: training loss 0.619
Epoch 17 iteration 0260/1263: training loss 0.620
Epoch 17 iteration 0280/1263: training loss 0.621
Epoch 17 iteration 0300/1263: training loss 0.622
Epoch 17 iteration 0320/1263: training loss 0.622
Epoch 17 iteration 0340/1263: training loss 0.622
Epoch 17 iteration 0360/1263: training loss 0.619
Epoch 17 iteration 0380/1263: training loss 0.622
Epoch 17 iteration 0400/1263: training loss 0.622
Epoch 17 iteration 0420/1263: training loss 0.621
Epoch 17 iteration 0440/1263: training loss 0.622
Epoch 17 iteration 0460/1263: training loss 0.624
Epoch 17 iteration 0480/1263: training loss 0.625
Epoch 17 iteration 0500/1263: training loss 0.626
Epoch 17 iteration 0520/1263: training loss 0.626
Epoch 17 iteration 0540/1263: training loss 0.626
Epoch 17 iteration 0560/1263: training loss 0.627
Epoch 17 iteration 0580/1263: training loss 0.628
Epoch 17 iteration 0600/1263: training loss 0.627
Epoch 17 iteration 0620/1263: training loss 0.625
Epoch 17 iteration 0640/1263: training loss 0.624
Epoch 17 iteration 0660/1263: training loss 0.623
Epoch 17 iteration 0680/1263: training loss 0.623
Epoch 17 iteration 0700/1263: training loss 0.623
Epoch 17 iteration 0720/1263: training loss 0.622
Epoch 17 iteration 0740/1263: training loss 0.622
Epoch 17 iteration 0760/1263: training loss 0.623
Epoch 17 iteration 0780/1263: training loss 0.623
Epoch 17 iteration 0800/1263: training loss 0.625
Epoch 17 iteration 0820/1263: training loss 0.626
Epoch 17 iteration 0840/1263: training loss 0.626
Epoch 17 iteration 0860/1263: training loss 0.625
Epoch 17 iteration 0880/1263: training loss 0.624
Epoch 17 iteration 0900/1263: training loss 0.625
Epoch 17 iteration 0920/1263: training loss 0.625
Epoch 17 iteration 0940/1263: training loss 0.625
Epoch 17 iteration 0960/1263: training loss 0.625
Epoch 17 iteration 0980/1263: training loss 0.626
Epoch 17 iteration 1000/1263: training loss 0.627
Epoch 17 iteration 1020/1263: training loss 0.628
Epoch 17 iteration 1040/1263: training loss 0.630
Epoch 17 iteration 1060/1263: training loss 0.631
Epoch 17 iteration 1080/1263: training loss 0.632
Epoch 17 iteration 1100/1263: training loss 0.634
Epoch 17 iteration 1120/1263: training loss 0.636
Epoch 17 iteration 1140/1263: training loss 0.637
Epoch 17 iteration 1160/1263: training loss 0.636
Epoch 17 iteration 1180/1263: training loss 0.636
Epoch 17 iteration 1200/1263: training loss 0.638
Epoch 17 iteration 1220/1263: training loss 0.638
Epoch 17 iteration 1240/1263: training loss 0.638
Epoch 17 iteration 1260/1263: training loss 0.638
Epoch 17 validation pixAcc: 0.784, mIoU: 0.413
Epoch 18 iteration 0020/1263: training loss 0.644
Epoch 18 iteration 0040/1263: training loss 0.656
Epoch 18 iteration 0060/1263: training loss 0.651
Epoch 18 iteration 0080/1263: training loss 0.642
Epoch 18 iteration 0100/1263: training loss 0.648
Epoch 18 iteration 0120/1263: training loss 0.646
Epoch 18 iteration 0140/1263: training loss 0.650
Epoch 18 iteration 0160/1263: training loss 0.643
Epoch 18 iteration 0180/1263: training loss 0.638
Epoch 18 iteration 0200/1263: training loss 0.635
Epoch 18 iteration 0220/1263: training loss 0.631
Epoch 18 iteration 0240/1263: training loss 0.631
Epoch 18 iteration 0260/1263: training loss 0.632
Epoch 18 iteration 0280/1263: training loss 0.631
Epoch 18 iteration 0300/1263: training loss 0.626
Epoch 18 iteration 0320/1263: training loss 0.625
Epoch 18 iteration 0340/1263: training loss 0.625
Epoch 18 iteration 0360/1263: training loss 0.622
Epoch 18 iteration 0380/1263: training loss 0.623
Epoch 18 iteration 0400/1263: training loss 0.623
Epoch 18 iteration 0420/1263: training loss 0.624
Epoch 18 iteration 0440/1263: training loss 0.622
Epoch 18 iteration 0460/1263: training loss 0.622
Epoch 18 iteration 0480/1263: training loss 0.620
Epoch 18 iteration 0500/1263: training loss 0.620
Epoch 18 iteration 0520/1263: training loss 0.618
Epoch 18 iteration 0540/1263: training loss 0.618
Epoch 18 iteration 0560/1263: training loss 0.619
Epoch 18 iteration 0580/1263: training loss 0.620
Epoch 18 iteration 0600/1263: training loss 0.621
Epoch 18 iteration 0620/1263: training loss 0.623
Epoch 18 iteration 0640/1263: training loss 0.625
Epoch 18 iteration 0660/1263: training loss 0.627
Epoch 18 iteration 0680/1263: training loss 0.628
Epoch 18 iteration 0700/1263: training loss 0.627
Epoch 18 iteration 0720/1263: training loss 0.628
Epoch 18 iteration 0740/1263: training loss 0.627
Epoch 18 iteration 0760/1263: training loss 0.628
Epoch 18 iteration 0780/1263: training loss 0.630
Epoch 18 iteration 0800/1263: training loss 0.630
Epoch 18 iteration 0820/1263: training loss 0.630
Epoch 18 iteration 0840/1263: training loss 0.630
Epoch 18 iteration 0860/1263: training loss 0.630
Epoch 18 iteration 0880/1263: training loss 0.631
Epoch 18 iteration 0900/1263: training loss 0.632
Epoch 18 iteration 0920/1263: training loss 0.633
Epoch 18 iteration 0940/1263: training loss 0.632
Epoch 18 iteration 0960/1263: training loss 0.632
Epoch 18 iteration 0980/1263: training loss 0.632
Epoch 18 iteration 1000/1263: training loss 0.632
Epoch 18 iteration 1020/1263: training loss 0.633
Epoch 18 iteration 1040/1263: training loss 0.634
Epoch 18 iteration 1060/1263: training loss 0.634
Epoch 18 iteration 1080/1263: training loss 0.633
Epoch 18 iteration 1100/1263: training loss 0.631
Epoch 18 iteration 1120/1263: training loss 0.630
Epoch 18 iteration 1140/1263: training loss 0.629
Epoch 18 iteration 1160/1263: training loss 0.630
Epoch 18 iteration 1180/1263: training loss 0.629
Epoch 18 iteration 1200/1263: training loss 0.629
Epoch 18 iteration 1220/1263: training loss 0.630
Epoch 18 iteration 1240/1263: training loss 0.630
Epoch 18 iteration 1260/1263: training loss 0.632
Epoch 18 validation pixAcc: 0.775, mIoU: 0.395
Epoch 19 iteration 0020/1263: training loss 0.590
Epoch 19 iteration 0040/1263: training loss 0.579
Epoch 19 iteration 0060/1263: training loss 0.587
Epoch 19 iteration 0080/1263: training loss 0.582
Epoch 19 iteration 0100/1263: training loss 0.579
Epoch 19 iteration 0120/1263: training loss 0.597
Epoch 19 iteration 0140/1263: training loss 0.591
Epoch 19 iteration 0160/1263: training loss 0.593
Epoch 19 iteration 0180/1263: training loss 0.588
Epoch 19 iteration 0200/1263: training loss 0.585
Epoch 19 iteration 0220/1263: training loss 0.585
Epoch 19 iteration 0240/1263: training loss 0.589
Epoch 19 iteration 0260/1263: training loss 0.588
Epoch 19 iteration 0280/1263: training loss 0.587
Epoch 19 iteration 0300/1263: training loss 0.589
Epoch 19 iteration 0320/1263: training loss 0.589
Epoch 19 iteration 0340/1263: training loss 0.589
Epoch 19 iteration 0360/1263: training loss 0.591
Epoch 19 iteration 0380/1263: training loss 0.593
Epoch 19 iteration 0400/1263: training loss 0.595
Epoch 19 iteration 0420/1263: training loss 0.593
Epoch 19 iteration 0440/1263: training loss 0.593
Epoch 19 iteration 0460/1263: training loss 0.593
Epoch 19 iteration 0480/1263: training loss 0.591
Epoch 19 iteration 0500/1263: training loss 0.594
Epoch 19 iteration 0520/1263: training loss 0.595
Epoch 19 iteration 0540/1263: training loss 0.597
Epoch 19 iteration 0560/1263: training loss 0.599
Epoch 19 iteration 0580/1263: training loss 0.601
Epoch 19 iteration 0600/1263: training loss 0.600
Epoch 19 iteration 0620/1263: training loss 0.602
Epoch 19 iteration 0640/1263: training loss 0.602
Epoch 19 iteration 0660/1263: training loss 0.601
Epoch 19 iteration 0680/1263: training loss 0.601
Epoch 19 iteration 0700/1263: training loss 0.603
Epoch 19 iteration 0720/1263: training loss 0.605
Epoch 19 iteration 0740/1263: training loss 0.606
Epoch 19 iteration 0760/1263: training loss 0.606
Epoch 19 iteration 0780/1263: training loss 0.608
Epoch 19 iteration 0800/1263: training loss 0.609
Epoch 19 iteration 0820/1263: training loss 0.609
Epoch 19 iteration 0840/1263: training loss 0.609
Epoch 19 iteration 0860/1263: training loss 0.611
Epoch 19 iteration 0880/1263: training loss 0.611
Epoch 19 iteration 0900/1263: training loss 0.612
Epoch 19 iteration 0920/1263: training loss 0.613
Epoch 19 iteration 0940/1263: training loss 0.613
Epoch 19 iteration 0960/1263: training loss 0.612
Epoch 19 iteration 0980/1263: training loss 0.610
Epoch 19 iteration 1000/1263: training loss 0.611
Epoch 19 iteration 1020/1263: training loss 0.611
Epoch 19 iteration 1040/1263: training loss 0.612
Epoch 19 iteration 1060/1263: training loss 0.613
Epoch 19 iteration 1080/1263: training loss 0.614
Epoch 19 iteration 1100/1263: training loss 0.616
Epoch 19 iteration 1120/1263: training loss 0.618
Epoch 19 iteration 1140/1263: training loss 0.621
Epoch 19 iteration 1160/1263: training loss 0.621
Epoch 19 iteration 1180/1263: training loss 0.621
Epoch 19 iteration 1200/1263: training loss 0.622
Epoch 19 iteration 1220/1263: training loss 0.623
Epoch 19 iteration 1240/1263: training loss 0.623
Epoch 19 iteration 1260/1263: training loss 0.622
Epoch 19 validation pixAcc: 0.786, mIoU: 0.399
Epoch 20 iteration 0020/1263: training loss 0.570
Epoch 20 iteration 0040/1263: training loss 0.589
Epoch 20 iteration 0060/1263: training loss 0.602
Epoch 20 iteration 0080/1263: training loss 0.588
Epoch 20 iteration 0100/1263: training loss 0.591
Epoch 20 iteration 0120/1263: training loss 0.594
Epoch 20 iteration 0140/1263: training loss 0.591
Epoch 20 iteration 0160/1263: training loss 0.591
Epoch 20 iteration 0180/1263: training loss 0.587
Epoch 20 iteration 0200/1263: training loss 0.585
Epoch 20 iteration 0220/1263: training loss 0.585
Epoch 20 iteration 0240/1263: training loss 0.585
Epoch 20 iteration 0260/1263: training loss 0.582
Epoch 20 iteration 0280/1263: training loss 0.578
Epoch 20 iteration 0300/1263: training loss 0.576
Epoch 20 iteration 0320/1263: training loss 0.578
Epoch 20 iteration 0340/1263: training loss 0.580
Epoch 20 iteration 0360/1263: training loss 0.579
Epoch 20 iteration 0380/1263: training loss 0.583
Epoch 20 iteration 0400/1263: training loss 0.585
Epoch 20 iteration 0420/1263: training loss 0.585
Epoch 20 iteration 0440/1263: training loss 0.587
Epoch 20 iteration 0460/1263: training loss 0.588
Epoch 20 iteration 0480/1263: training loss 0.589
Epoch 20 iteration 0500/1263: training loss 0.590
Epoch 20 iteration 0520/1263: training loss 0.586
Epoch 20 iteration 0540/1263: training loss 0.586
Epoch 20 iteration 0560/1263: training loss 0.587
Epoch 20 iteration 0580/1263: training loss 0.588
Epoch 20 iteration 0600/1263: training loss 0.588
Epoch 20 iteration 0620/1263: training loss 0.589
Epoch 20 iteration 0640/1263: training loss 0.590
Epoch 20 iteration 0660/1263: training loss 0.592
Epoch 20 iteration 0680/1263: training loss 0.593
Epoch 20 iteration 0700/1263: training loss 0.594
Epoch 20 iteration 0720/1263: training loss 0.593
Epoch 20 iteration 0740/1263: training loss 0.593
Epoch 20 iteration 0760/1263: training loss 0.594
Epoch 20 iteration 0780/1263: training loss 0.594
Epoch 20 iteration 0800/1263: training loss 0.595
Epoch 20 iteration 0820/1263: training loss 0.595
Epoch 20 iteration 0840/1263: training loss 0.596
Epoch 20 iteration 0860/1263: training loss 0.596
Epoch 20 iteration 0880/1263: training loss 0.596
Epoch 20 iteration 0900/1263: training loss 0.596
Epoch 20 iteration 0920/1263: training loss 0.594
Epoch 20 iteration 0940/1263: training loss 0.595
Epoch 20 iteration 0960/1263: training loss 0.595
Epoch 20 iteration 0980/1263: training loss 0.596
Epoch 20 iteration 1000/1263: training loss 0.597
Epoch 20 iteration 1020/1263: training loss 0.597
Epoch 20 iteration 1040/1263: training loss 0.597
Epoch 20 iteration 1060/1263: training loss 0.597
Epoch 20 iteration 1080/1263: training loss 0.597
Epoch 20 iteration 1100/1263: training loss 0.597
Epoch 20 iteration 1120/1263: training loss 0.600
Epoch 20 iteration 1140/1263: training loss 0.601
Epoch 20 iteration 1160/1263: training loss 0.602
Epoch 20 iteration 1180/1263: training loss 0.603
Epoch 20 iteration 1200/1263: training loss 0.603
Epoch 20 iteration 1220/1263: training loss 0.604
Epoch 20 iteration 1240/1263: training loss 0.604
Epoch 20 iteration 1260/1263: training loss 0.605
Epoch 20 validation pixAcc: 0.777, mIoU: 0.412
Epoch 21 iteration 0020/1263: training loss 0.603
Epoch 21 iteration 0040/1263: training loss 0.592
Epoch 21 iteration 0060/1263: training loss 0.605
Epoch 21 iteration 0080/1263: training loss 0.608
Epoch 21 iteration 0100/1263: training loss 0.614
Epoch 21 iteration 0120/1263: training loss 0.611
Epoch 21 iteration 0140/1263: training loss 0.598
Epoch 21 iteration 0160/1263: training loss 0.600
Epoch 21 iteration 0180/1263: training loss 0.598
Epoch 21 iteration 0200/1263: training loss 0.599
Epoch 21 iteration 0220/1263: training loss 0.601
Epoch 21 iteration 0240/1263: training loss 0.604
Epoch 21 iteration 0260/1263: training loss 0.604
Epoch 21 iteration 0280/1263: training loss 0.600
Epoch 21 iteration 0300/1263: training loss 0.598
Epoch 21 iteration 0320/1263: training loss 0.597
Epoch 21 iteration 0340/1263: training loss 0.599
Epoch 21 iteration 0360/1263: training loss 0.598
Epoch 21 iteration 0380/1263: training loss 0.594
Epoch 21 iteration 0400/1263: training loss 0.596
Epoch 21 iteration 0420/1263: training loss 0.596
Epoch 21 iteration 0440/1263: training loss 0.593
Epoch 21 iteration 0460/1263: training loss 0.592
Epoch 21 iteration 0480/1263: training loss 0.591
Epoch 21 iteration 0500/1263: training loss 0.593
Epoch 21 iteration 0520/1263: training loss 0.592
Epoch 21 iteration 0540/1263: training loss 0.590
Epoch 21 iteration 0560/1263: training loss 0.590
Epoch 21 iteration 0580/1263: training loss 0.589
Epoch 21 iteration 0600/1263: training loss 0.588
Epoch 21 iteration 0620/1263: training loss 0.587
Epoch 21 iteration 0640/1263: training loss 0.586
Epoch 21 iteration 0660/1263: training loss 0.586
Epoch 21 iteration 0680/1263: training loss 0.586
Epoch 21 iteration 0700/1263: training loss 0.586
Epoch 21 iteration 0720/1263: training loss 0.587
Epoch 21 iteration 0740/1263: training loss 0.587
Epoch 21 iteration 0760/1263: training loss 0.587
Epoch 21 iteration 0780/1263: training loss 0.588
Epoch 21 iteration 0800/1263: training loss 0.586
Epoch 21 iteration 0820/1263: training loss 0.587
Epoch 21 iteration 0840/1263: training loss 0.585
Epoch 21 iteration 0860/1263: training loss 0.585
Epoch 21 iteration 0880/1263: training loss 0.586
Epoch 21 iteration 0900/1263: training loss 0.586
Epoch 21 iteration 0920/1263: training loss 0.586
Epoch 21 iteration 0940/1263: training loss 0.587
Epoch 21 iteration 0960/1263: training loss 0.588
Epoch 21 iteration 0980/1263: training loss 0.589
Epoch 21 iteration 1000/1263: training loss 0.589
Epoch 21 iteration 1020/1263: training loss 0.589
Epoch 21 iteration 1040/1263: training loss 0.589
Epoch 21 iteration 1060/1263: training loss 0.590
Epoch 21 iteration 1080/1263: training loss 0.591
Epoch 21 iteration 1100/1263: training loss 0.591
Epoch 21 iteration 1120/1263: training loss 0.593
Epoch 21 iteration 1140/1263: training loss 0.596
Epoch 21 iteration 1160/1263: training loss 0.597
Epoch 21 iteration 1180/1263: training loss 0.598
Epoch 21 iteration 1200/1263: training loss 0.599
Epoch 21 iteration 1220/1263: training loss 0.599
Epoch 21 iteration 1240/1263: training loss 0.600
Epoch 21 iteration 1260/1263: training loss 0.601
Epoch 21 validation pixAcc: 0.777, mIoU: 0.397
Epoch 22 iteration 0020/1263: training loss 0.628
Epoch 22 iteration 0040/1263: training loss 0.595
Epoch 22 iteration 0060/1263: training loss 0.600
Epoch 22 iteration 0080/1263: training loss 0.609
Epoch 22 iteration 0100/1263: training loss 0.606
Epoch 22 iteration 0120/1263: training loss 0.601
Epoch 22 iteration 0140/1263: training loss 0.604
Epoch 22 iteration 0160/1263: training loss 0.597
Epoch 22 iteration 0180/1263: training loss 0.597
Epoch 22 iteration 0200/1263: training loss 0.596
Epoch 22 iteration 0220/1263: training loss 0.602
Epoch 22 iteration 0240/1263: training loss 0.598
Epoch 22 iteration 0260/1263: training loss 0.599
Epoch 22 iteration 0280/1263: training loss 0.598
Epoch 22 iteration 0300/1263: training loss 0.597
Epoch 22 iteration 0320/1263: training loss 0.597
Epoch 22 iteration 0340/1263: training loss 0.598
Epoch 22 iteration 0360/1263: training loss 0.597
Epoch 22 iteration 0380/1263: training loss 0.593
Epoch 22 iteration 0400/1263: training loss 0.596
Epoch 22 iteration 0420/1263: training loss 0.597
Epoch 22 iteration 0440/1263: training loss 0.596
Epoch 22 iteration 0460/1263: training loss 0.596
Epoch 22 iteration 0480/1263: training loss 0.594
Epoch 22 iteration 0500/1263: training loss 0.594
Epoch 22 iteration 0520/1263: training loss 0.592
Epoch 22 iteration 0540/1263: training loss 0.590
Epoch 22 iteration 0560/1263: training loss 0.588
Epoch 22 iteration 0580/1263: training loss 0.586
Epoch 22 iteration 0600/1263: training loss 0.585
Epoch 22 iteration 0620/1263: training loss 0.583
Epoch 22 iteration 0640/1263: training loss 0.584
Epoch 22 iteration 0660/1263: training loss 0.587
Epoch 22 iteration 0680/1263: training loss 0.588
Epoch 22 iteration 0700/1263: training loss 0.588
Epoch 22 iteration 0720/1263: training loss 0.590
Epoch 22 iteration 0740/1263: training loss 0.591
Epoch 22 iteration 0760/1263: training loss 0.593
Epoch 22 iteration 0780/1263: training loss 0.593
Epoch 22 iteration 0800/1263: training loss 0.595
Epoch 22 iteration 0820/1263: training loss 0.595
Epoch 22 iteration 0840/1263: training loss 0.596
Epoch 22 iteration 0860/1263: training loss 0.595
Epoch 22 iteration 0880/1263: training loss 0.595
Epoch 22 iteration 0900/1263: training loss 0.595
Epoch 22 iteration 0920/1263: training loss 0.595
Epoch 22 iteration 0940/1263: training loss 0.595
Epoch 22 iteration 0960/1263: training loss 0.595
Epoch 22 iteration 0980/1263: training loss 0.594
Epoch 22 iteration 1000/1263: training loss 0.594
Epoch 22 iteration 1020/1263: training loss 0.593
Epoch 22 iteration 1040/1263: training loss 0.594
Epoch 22 iteration 1060/1263: training loss 0.594
Epoch 22 iteration 1080/1263: training loss 0.594
Epoch 22 iteration 1100/1263: training loss 0.595
Epoch 22 iteration 1120/1263: training loss 0.594
Epoch 22 iteration 1140/1263: training loss 0.595
Epoch 22 iteration 1160/1263: training loss 0.595
Epoch 22 iteration 1180/1264: training loss 0.596
Epoch 22 iteration 1200/1264: training loss 0.596
Epoch 22 iteration 1220/1264: training loss 0.595
Epoch 22 iteration 1240/1264: training loss 0.596
Epoch 22 iteration 1260/1264: training loss 0.596
Epoch 22 validation pixAcc: 0.780, mIoU: 0.405
Epoch 23 iteration 0020/1263: training loss 0.598
Epoch 23 iteration 0040/1263: training loss 0.583
Epoch 23 iteration 0060/1263: training loss 0.597
Epoch 23 iteration 0080/1263: training loss 0.606
Epoch 23 iteration 0100/1263: training loss 0.590
Epoch 23 iteration 0120/1263: training loss 0.573
Epoch 23 iteration 0140/1263: training loss 0.573
Epoch 23 iteration 0160/1263: training loss 0.566
Epoch 23 iteration 0180/1263: training loss 0.560
Epoch 23 iteration 0200/1263: training loss 0.555
Epoch 23 iteration 0220/1263: training loss 0.553
Epoch 23 iteration 0240/1263: training loss 0.555
Epoch 23 iteration 0260/1263: training loss 0.555
Epoch 23 iteration 0280/1263: training loss 0.555
Epoch 23 iteration 0300/1263: training loss 0.553
Epoch 23 iteration 0320/1263: training loss 0.554
Epoch 23 iteration 0340/1263: training loss 0.555
Epoch 23 iteration 0360/1263: training loss 0.555
Epoch 23 iteration 0380/1263: training loss 0.552
Epoch 23 iteration 0400/1263: training loss 0.550
Epoch 23 iteration 0420/1263: training loss 0.546
Epoch 23 iteration 0440/1263: training loss 0.546
Epoch 23 iteration 0460/1263: training loss 0.547
Epoch 23 iteration 0480/1263: training loss 0.545
Epoch 23 iteration 0500/1263: training loss 0.545
Epoch 23 iteration 0520/1263: training loss 0.543
Epoch 23 iteration 0540/1263: training loss 0.542
Epoch 23 iteration 0560/1263: training loss 0.543
Epoch 23 iteration 0580/1263: training loss 0.543
Epoch 23 iteration 0600/1263: training loss 0.544
Epoch 23 iteration 0620/1263: training loss 0.544
Epoch 23 iteration 0640/1263: training loss 0.548
Epoch 23 iteration 0660/1263: training loss 0.550
Epoch 23 iteration 0680/1263: training loss 0.554
Epoch 23 iteration 0700/1263: training loss 0.555
Epoch 23 iteration 0720/1263: training loss 0.557
Epoch 23 iteration 0740/1263: training loss 0.557
Epoch 23 iteration 0760/1263: training loss 0.559
Epoch 23 iteration 0780/1263: training loss 0.559
Epoch 23 iteration 0800/1263: training loss 0.561
Epoch 23 iteration 0820/1263: training loss 0.561
Epoch 23 iteration 0840/1263: training loss 0.563
Epoch 23 iteration 0860/1263: training loss 0.563
Epoch 23 iteration 0880/1263: training loss 0.565
Epoch 23 iteration 0900/1263: training loss 0.565
Epoch 23 iteration 0920/1263: training loss 0.564
Epoch 23 iteration 0940/1263: training loss 0.563
Epoch 23 iteration 0960/1263: training loss 0.562
Epoch 23 iteration 0980/1263: training loss 0.562
Epoch 23 iteration 1000/1263: training loss 0.562
Epoch 23 iteration 1020/1263: training loss 0.563
Epoch 23 iteration 1040/1263: training loss 0.562
Epoch 23 iteration 1060/1263: training loss 0.562
Epoch 23 iteration 1080/1263: training loss 0.563
Epoch 23 iteration 1100/1263: training loss 0.566
Epoch 23 iteration 1120/1263: training loss 0.567
Epoch 23 iteration 1140/1263: training loss 0.568
Epoch 23 iteration 1160/1263: training loss 0.569
Epoch 23 iteration 1180/1263: training loss 0.569
Epoch 23 iteration 1200/1263: training loss 0.569
Epoch 23 iteration 1220/1263: training loss 0.570
Epoch 23 iteration 1240/1263: training loss 0.570
Epoch 23 iteration 1260/1263: training loss 0.571
Epoch 23 validation pixAcc: 0.775, mIoU: 0.406
Epoch 24 iteration 0020/1263: training loss 0.633
Epoch 24 iteration 0040/1263: training loss 0.592
Epoch 24 iteration 0060/1263: training loss 0.574
Epoch 24 iteration 0080/1263: training loss 0.556
Epoch 24 iteration 0100/1263: training loss 0.549
Epoch 24 iteration 0120/1263: training loss 0.552
Epoch 24 iteration 0140/1263: training loss 0.551
Epoch 24 iteration 0160/1263: training loss 0.550
Epoch 24 iteration 0180/1263: training loss 0.542
Epoch 24 iteration 0200/1263: training loss 0.544
Epoch 24 iteration 0220/1263: training loss 0.543
Epoch 24 iteration 0240/1263: training loss 0.544
Epoch 24 iteration 0260/1263: training loss 0.545
Epoch 24 iteration 0280/1263: training loss 0.547
Epoch 24 iteration 0300/1263: training loss 0.547
Epoch 24 iteration 0320/1263: training loss 0.545
Epoch 24 iteration 0340/1263: training loss 0.546
Epoch 24 iteration 0360/1263: training loss 0.547
Epoch 24 iteration 0380/1263: training loss 0.550
Epoch 24 iteration 0400/1263: training loss 0.550
Epoch 24 iteration 0420/1263: training loss 0.551
Epoch 24 iteration 0440/1263: training loss 0.550
Epoch 24 iteration 0460/1263: training loss 0.548
Epoch 24 iteration 0480/1263: training loss 0.551
Epoch 24 iteration 0500/1263: training loss 0.555
Epoch 24 iteration 0520/1263: training loss 0.556
Epoch 24 iteration 0540/1263: training loss 0.556
Epoch 24 iteration 0560/1263: training loss 0.557
Epoch 24 iteration 0580/1263: training loss 0.561
Epoch 24 iteration 0600/1263: training loss 0.560
Epoch 24 iteration 0620/1263: training loss 0.560
Epoch 24 iteration 0640/1263: training loss 0.560
Epoch 24 iteration 0660/1263: training loss 0.561
Epoch 24 iteration 0680/1263: training loss 0.560
Epoch 24 iteration 0700/1263: training loss 0.560
Epoch 24 iteration 0720/1263: training loss 0.560
Epoch 24 iteration 0740/1263: training loss 0.560
Epoch 24 iteration 0760/1263: training loss 0.559
Epoch 24 iteration 0780/1263: training loss 0.558
Epoch 24 iteration 0800/1263: training loss 0.557
Epoch 24 iteration 0820/1263: training loss 0.557
Epoch 24 iteration 0840/1263: training loss 0.558
Epoch 24 iteration 0860/1263: training loss 0.557
Epoch 24 iteration 0880/1263: training loss 0.556
Epoch 24 iteration 0900/1263: training loss 0.556
Epoch 24 iteration 0920/1263: training loss 0.558
Epoch 24 iteration 0940/1263: training loss 0.559
Epoch 24 iteration 0960/1263: training loss 0.559
Epoch 24 iteration 0980/1263: training loss 0.561
Epoch 24 iteration 1000/1263: training loss 0.560
Epoch 24 iteration 1020/1263: training loss 0.561
Epoch 24 iteration 1040/1263: training loss 0.561
Epoch 24 iteration 1060/1263: training loss 0.562
Epoch 24 iteration 1080/1263: training loss 0.562
Epoch 24 iteration 1100/1263: training loss 0.563
Epoch 24 iteration 1120/1263: training loss 0.563
Epoch 24 iteration 1140/1263: training loss 0.562
Epoch 24 iteration 1160/1263: training loss 0.560
Epoch 24 iteration 1180/1263: training loss 0.561
Epoch 24 iteration 1200/1263: training loss 0.562
Epoch 24 iteration 1220/1263: training loss 0.562
Epoch 24 iteration 1240/1263: training loss 0.562
Epoch 24 iteration 1260/1263: training loss 0.562
Epoch 24 validation pixAcc: 0.775, mIoU: 0.407
Epoch 25 iteration 0020/1263: training loss 0.560
Epoch 25 iteration 0040/1263: training loss 0.568
Epoch 25 iteration 0060/1263: training loss 0.573
Epoch 25 iteration 0080/1263: training loss 0.559
Epoch 25 iteration 0100/1263: training loss 0.547
Epoch 25 iteration 0120/1263: training loss 0.553
Epoch 25 iteration 0140/1263: training loss 0.555
Epoch 25 iteration 0160/1263: training loss 0.553
Epoch 25 iteration 0180/1263: training loss 0.554
Epoch 25 iteration 0200/1263: training loss 0.555
Epoch 25 iteration 0220/1263: training loss 0.557
Epoch 25 iteration 0240/1263: training loss 0.559
Epoch 25 iteration 0260/1263: training loss 0.564
Epoch 25 iteration 0280/1263: training loss 0.564
Epoch 25 iteration 0300/1263: training loss 0.565
Epoch 25 iteration 0320/1263: training loss 0.568
Epoch 25 iteration 0340/1263: training loss 0.567
Epoch 25 iteration 0360/1263: training loss 0.571
Epoch 25 iteration 0380/1263: training loss 0.574
Epoch 25 iteration 0400/1263: training loss 0.574
Epoch 25 iteration 0420/1263: training loss 0.579
Epoch 25 iteration 0440/1263: training loss 0.580
Epoch 25 iteration 0460/1263: training loss 0.578
Epoch 25 iteration 0480/1263: training loss 0.577
Epoch 25 iteration 0500/1263: training loss 0.578
Epoch 25 iteration 0520/1263: training loss 0.581
Epoch 25 iteration 0540/1263: training loss 0.582
Epoch 25 iteration 0560/1263: training loss 0.581
Epoch 25 iteration 0580/1263: training loss 0.582
Epoch 25 iteration 0600/1263: training loss 0.581
Epoch 25 iteration 0620/1263: training loss 0.580
Epoch 25 iteration 0640/1263: training loss 0.579
Epoch 25 iteration 0660/1263: training loss 0.577
Epoch 25 iteration 0680/1263: training loss 0.576
Epoch 25 iteration 0700/1263: training loss 0.578
Epoch 25 iteration 0720/1263: training loss 0.577
Epoch 25 iteration 0740/1263: training loss 0.576
Epoch 25 iteration 0760/1263: training loss 0.577
Epoch 25 iteration 0780/1263: training loss 0.575
Epoch 25 iteration 0800/1263: training loss 0.576
Epoch 25 iteration 0820/1263: training loss 0.576
Epoch 25 iteration 0840/1263: training loss 0.575
Epoch 25 iteration 0860/1263: training loss 0.574
Epoch 25 iteration 0880/1263: training loss 0.572
Epoch 25 iteration 0900/1263: training loss 0.571
Epoch 25 iteration 0920/1263: training loss 0.569
Epoch 25 iteration 0940/1263: training loss 0.568
Epoch 25 iteration 0960/1263: training loss 0.568
Epoch 25 iteration 0980/1263: training loss 0.569
Epoch 25 iteration 1000/1263: training loss 0.570
Epoch 25 iteration 1020/1263: training loss 0.570
Epoch 25 iteration 1040/1263: training loss 0.570
Epoch 25 iteration 1060/1263: training loss 0.570
Epoch 25 iteration 1080/1263: training loss 0.570
Epoch 25 iteration 1100/1263: training loss 0.569
Epoch 25 iteration 1120/1263: training loss 0.570
Epoch 25 iteration 1140/1263: training loss 0.568
Epoch 25 iteration 1160/1263: training loss 0.567
Epoch 25 iteration 1180/1263: training loss 0.567
Epoch 25 iteration 1200/1263: training loss 0.567
Epoch 25 iteration 1220/1263: training loss 0.567
Epoch 25 iteration 1240/1263: training loss 0.567
Epoch 25 iteration 1260/1263: training loss 0.568
Epoch 25 validation pixAcc: 0.788, mIoU: 0.427
Epoch 26 iteration 0020/1263: training loss 0.549
Epoch 26 iteration 0040/1263: training loss 0.546
Epoch 26 iteration 0060/1263: training loss 0.537
Epoch 26 iteration 0080/1263: training loss 0.538
Epoch 26 iteration 0100/1263: training loss 0.537
Epoch 26 iteration 0120/1263: training loss 0.540
Epoch 26 iteration 0140/1263: training loss 0.535
Epoch 26 iteration 0160/1263: training loss 0.529
Epoch 26 iteration 0180/1263: training loss 0.525
Epoch 26 iteration 0200/1263: training loss 0.519
Epoch 26 iteration 0220/1263: training loss 0.516
Epoch 26 iteration 0240/1263: training loss 0.514
Epoch 26 iteration 0260/1263: training loss 0.514
Epoch 26 iteration 0280/1263: training loss 0.514
Epoch 26 iteration 0300/1263: training loss 0.516
Epoch 26 iteration 0320/1263: training loss 0.514
Epoch 26 iteration 0340/1263: training loss 0.520
Epoch 26 iteration 0360/1263: training loss 0.522
Epoch 26 iteration 0380/1263: training loss 0.523
Epoch 26 iteration 0400/1263: training loss 0.525
Epoch 26 iteration 0420/1263: training loss 0.522
Epoch 26 iteration 0440/1263: training loss 0.522
Epoch 26 iteration 0460/1263: training loss 0.523
Epoch 26 iteration 0480/1263: training loss 0.522
Epoch 26 iteration 0500/1263: training loss 0.523
Epoch 26 iteration 0520/1263: training loss 0.522
Epoch 26 iteration 0540/1263: training loss 0.521
Epoch 26 iteration 0560/1263: training loss 0.520
Epoch 26 iteration 0580/1263: training loss 0.519
Epoch 26 iteration 0600/1263: training loss 0.519
Epoch 26 iteration 0620/1263: training loss 0.519
Epoch 26 iteration 0640/1263: training loss 0.520
Epoch 26 iteration 0660/1263: training loss 0.521
Epoch 26 iteration 0680/1263: training loss 0.520
Epoch 26 iteration 0700/1263: training loss 0.521
Epoch 26 iteration 0720/1263: training loss 0.522
Epoch 26 iteration 0740/1263: training loss 0.523
Epoch 26 iteration 0760/1263: training loss 0.523
Epoch 26 iteration 0780/1263: training loss 0.523
Epoch 26 iteration 0800/1263: training loss 0.523
Epoch 26 iteration 0820/1263: training loss 0.524
Epoch 26 iteration 0840/1263: training loss 0.523
Epoch 26 iteration 0860/1263: training loss 0.524
Epoch 26 iteration 0880/1263: training loss 0.524
Epoch 26 iteration 0900/1263: training loss 0.524
Epoch 26 iteration 0920/1263: training loss 0.525
Epoch 26 iteration 0940/1263: training loss 0.526
Epoch 26 iteration 0960/1263: training loss 0.527
Epoch 26 iteration 0980/1263: training loss 0.526
Epoch 26 iteration 1000/1263: training loss 0.526
Epoch 26 iteration 1020/1263: training loss 0.527
Epoch 26 iteration 1040/1263: training loss 0.526
Epoch 26 iteration 1060/1263: training loss 0.526
Epoch 26 iteration 1080/1263: training loss 0.527
Epoch 26 iteration 1100/1263: training loss 0.527
Epoch 26 iteration 1120/1263: training loss 0.527
Epoch 26 iteration 1140/1263: training loss 0.527
Epoch 26 iteration 1160/1263: training loss 0.527
Epoch 26 iteration 1180/1263: training loss 0.526
Epoch 26 iteration 1200/1263: training loss 0.527
Epoch 26 iteration 1220/1263: training loss 0.528
Epoch 26 iteration 1240/1263: training loss 0.528
Epoch 26 iteration 1260/1263: training loss 0.528
Epoch 26 validation pixAcc: 0.790, mIoU: 0.416
Epoch 27 iteration 0020/1263: training loss 0.490
Epoch 27 iteration 0040/1263: training loss 0.509
Epoch 27 iteration 0060/1263: training loss 0.531
Epoch 27 iteration 0080/1263: training loss 0.524
Epoch 27 iteration 0100/1263: training loss 0.530
Epoch 27 iteration 0120/1263: training loss 0.532
Epoch 27 iteration 0140/1263: training loss 0.527
Epoch 27 iteration 0160/1263: training loss 0.521
Epoch 27 iteration 0180/1263: training loss 0.518
Epoch 27 iteration 0200/1263: training loss 0.511
Epoch 27 iteration 0220/1263: training loss 0.508
Epoch 27 iteration 0240/1263: training loss 0.508
Epoch 27 iteration 0260/1263: training loss 0.505
Epoch 27 iteration 0280/1263: training loss 0.504
Epoch 27 iteration 0300/1263: training loss 0.504
Epoch 27 iteration 0320/1263: training loss 0.503
Epoch 27 iteration 0340/1263: training loss 0.503
Epoch 27 iteration 0360/1263: training loss 0.501
Epoch 27 iteration 0380/1263: training loss 0.500
Epoch 27 iteration 0400/1263: training loss 0.499
Epoch 27 iteration 0420/1263: training loss 0.499
Epoch 27 iteration 0440/1263: training loss 0.500
Epoch 27 iteration 0460/1263: training loss 0.502
Epoch 27 iteration 0480/1263: training loss 0.503
Epoch 27 iteration 0500/1263: training loss 0.502
Epoch 27 iteration 0520/1263: training loss 0.502
Epoch 27 iteration 0540/1263: training loss 0.502
Epoch 27 iteration 0560/1263: training loss 0.503
Epoch 27 iteration 0580/1263: training loss 0.503
Epoch 27 iteration 0600/1263: training loss 0.503
Epoch 27 iteration 0620/1263: training loss 0.501
Epoch 27 iteration 0640/1263: training loss 0.502
Epoch 27 iteration 0660/1263: training loss 0.502
Epoch 27 iteration 0680/1263: training loss 0.505
Epoch 27 iteration 0700/1263: training loss 0.506
Epoch 27 iteration 0720/1263: training loss 0.506
Epoch 27 iteration 0740/1263: training loss 0.507
Epoch 27 iteration 0760/1263: training loss 0.506
Epoch 27 iteration 0780/1263: training loss 0.506
Epoch 27 iteration 0800/1263: training loss 0.506
Epoch 27 iteration 0820/1263: training loss 0.505
Epoch 27 iteration 0840/1263: training loss 0.505
Epoch 27 iteration 0860/1263: training loss 0.505
Epoch 27 iteration 0880/1263: training loss 0.505
Epoch 27 iteration 0900/1263: training loss 0.505
Epoch 27 iteration 0920/1263: training loss 0.505
Epoch 27 iteration 0940/1263: training loss 0.505
Epoch 27 iteration 0960/1263: training loss 0.506
Epoch 27 iteration 0980/1263: training loss 0.507
Epoch 27 iteration 1000/1263: training loss 0.507
Epoch 27 iteration 1020/1263: training loss 0.506
Epoch 27 iteration 1040/1263: training loss 0.506
Epoch 27 iteration 1060/1263: training loss 0.506
Epoch 27 iteration 1080/1263: training loss 0.507
Epoch 27 iteration 1100/1263: training loss 0.507
Epoch 27 iteration 1120/1263: training loss 0.507
Epoch 27 iteration 1140/1263: training loss 0.507
Epoch 27 iteration 1160/1263: training loss 0.507
Epoch 27 iteration 1180/1263: training loss 0.508
Epoch 27 iteration 1200/1263: training loss 0.509
Epoch 27 iteration 1220/1263: training loss 0.509
Epoch 27 iteration 1240/1263: training loss 0.510
Epoch 27 iteration 1260/1263: training loss 0.510
Epoch 27 validation pixAcc: 0.780, mIoU: 0.413
Epoch 28 iteration 0020/1263: training loss 0.500
Epoch 28 iteration 0040/1263: training loss 0.494
Epoch 28 iteration 0060/1263: training loss 0.499
Epoch 28 iteration 0080/1263: training loss 0.493
Epoch 28 iteration 0100/1263: training loss 0.488
Epoch 28 iteration 0120/1263: training loss 0.496
Epoch 28 iteration 0140/1263: training loss 0.508
Epoch 28 iteration 0160/1263: training loss 0.511
Epoch 28 iteration 0180/1263: training loss 0.515
Epoch 28 iteration 0200/1263: training loss 0.515
Epoch 28 iteration 0220/1263: training loss 0.511
Epoch 28 iteration 0240/1263: training loss 0.509
Epoch 28 iteration 0260/1263: training loss 0.508
Epoch 28 iteration 0280/1263: training loss 0.510
Epoch 28 iteration 0300/1263: training loss 0.510
Epoch 28 iteration 0320/1263: training loss 0.508
Epoch 28 iteration 0340/1263: training loss 0.506
Epoch 28 iteration 0360/1263: training loss 0.507
Epoch 28 iteration 0380/1263: training loss 0.507
Epoch 28 iteration 0400/1263: training loss 0.506
Epoch 28 iteration 0420/1263: training loss 0.506
Epoch 28 iteration 0440/1263: training loss 0.507
Epoch 28 iteration 0460/1263: training loss 0.507
Epoch 28 iteration 0480/1263: training loss 0.507
Epoch 28 iteration 0500/1263: training loss 0.507
Epoch 28 iteration 0520/1263: training loss 0.508
Epoch 28 iteration 0540/1263: training loss 0.508
Epoch 28 iteration 0560/1263: training loss 0.509
Epoch 28 iteration 0580/1263: training loss 0.509
Epoch 28 iteration 0600/1263: training loss 0.510
Epoch 28 iteration 0620/1263: training loss 0.511
Epoch 28 iteration 0640/1263: training loss 0.514
Epoch 28 iteration 0660/1263: training loss 0.514
Epoch 28 iteration 0680/1263: training loss 0.515
Epoch 28 iteration 0700/1263: training loss 0.515
Epoch 28 iteration 0720/1263: training loss 0.515
Epoch 28 iteration 0740/1263: training loss 0.514
Epoch 28 iteration 0760/1263: training loss 0.515
Epoch 28 iteration 0780/1263: training loss 0.514
Epoch 28 iteration 0800/1263: training loss 0.514
Epoch 28 iteration 0820/1263: training loss 0.514
Epoch 28 iteration 0840/1263: training loss 0.516
Epoch 28 iteration 0860/1263: training loss 0.518
Epoch 28 iteration 0880/1263: training loss 0.520
Epoch 28 iteration 0900/1263: training loss 0.521
Epoch 28 iteration 0920/1263: training loss 0.521
Epoch 28 iteration 0940/1263: training loss 0.522
Epoch 28 iteration 0960/1263: training loss 0.524
Epoch 28 iteration 0980/1263: training loss 0.525
Epoch 28 iteration 1000/1263: training loss 0.525
Epoch 28 iteration 1020/1263: training loss 0.527
Epoch 28 iteration 1040/1263: training loss 0.529
Epoch 28 iteration 1060/1263: training loss 0.530
Epoch 28 iteration 1080/1263: training loss 0.531
Epoch 28 iteration 1100/1263: training loss 0.531
Epoch 28 iteration 1120/1263: training loss 0.532
Epoch 28 iteration 1140/1263: training loss 0.531
Epoch 28 iteration 1160/1263: training loss 0.532
Epoch 28 iteration 1180/1263: training loss 0.532
Epoch 28 iteration 1200/1263: training loss 0.532
Epoch 28 iteration 1220/1263: training loss 0.533
Epoch 28 iteration 1240/1263: training loss 0.535
Epoch 28 iteration 1260/1263: training loss 0.535
Epoch 28 validation pixAcc: 0.785, mIoU: 0.428
Epoch 29 iteration 0020/1263: training loss 0.510
Epoch 29 iteration 0040/1263: training loss 0.503
Epoch 29 iteration 0060/1263: training loss 0.510
Epoch 29 iteration 0080/1263: training loss 0.516
Epoch 29 iteration 0100/1263: training loss 0.517
Epoch 29 iteration 0120/1263: training loss 0.513
Epoch 29 iteration 0140/1263: training loss 0.513
Epoch 29 iteration 0160/1263: training loss 0.509
Epoch 29 iteration 0180/1263: training loss 0.509
Epoch 29 iteration 0200/1263: training loss 0.507
Epoch 29 iteration 0220/1263: training loss 0.501
Epoch 29 iteration 0240/1263: training loss 0.504
Epoch 29 iteration 0260/1263: training loss 0.506
Epoch 29 iteration 0280/1263: training loss 0.506
Epoch 29 iteration 0300/1263: training loss 0.505
Epoch 29 iteration 0320/1263: training loss 0.505
Epoch 29 iteration 0340/1263: training loss 0.506
Epoch 29 iteration 0360/1263: training loss 0.507
Epoch 29 iteration 0380/1263: training loss 0.506
Epoch 29 iteration 0400/1263: training loss 0.505
Epoch 29 iteration 0420/1263: training loss 0.506
Epoch 29 iteration 0440/1263: training loss 0.504
Epoch 29 iteration 0460/1263: training loss 0.503
Epoch 29 iteration 0480/1263: training loss 0.504
Epoch 29 iteration 0500/1263: training loss 0.504
Epoch 29 iteration 0520/1263: training loss 0.504
Epoch 29 iteration 0540/1263: training loss 0.506
Epoch 29 iteration 0560/1263: training loss 0.509
Epoch 29 iteration 0580/1263: training loss 0.508
Epoch 29 iteration 0600/1263: training loss 0.511
Epoch 29 iteration 0620/1263: training loss 0.509
Epoch 29 iteration 0640/1263: training loss 0.509
Epoch 29 iteration 0660/1263: training loss 0.510
Epoch 29 iteration 0680/1263: training loss 0.509
Epoch 29 iteration 0700/1263: training loss 0.509
Epoch 29 iteration 0720/1263: training loss 0.508
Epoch 29 iteration 0740/1263: training loss 0.508
Epoch 29 iteration 0760/1263: training loss 0.508
Epoch 29 iteration 0780/1263: training loss 0.509
Epoch 29 iteration 0800/1263: training loss 0.509
Epoch 29 iteration 0820/1263: training loss 0.508
Epoch 29 iteration 0840/1263: training loss 0.508
Epoch 29 iteration 0860/1263: training loss 0.508
Epoch 29 iteration 0880/1263: training loss 0.509
Epoch 29 iteration 0900/1263: training loss 0.509
Epoch 29 iteration 0920/1263: training loss 0.510
Epoch 29 iteration 0940/1263: training loss 0.510
Epoch 29 iteration 0960/1263: training loss 0.511
Epoch 29 iteration 0980/1263: training loss 0.511
Epoch 29 iteration 1000/1263: training loss 0.511
Epoch 29 iteration 1020/1263: training loss 0.511
Epoch 29 iteration 1040/1263: training loss 0.511
Epoch 29 iteration 1060/1263: training loss 0.511
Epoch 29 iteration 1080/1263: training loss 0.512
Epoch 29 iteration 1100/1263: training loss 0.512
Epoch 29 iteration 1120/1263: training loss 0.512
Epoch 29 iteration 1140/1263: training loss 0.512
Epoch 29 iteration 1160/1263: training loss 0.513
Epoch 29 iteration 1180/1263: training loss 0.513
Epoch 29 iteration 1200/1263: training loss 0.513
Epoch 29 iteration 1220/1263: training loss 0.513
Epoch 29 iteration 1240/1263: training loss 0.514
Epoch 29 iteration 1260/1263: training loss 0.514
Epoch 29 validation pixAcc: 0.787, mIoU: 0.414
Epoch 30 iteration 0020/1263: training loss 0.502
Epoch 30 iteration 0040/1263: training loss 0.483
Epoch 30 iteration 0060/1263: training loss 0.481
Epoch 30 iteration 0080/1263: training loss 0.474
Epoch 30 iteration 0100/1263: training loss 0.466
Epoch 30 iteration 0120/1263: training loss 0.473
Epoch 30 iteration 0140/1263: training loss 0.476
Epoch 30 iteration 0160/1263: training loss 0.480
Epoch 30 iteration 0180/1263: training loss 0.481
Epoch 30 iteration 0200/1263: training loss 0.482
Epoch 30 iteration 0220/1263: training loss 0.479
Epoch 30 iteration 0240/1263: training loss 0.482
Epoch 30 iteration 0260/1263: training loss 0.479
Epoch 30 iteration 0280/1263: training loss 0.481
Epoch 30 iteration 0300/1263: training loss 0.483
Epoch 30 iteration 0320/1263: training loss 0.484
Epoch 30 iteration 0340/1263: training loss 0.489
Epoch 30 iteration 0360/1263: training loss 0.488
Epoch 30 iteration 0380/1263: training loss 0.490
Epoch 30 iteration 0400/1263: training loss 0.496
Epoch 30 iteration 0420/1263: training loss 0.497
Epoch 30 iteration 0440/1263: training loss 0.498
Epoch 30 iteration 0460/1263: training loss 0.499
Epoch 30 iteration 0480/1263: training loss 0.499
Epoch 30 iteration 0500/1263: training loss 0.499
Epoch 30 iteration 0520/1263: training loss 0.501
Epoch 30 iteration 0540/1263: training loss 0.502
Epoch 30 iteration 0560/1263: training loss 0.502
Epoch 30 iteration 0580/1263: training loss 0.503
Epoch 30 iteration 0600/1263: training loss 0.503
Epoch 30 iteration 0620/1263: training loss 0.503
Epoch 30 iteration 0640/1263: training loss 0.504
Epoch 30 iteration 0660/1263: training loss 0.502
Epoch 30 iteration 0680/1263: training loss 0.502
Epoch 30 iteration 0700/1263: training loss 0.501
Epoch 30 iteration 0720/1263: training loss 0.501
Epoch 30 iteration 0740/1263: training loss 0.502
Epoch 30 iteration 0760/1263: training loss 0.501
Epoch 30 iteration 0780/1263: training loss 0.501
Epoch 30 iteration 0800/1263: training loss 0.501
Epoch 30 iteration 0820/1263: training loss 0.501
Epoch 30 iteration 0840/1263: training loss 0.503
Epoch 30 iteration 0860/1263: training loss 0.502
Epoch 30 iteration 0880/1263: training loss 0.503
Epoch 30 iteration 0900/1263: training loss 0.503
Epoch 30 iteration 0920/1263: training loss 0.503
Epoch 30 iteration 0940/1263: training loss 0.503
Epoch 30 iteration 0960/1263: training loss 0.504
Epoch 30 iteration 0980/1263: training loss 0.505
Epoch 30 iteration 1000/1263: training loss 0.506
Epoch 30 iteration 1020/1263: training loss 0.507
Epoch 30 iteration 1040/1263: training loss 0.508
Epoch 30 iteration 1060/1263: training loss 0.508
Epoch 30 iteration 1080/1263: training loss 0.508
Epoch 30 iteration 1100/1263: training loss 0.507
Epoch 30 iteration 1120/1263: training loss 0.508
Epoch 30 iteration 1140/1263: training loss 0.508
Epoch 30 iteration 1160/1263: training loss 0.509
Epoch 30 iteration 1180/1264: training loss 0.511
Epoch 30 iteration 1200/1264: training loss 0.513
Epoch 30 iteration 1220/1264: training loss 0.515
Epoch 30 iteration 1240/1264: training loss 0.516
Epoch 30 iteration 1260/1264: training loss 0.516
Epoch 30 validation pixAcc: 0.778, mIoU: 0.407
Epoch 31 iteration 0020/1263: training loss 0.540
Epoch 31 iteration 0040/1263: training loss 0.513
Epoch 31 iteration 0060/1263: training loss 0.515
Epoch 31 iteration 0080/1263: training loss 0.518
Epoch 31 iteration 0100/1263: training loss 0.515
Epoch 31 iteration 0120/1263: training loss 0.512
Epoch 31 iteration 0140/1263: training loss 0.508
Epoch 31 iteration 0160/1263: training loss 0.504
Epoch 31 iteration 0180/1263: training loss 0.499
Epoch 31 iteration 0200/1263: training loss 0.499
Epoch 31 iteration 0220/1263: training loss 0.501
Epoch 31 iteration 0240/1263: training loss 0.505
Epoch 31 iteration 0260/1263: training loss 0.511
Epoch 31 iteration 0280/1263: training loss 0.516
Epoch 31 iteration 0300/1263: training loss 0.516
Epoch 31 iteration 0320/1263: training loss 0.516
Epoch 31 iteration 0340/1263: training loss 0.517
Epoch 31 iteration 0360/1263: training loss 0.519
Epoch 31 iteration 0380/1263: training loss 0.520
Epoch 31 iteration 0400/1263: training loss 0.522
Epoch 31 iteration 0420/1263: training loss 0.521
Epoch 31 iteration 0440/1263: training loss 0.523
Epoch 31 iteration 0460/1263: training loss 0.524
Epoch 31 iteration 0480/1263: training loss 0.525
Epoch 31 iteration 0500/1263: training loss 0.526
Epoch 31 iteration 0520/1263: training loss 0.525
Epoch 31 iteration 0540/1263: training loss 0.524
Epoch 31 iteration 0560/1263: training loss 0.524
Epoch 31 iteration 0580/1263: training loss 0.525
Epoch 31 iteration 0600/1263: training loss 0.525
Epoch 31 iteration 0620/1263: training loss 0.525
Epoch 31 iteration 0640/1263: training loss 0.524
Epoch 31 iteration 0660/1263: training loss 0.524
Epoch 31 iteration 0680/1263: training loss 0.526
Epoch 31 iteration 0700/1263: training loss 0.527
Epoch 31 iteration 0720/1263: training loss 0.526
Epoch 31 iteration 0740/1263: training loss 0.526
Epoch 31 iteration 0760/1263: training loss 0.527
Epoch 31 iteration 0780/1263: training loss 0.526
Epoch 31 iteration 0800/1263: training loss 0.527
Epoch 31 iteration 0820/1263: training loss 0.528
Epoch 31 iteration 0840/1263: training loss 0.527
Epoch 31 iteration 0860/1263: training loss 0.527
Epoch 31 iteration 0880/1263: training loss 0.526
Epoch 31 iteration 0900/1263: training loss 0.526
Epoch 31 iteration 0920/1263: training loss 0.525
Epoch 31 iteration 0940/1263: training loss 0.526
Epoch 31 iteration 0960/1263: training loss 0.528
Epoch 31 iteration 0980/1263: training loss 0.528
Epoch 31 iteration 1000/1263: training loss 0.528
Epoch 31 iteration 1020/1263: training loss 0.527
Epoch 31 iteration 1040/1263: training loss 0.526
Epoch 31 iteration 1060/1263: training loss 0.525
Epoch 31 iteration 1080/1263: training loss 0.525
Epoch 31 iteration 1100/1263: training loss 0.524
Epoch 31 iteration 1120/1263: training loss 0.524
Epoch 31 iteration 1140/1263: training loss 0.523
Epoch 31 iteration 1160/1263: training loss 0.523
Epoch 31 iteration 1180/1263: training loss 0.523
Epoch 31 iteration 1200/1263: training loss 0.523
Epoch 31 iteration 1220/1263: training loss 0.523
Epoch 31 iteration 1240/1263: training loss 0.523
Epoch 31 iteration 1260/1263: training loss 0.523
Epoch 31 validation pixAcc: 0.789, mIoU: 0.426
Epoch 32 iteration 0020/1263: training loss 0.478
Epoch 32 iteration 0040/1263: training loss 0.473
Epoch 32 iteration 0060/1263: training loss 0.475
Epoch 32 iteration 0080/1263: training loss 0.470
Epoch 32 iteration 0100/1263: training loss 0.470
Epoch 32 iteration 0120/1263: training loss 0.466
Epoch 32 iteration 0140/1263: training loss 0.464
Epoch 32 iteration 0160/1263: training loss 0.470
Epoch 32 iteration 0180/1263: training loss 0.477
Epoch 32 iteration 0200/1263: training loss 0.471
Epoch 32 iteration 0220/1263: training loss 0.470
Epoch 32 iteration 0240/1263: training loss 0.472
Epoch 32 iteration 0260/1263: training loss 0.473
Epoch 32 iteration 0280/1263: training loss 0.475
Epoch 32 iteration 0300/1263: training loss 0.474
Epoch 32 iteration 0320/1263: training loss 0.476
Epoch 32 iteration 0340/1263: training loss 0.480
Epoch 32 iteration 0360/1263: training loss 0.485
Epoch 32 iteration 0380/1263: training loss 0.489
Epoch 32 iteration 0400/1263: training loss 0.488
Epoch 32 iteration 0420/1263: training loss 0.490
Epoch 32 iteration 0440/1263: training loss 0.492
Epoch 32 iteration 0460/1263: training loss 0.494
Epoch 32 iteration 0480/1263: training loss 0.491
Epoch 32 iteration 0500/1263: training loss 0.491
Epoch 32 iteration 0520/1263: training loss 0.491
Epoch 32 iteration 0540/1263: training loss 0.492
Epoch 32 iteration 0560/1263: training loss 0.492
Epoch 32 iteration 0580/1263: training loss 0.493
Epoch 32 iteration 0600/1263: training loss 0.495
Epoch 32 iteration 0620/1263: training loss 0.494
Epoch 32 iteration 0640/1263: training loss 0.492
Epoch 32 iteration 0660/1263: training loss 0.492
Epoch 32 iteration 0680/1263: training loss 0.490
Epoch 32 iteration 0700/1263: training loss 0.490
Epoch 32 iteration 0720/1263: training loss 0.489
Epoch 32 iteration 0740/1263: training loss 0.489
Epoch 32 iteration 0760/1263: training loss 0.489
Epoch 32 iteration 0780/1263: training loss 0.489
Epoch 32 iteration 0800/1263: training loss 0.489
Epoch 32 iteration 0820/1263: training loss 0.490
Epoch 32 iteration 0840/1263: training loss 0.491
Epoch 32 iteration 0860/1263: training loss 0.490
Epoch 32 iteration 0880/1263: training loss 0.490
Epoch 32 iteration 0900/1263: training loss 0.490
Epoch 32 iteration 0920/1263: training loss 0.492
Epoch 32 iteration 0940/1263: training loss 0.494
Epoch 32 iteration 0960/1263: training loss 0.494
Epoch 32 iteration 0980/1263: training loss 0.493
Epoch 32 iteration 1000/1263: training loss 0.494
Epoch 32 iteration 1020/1263: training loss 0.494
Epoch 32 iteration 1040/1263: training loss 0.494
Epoch 32 iteration 1060/1263: training loss 0.496
Epoch 32 iteration 1080/1263: training loss 0.497
Epoch 32 iteration 1100/1263: training loss 0.496
Epoch 32 iteration 1120/1263: training loss 0.497
Epoch 32 iteration 1140/1263: training loss 0.497
Epoch 32 iteration 1160/1263: training loss 0.498
Epoch 32 iteration 1180/1263: training loss 0.499
Epoch 32 iteration 1200/1263: training loss 0.499
Epoch 32 iteration 1220/1263: training loss 0.499
Epoch 32 iteration 1240/1263: training loss 0.499
Epoch 32 iteration 1260/1263: training loss 0.498
Epoch 32 validation pixAcc: 0.787, mIoU: 0.430
Epoch 33 iteration 0020/1263: training loss 0.419
Epoch 33 iteration 0040/1263: training loss 0.427
Epoch 33 iteration 0060/1263: training loss 0.441
Epoch 33 iteration 0080/1263: training loss 0.447
Epoch 33 iteration 0100/1263: training loss 0.453
Epoch 33 iteration 0120/1263: training loss 0.450
Epoch 33 iteration 0140/1263: training loss 0.445
Epoch 33 iteration 0160/1263: training loss 0.449
Epoch 33 iteration 0180/1263: training loss 0.450
Epoch 33 iteration 0200/1263: training loss 0.450
Epoch 33 iteration 0220/1263: training loss 0.447
Epoch 33 iteration 0240/1263: training loss 0.444
Epoch 33 iteration 0260/1263: training loss 0.445
Epoch 33 iteration 0280/1263: training loss 0.444
Epoch 33 iteration 0300/1263: training loss 0.442
Epoch 33 iteration 0320/1263: training loss 0.442
Epoch 33 iteration 0340/1263: training loss 0.440
Epoch 33 iteration 0360/1263: training loss 0.440
Epoch 33 iteration 0380/1263: training loss 0.441
Epoch 33 iteration 0400/1263: training loss 0.441
Epoch 33 iteration 0420/1263: training loss 0.444
Epoch 33 iteration 0440/1263: training loss 0.445
Epoch 33 iteration 0460/1263: training loss 0.446
Epoch 33 iteration 0480/1263: training loss 0.448
Epoch 33 iteration 0500/1263: training loss 0.450
Epoch 33 iteration 0520/1263: training loss 0.450
Epoch 33 iteration 0540/1263: training loss 0.452
Epoch 33 iteration 0560/1263: training loss 0.454
Epoch 33 iteration 0580/1263: training loss 0.454
Epoch 33 iteration 0600/1263: training loss 0.458
Epoch 33 iteration 0620/1263: training loss 0.458
Epoch 33 iteration 0640/1263: training loss 0.458
Epoch 33 iteration 0660/1263: training loss 0.458
Epoch 33 iteration 0680/1263: training loss 0.459
Epoch 33 iteration 0700/1263: training loss 0.458
Epoch 33 iteration 0720/1263: training loss 0.457
Epoch 33 iteration 0740/1263: training loss 0.457
Epoch 33 iteration 0760/1263: training loss 0.456
Epoch 33 iteration 0780/1263: training loss 0.455
Epoch 33 iteration 0800/1263: training loss 0.456
Epoch 33 iteration 0820/1263: training loss 0.456
Epoch 33 iteration 0840/1263: training loss 0.457
Epoch 33 iteration 0860/1263: training loss 0.458
Epoch 33 iteration 0880/1263: training loss 0.459
Epoch 33 iteration 0900/1263: training loss 0.460
Epoch 33 iteration 0920/1263: training loss 0.460
Epoch 33 iteration 0940/1263: training loss 0.460
Epoch 33 iteration 0960/1263: training loss 0.459
Epoch 33 iteration 0980/1263: training loss 0.460
Epoch 33 iteration 1000/1263: training loss 0.461
Epoch 33 iteration 1020/1263: training loss 0.460
Epoch 33 iteration 1040/1263: training loss 0.460
Epoch 33 iteration 1060/1263: training loss 0.460
Epoch 33 iteration 1080/1263: training loss 0.460
Epoch 33 iteration 1100/1263: training loss 0.461
Epoch 33 iteration 1120/1263: training loss 0.461
Epoch 33 iteration 1140/1263: training loss 0.462
Epoch 33 iteration 1160/1263: training loss 0.462
Epoch 33 iteration 1180/1263: training loss 0.462
Epoch 33 iteration 1200/1263: training loss 0.462
Epoch 33 iteration 1220/1263: training loss 0.461
Epoch 33 iteration 1240/1263: training loss 0.461
Epoch 33 iteration 1260/1263: training loss 0.461
Epoch 33 validation pixAcc: 0.793, mIoU: 0.434
Epoch 34 iteration 0020/1263: training loss 0.449
Epoch 34 iteration 0040/1263: training loss 0.444
Epoch 34 iteration 0060/1263: training loss 0.434
Epoch 34 iteration 0080/1263: training loss 0.418
Epoch 34 iteration 0100/1263: training loss 0.412
Epoch 34 iteration 0120/1263: training loss 0.416
Epoch 34 iteration 0140/1263: training loss 0.417
Epoch 34 iteration 0160/1263: training loss 0.419
Epoch 34 iteration 0180/1263: training loss 0.420
Epoch 34 iteration 0200/1263: training loss 0.419
Epoch 34 iteration 0220/1263: training loss 0.421
Epoch 34 iteration 0240/1263: training loss 0.421
Epoch 34 iteration 0260/1263: training loss 0.421
Epoch 34 iteration 0280/1263: training loss 0.424
Epoch 34 iteration 0300/1263: training loss 0.424
Epoch 34 iteration 0320/1263: training loss 0.427
Epoch 34 iteration 0340/1263: training loss 0.427
Epoch 34 iteration 0360/1263: training loss 0.429
Epoch 34 iteration 0380/1263: training loss 0.428
Epoch 34 iteration 0400/1263: training loss 0.428
Epoch 34 iteration 0420/1263: training loss 0.429
Epoch 34 iteration 0440/1263: training loss 0.431
Epoch 34 iteration 0460/1263: training loss 0.431
Epoch 34 iteration 0480/1263: training loss 0.431
Epoch 34 iteration 0500/1263: training loss 0.431
Epoch 34 iteration 0520/1263: training loss 0.430
Epoch 34 iteration 0540/1263: training loss 0.431
Epoch 34 iteration 0560/1263: training loss 0.433
Epoch 34 iteration 0580/1263: training loss 0.433
Epoch 34 iteration 0600/1263: training loss 0.439
Epoch 34 iteration 0620/1263: training loss 0.446
Epoch 34 iteration 0640/1263: training loss 0.448
Epoch 34 iteration 0660/1263: training loss 0.454
Epoch 34 iteration 0680/1263: training loss 0.457
Epoch 34 iteration 0700/1263: training loss 0.460
Epoch 34 iteration 0720/1263: training loss 0.462
Epoch 34 iteration 0740/1263: training loss 0.464
Epoch 34 iteration 0760/1263: training loss 0.467
Epoch 34 iteration 0780/1263: training loss 0.469
Epoch 34 iteration 0800/1263: training loss 0.470
Epoch 34 iteration 0820/1263: training loss 0.470
Epoch 34 iteration 0840/1263: training loss 0.470
Epoch 34 iteration 0860/1263: training loss 0.471
Epoch 34 iteration 0880/1263: training loss 0.471
Epoch 34 iteration 0900/1263: training loss 0.473
Epoch 34 iteration 0920/1263: training loss 0.475
Epoch 34 iteration 0940/1263: training loss 0.477
Epoch 34 iteration 0960/1263: training loss 0.478
Epoch 34 iteration 0980/1263: training loss 0.477
Epoch 34 iteration 1000/1263: training loss 0.479
Epoch 34 iteration 1020/1263: training loss 0.478
Epoch 34 iteration 1040/1263: training loss 0.479
Epoch 34 iteration 1060/1263: training loss 0.479
Epoch 34 iteration 1080/1263: training loss 0.480
Epoch 34 iteration 1100/1263: training loss 0.480
Epoch 34 iteration 1120/1263: training loss 0.481
Epoch 34 iteration 1140/1263: training loss 0.481
Epoch 34 iteration 1160/1263: training loss 0.482
Epoch 34 iteration 1180/1263: training loss 0.482
Epoch 34 iteration 1200/1263: training loss 0.483
Epoch 34 iteration 1220/1263: training loss 0.484
Epoch 34 iteration 1240/1263: training loss 0.484
Epoch 34 iteration 1260/1263: training loss 0.483
Epoch 34 validation pixAcc: 0.785, mIoU: 0.417
Epoch 35 iteration 0020/1263: training loss 0.489
Epoch 35 iteration 0040/1263: training loss 0.454
Epoch 35 iteration 0060/1263: training loss 0.449
Epoch 35 iteration 0080/1263: training loss 0.465
Epoch 35 iteration 0100/1263: training loss 0.476
Epoch 35 iteration 0120/1263: training loss 0.477
Epoch 35 iteration 0140/1263: training loss 0.477
Epoch 35 iteration 0160/1263: training loss 0.472
Epoch 35 iteration 0180/1263: training loss 0.471
Epoch 35 iteration 0200/1263: training loss 0.470
Epoch 35 iteration 0220/1263: training loss 0.472
Epoch 35 iteration 0240/1263: training loss 0.471
Epoch 35 iteration 0260/1263: training loss 0.468
Epoch 35 iteration 0280/1263: training loss 0.467
Epoch 35 iteration 0300/1263: training loss 0.469
Epoch 35 iteration 0320/1263: training loss 0.469
Epoch 35 iteration 0340/1263: training loss 0.468
Epoch 35 iteration 0360/1263: training loss 0.467
Epoch 35 iteration 0380/1263: training loss 0.470
Epoch 35 iteration 0400/1263: training loss 0.470
Epoch 35 iteration 0420/1263: training loss 0.469
Epoch 35 iteration 0440/1263: training loss 0.469
Epoch 35 iteration 0460/1263: training loss 0.468
Epoch 35 iteration 0480/1263: training loss 0.468
Epoch 35 iteration 0500/1263: training loss 0.468
Epoch 35 iteration 0520/1263: training loss 0.468
Epoch 35 iteration 0540/1263: training loss 0.467
Epoch 35 iteration 0560/1263: training loss 0.469
Epoch 35 iteration 0580/1263: training loss 0.469
Epoch 35 iteration 0600/1263: training loss 0.468
Epoch 35 iteration 0620/1263: training loss 0.469
Epoch 35 iteration 0640/1263: training loss 0.468
Epoch 35 iteration 0660/1263: training loss 0.468
Epoch 35 iteration 0680/1263: training loss 0.468
Epoch 35 iteration 0700/1263: training loss 0.467
Epoch 35 iteration 0720/1263: training loss 0.468
Epoch 35 iteration 0740/1263: training loss 0.468
Epoch 35 iteration 0760/1263: training loss 0.467
Epoch 35 iteration 0780/1263: training loss 0.466
Epoch 35 iteration 0800/1263: training loss 0.466
Epoch 35 iteration 0820/1263: training loss 0.465
Epoch 35 iteration 0840/1263: training loss 0.466
Epoch 35 iteration 0860/1263: training loss 0.464
Epoch 35 iteration 0880/1263: training loss 0.466
Epoch 35 iteration 0900/1263: training loss 0.464
Epoch 35 iteration 0920/1263: training loss 0.465
Epoch 35 iteration 0940/1263: training loss 0.464
Epoch 35 iteration 0960/1263: training loss 0.464
Epoch 35 iteration 0980/1263: training loss 0.463
Epoch 35 iteration 1000/1263: training loss 0.462
Epoch 35 iteration 1020/1263: training loss 0.462
Epoch 35 iteration 1040/1263: training loss 0.462
Epoch 35 iteration 1060/1263: training loss 0.462
Epoch 35 iteration 1080/1263: training loss 0.463
Epoch 35 iteration 1100/1263: training loss 0.462
Epoch 35 iteration 1120/1263: training loss 0.463
Epoch 35 iteration 1140/1263: training loss 0.464
Epoch 35 iteration 1160/1263: training loss 0.464
Epoch 35 iteration 1180/1263: training loss 0.465
Epoch 35 iteration 1200/1263: training loss 0.467
Epoch 35 iteration 1220/1263: training loss 0.467
Epoch 35 iteration 1240/1263: training loss 0.468
Epoch 35 iteration 1260/1263: training loss 0.470
Epoch 35 validation pixAcc: 0.772, mIoU: 0.408
Epoch 36 iteration 0020/1263: training loss 0.502
Epoch 36 iteration 0040/1263: training loss 0.504
Epoch 36 iteration 0060/1263: training loss 0.502
Epoch 36 iteration 0080/1263: training loss 0.498
Epoch 36 iteration 0100/1263: training loss 0.481
Epoch 36 iteration 0120/1263: training loss 0.475
Epoch 36 iteration 0140/1263: training loss 0.477
Epoch 36 iteration 0160/1263: training loss 0.477
Epoch 36 iteration 0180/1263: training loss 0.475
Epoch 36 iteration 0200/1263: training loss 0.474
Epoch 36 iteration 0220/1263: training loss 0.476
Epoch 36 iteration 0240/1263: training loss 0.476
Epoch 36 iteration 0260/1263: training loss 0.473
Epoch 36 iteration 0280/1263: training loss 0.469
Epoch 36 iteration 0300/1263: training loss 0.468
Epoch 36 iteration 0320/1263: training loss 0.466
Epoch 36 iteration 0340/1263: training loss 0.465
Epoch 36 iteration 0360/1263: training loss 0.463
Epoch 36 iteration 0380/1263: training loss 0.462
Epoch 36 iteration 0400/1263: training loss 0.459
Epoch 36 iteration 0420/1263: training loss 0.457
Epoch 36 iteration 0440/1263: training loss 0.458
Epoch 36 iteration 0460/1263: training loss 0.459
Epoch 36 iteration 0480/1263: training loss 0.460
Epoch 36 iteration 0500/1263: training loss 0.461
Epoch 36 iteration 0520/1263: training loss 0.461
Epoch 36 iteration 0540/1263: training loss 0.463
Epoch 36 iteration 0560/1263: training loss 0.463
Epoch 36 iteration 0580/1263: training loss 0.463
Epoch 36 iteration 0600/1263: training loss 0.463
Epoch 36 iteration 0620/1263: training loss 0.462
Epoch 36 iteration 0640/1263: training loss 0.462
Epoch 36 iteration 0660/1263: training loss 0.462
Epoch 36 iteration 0680/1263: training loss 0.461
Epoch 36 iteration 0700/1263: training loss 0.460
Epoch 36 iteration 0720/1263: training loss 0.460
Epoch 36 iteration 0740/1263: training loss 0.460
Epoch 36 iteration 0760/1263: training loss 0.459
Epoch 36 iteration 0780/1263: training loss 0.458
Epoch 36 iteration 0800/1263: training loss 0.458
Epoch 36 iteration 0820/1263: training loss 0.460
Epoch 36 iteration 0840/1263: training loss 0.459
Epoch 36 iteration 0860/1263: training loss 0.458
Epoch 36 iteration 0880/1263: training loss 0.456
Epoch 36 iteration 0900/1263: training loss 0.456
Epoch 36 iteration 0920/1263: training loss 0.455
Epoch 36 iteration 0940/1263: training loss 0.455
Epoch 36 iteration 0960/1263: training loss 0.455
Epoch 36 iteration 0980/1263: training loss 0.454
Epoch 36 iteration 1000/1263: training loss 0.453
Epoch 36 iteration 1020/1263: training loss 0.454
Epoch 36 iteration 1040/1263: training loss 0.453
Epoch 36 iteration 1060/1263: training loss 0.453
Epoch 36 iteration 1080/1263: training loss 0.454
Epoch 36 iteration 1100/1263: training loss 0.454
Epoch 36 iteration 1120/1263: training loss 0.454
Epoch 36 iteration 1140/1263: training loss 0.453
Epoch 36 iteration 1160/1263: training loss 0.452
Epoch 36 iteration 1180/1263: training loss 0.453
Epoch 36 iteration 1200/1263: training loss 0.454
Epoch 36 iteration 1220/1263: training loss 0.454
Epoch 36 iteration 1240/1263: training loss 0.454
Epoch 36 iteration 1260/1263: training loss 0.454
Epoch 36 validation pixAcc: 0.792, mIoU: 0.428
Epoch 37 iteration 0020/1263: training loss 0.471
Epoch 37 iteration 0040/1263: training loss 0.463
Epoch 37 iteration 0060/1263: training loss 0.452
Epoch 37 iteration 0080/1263: training loss 0.446
Epoch 37 iteration 0100/1263: training loss 0.440
Epoch 37 iteration 0120/1263: training loss 0.435
Epoch 37 iteration 0140/1263: training loss 0.433
Epoch 37 iteration 0160/1263: training loss 0.435
Epoch 37 iteration 0180/1263: training loss 0.434
Epoch 37 iteration 0200/1263: training loss 0.432
Epoch 37 iteration 0220/1263: training loss 0.430
Epoch 37 iteration 0240/1263: training loss 0.430
Epoch 37 iteration 0260/1263: training loss 0.432
Epoch 37 iteration 0280/1263: training loss 0.434
Epoch 37 iteration 0300/1263: training loss 0.433
Epoch 37 iteration 0320/1263: training loss 0.435
Epoch 37 iteration 0340/1263: training loss 0.434
Epoch 37 iteration 0360/1263: training loss 0.433
Epoch 37 iteration 0380/1263: training loss 0.433
Epoch 37 iteration 0400/1263: training loss 0.435
Epoch 37 iteration 0420/1263: training loss 0.436
Epoch 37 iteration 0440/1263: training loss 0.435
Epoch 37 iteration 0460/1263: training loss 0.437
Epoch 37 iteration 0480/1263: training loss 0.436
Epoch 37 iteration 0500/1263: training loss 0.435
Epoch 37 iteration 0520/1263: training loss 0.435
Epoch 37 iteration 0540/1263: training loss 0.434
Epoch 37 iteration 0560/1263: training loss 0.434
Epoch 37 iteration 0580/1263: training loss 0.434
Epoch 37 iteration 0600/1263: training loss 0.434
Epoch 37 iteration 0620/1263: training loss 0.433
Epoch 37 iteration 0640/1263: training loss 0.432
Epoch 37 iteration 0660/1263: training loss 0.431
Epoch 37 iteration 0680/1263: training loss 0.431
Epoch 37 iteration 0700/1263: training loss 0.431
Epoch 37 iteration 0720/1263: training loss 0.430
Epoch 37 iteration 0740/1263: training loss 0.430
Epoch 37 iteration 0760/1263: training loss 0.430
Epoch 37 iteration 0780/1263: training loss 0.428
Epoch 37 iteration 0800/1263: training loss 0.430
Epoch 37 iteration 0820/1263: training loss 0.431
Epoch 37 iteration 0840/1263: training loss 0.432
Epoch 37 iteration 0860/1263: training loss 0.432
Epoch 37 iteration 0880/1263: training loss 0.432
Epoch 37 iteration 0900/1263: training loss 0.431
Epoch 37 iteration 0920/1263: training loss 0.431
Epoch 37 iteration 0940/1263: training loss 0.431
Epoch 37 iteration 0960/1263: training loss 0.431
Epoch 37 iteration 0980/1263: training loss 0.432
Epoch 37 iteration 1000/1263: training loss 0.432
Epoch 37 iteration 1020/1263: training loss 0.432
Epoch 37 iteration 1040/1263: training loss 0.432
Epoch 37 iteration 1060/1263: training loss 0.432
Epoch 37 iteration 1080/1263: training loss 0.432
Epoch 37 iteration 1100/1263: training loss 0.432
Epoch 37 iteration 1120/1263: training loss 0.433
Epoch 37 iteration 1140/1263: training loss 0.433
Epoch 37 iteration 1160/1263: training loss 0.433
Epoch 37 iteration 1180/1263: training loss 0.433
Epoch 37 iteration 1200/1263: training loss 0.434
Epoch 37 iteration 1220/1263: training loss 0.434
Epoch 37 iteration 1240/1263: training loss 0.434
Epoch 37 iteration 1260/1263: training loss 0.434
Epoch 37 validation pixAcc: 0.795, mIoU: 0.441
Epoch 38 iteration 0020/1263: training loss 0.424
Epoch 38 iteration 0040/1263: training loss 0.407
Epoch 38 iteration 0060/1263: training loss 0.424
Epoch 38 iteration 0080/1263: training loss 0.426
Epoch 38 iteration 0100/1263: training loss 0.423
Epoch 38 iteration 0120/1263: training loss 0.420
Epoch 38 iteration 0140/1263: training loss 0.417
Epoch 38 iteration 0160/1263: training loss 0.421
Epoch 38 iteration 0180/1263: training loss 0.417
Epoch 38 iteration 0200/1263: training loss 0.417
Epoch 38 iteration 0220/1263: training loss 0.417
Epoch 38 iteration 0240/1263: training loss 0.418
Epoch 38 iteration 0260/1263: training loss 0.420
Epoch 38 iteration 0280/1263: training loss 0.424
Epoch 38 iteration 0300/1263: training loss 0.424
Epoch 38 iteration 0320/1263: training loss 0.425
Epoch 38 iteration 0340/1263: training loss 0.423
Epoch 38 iteration 0360/1263: training loss 0.424
Epoch 38 iteration 0380/1263: training loss 0.430
Epoch 38 iteration 0400/1263: training loss 0.433
Epoch 38 iteration 0420/1263: training loss 0.437
Epoch 38 iteration 0440/1263: training loss 0.441
Epoch 38 iteration 0460/1263: training loss 0.441
Epoch 38 iteration 0480/1263: training loss 0.442
Epoch 38 iteration 0500/1263: training loss 0.442
Epoch 38 iteration 0520/1263: training loss 0.443
Epoch 38 iteration 0540/1263: training loss 0.442
Epoch 38 iteration 0560/1263: training loss 0.442
Epoch 38 iteration 0580/1263: training loss 0.442
Epoch 38 iteration 0600/1263: training loss 0.441
Epoch 38 iteration 0620/1263: training loss 0.441
Epoch 38 iteration 0640/1263: training loss 0.440
Epoch 38 iteration 0660/1263: training loss 0.439
Epoch 38 iteration 0680/1263: training loss 0.438
Epoch 38 iteration 0700/1263: training loss 0.440
Epoch 38 iteration 0720/1263: training loss 0.441
Epoch 38 iteration 0740/1263: training loss 0.441
Epoch 38 iteration 0760/1263: training loss 0.441
Epoch 38 iteration 0780/1263: training loss 0.442
Epoch 38 iteration 0800/1263: training loss 0.442
Epoch 38 iteration 0820/1263: training loss 0.441
Epoch 38 iteration 0840/1263: training loss 0.441
Epoch 38 iteration 0860/1263: training loss 0.442
Epoch 38 iteration 0880/1263: training loss 0.442
Epoch 38 iteration 0900/1263: training loss 0.441
Epoch 38 iteration 0920/1263: training loss 0.443
Epoch 38 iteration 0940/1263: training loss 0.442
Epoch 38 iteration 0960/1263: training loss 0.442
Epoch 38 iteration 0980/1263: training loss 0.442
Epoch 38 iteration 1000/1263: training loss 0.443
Epoch 38 iteration 1020/1263: training loss 0.444
Epoch 38 iteration 1040/1263: training loss 0.443
Epoch 38 iteration 1060/1263: training loss 0.444
Epoch 38 iteration 1080/1263: training loss 0.444
Epoch 38 iteration 1100/1263: training loss 0.444
Epoch 38 iteration 1120/1263: training loss 0.445
Epoch 38 iteration 1140/1263: training loss 0.445
Epoch 38 iteration 1160/1263: training loss 0.445
Epoch 38 iteration 1180/1264: training loss 0.445
Epoch 38 iteration 1200/1264: training loss 0.444
Epoch 38 iteration 1220/1264: training loss 0.445
Epoch 38 iteration 1240/1264: training loss 0.445
Epoch 38 iteration 1260/1264: training loss 0.445
Epoch 38 validation pixAcc: 0.790, mIoU: 0.437
Epoch 39 iteration 0020/1263: training loss 0.400
Epoch 39 iteration 0040/1263: training loss 0.395
Epoch 39 iteration 0060/1263: training loss 0.391
Epoch 39 iteration 0080/1263: training loss 0.394
Epoch 39 iteration 0100/1263: training loss 0.401
Epoch 39 iteration 0120/1263: training loss 0.409
Epoch 39 iteration 0140/1263: training loss 0.410
Epoch 39 iteration 0160/1263: training loss 0.412
Epoch 39 iteration 0180/1263: training loss 0.416
Epoch 39 iteration 0200/1263: training loss 0.415
Epoch 39 iteration 0220/1263: training loss 0.419
Epoch 39 iteration 0240/1263: training loss 0.421
Epoch 39 iteration 0260/1263: training loss 0.420
Epoch 39 iteration 0280/1263: training loss 0.420
Epoch 39 iteration 0300/1263: training loss 0.422
Epoch 39 iteration 0320/1263: training loss 0.421
Epoch 39 iteration 0340/1263: training loss 0.421
Epoch 39 iteration 0360/1263: training loss 0.420
Epoch 39 iteration 0380/1263: training loss 0.419
Epoch 39 iteration 0400/1263: training loss 0.419
Epoch 39 iteration 0420/1263: training loss 0.419
Epoch 39 iteration 0440/1263: training loss 0.421
Epoch 39 iteration 0460/1263: training loss 0.420
Epoch 39 iteration 0480/1263: training loss 0.421
Epoch 39 iteration 0500/1263: training loss 0.421
Epoch 39 iteration 0520/1263: training loss 0.420
Epoch 39 iteration 0540/1263: training loss 0.422
Epoch 39 iteration 0560/1263: training loss 0.423
Epoch 39 iteration 0580/1263: training loss 0.425
Epoch 39 iteration 0600/1263: training loss 0.424
Epoch 39 iteration 0620/1263: training loss 0.424
Epoch 39 iteration 0640/1263: training loss 0.423
Epoch 39 iteration 0660/1263: training loss 0.423
Epoch 39 iteration 0680/1263: training loss 0.423
Epoch 39 iteration 0700/1263: training loss 0.425
Epoch 39 iteration 0720/1263: training loss 0.425
Epoch 39 iteration 0740/1263: training loss 0.425
Epoch 39 iteration 0760/1263: training loss 0.426
Epoch 39 iteration 0780/1263: training loss 0.425
Epoch 39 iteration 0800/1263: training loss 0.425
Epoch 39 iteration 0820/1263: training loss 0.425
Epoch 39 iteration 0840/1263: training loss 0.425
Epoch 39 iteration 0860/1263: training loss 0.427
Epoch 39 iteration 0880/1263: training loss 0.428
Epoch 39 iteration 0900/1263: training loss 0.429
Epoch 39 iteration 0920/1263: training loss 0.430
Epoch 39 iteration 0940/1263: training loss 0.430
Epoch 39 iteration 0960/1263: training loss 0.430
Epoch 39 iteration 0980/1263: training loss 0.431
Epoch 39 iteration 1000/1263: training loss 0.430
Epoch 39 iteration 1020/1263: training loss 0.430
Epoch 39 iteration 1040/1263: training loss 0.430
Epoch 39 iteration 1060/1263: training loss 0.430
Epoch 39 iteration 1080/1263: training loss 0.429
Epoch 39 iteration 1100/1263: training loss 0.430
Epoch 39 iteration 1120/1263: training loss 0.429
Epoch 39 iteration 1140/1263: training loss 0.429
Epoch 39 iteration 1160/1263: training loss 0.428
Epoch 39 iteration 1180/1263: training loss 0.428
Epoch 39 iteration 1200/1263: training loss 0.429
Epoch 39 iteration 1220/1263: training loss 0.429
Epoch 39 iteration 1240/1263: training loss 0.429
Epoch 39 iteration 1260/1263: training loss 0.429
Epoch 39 validation pixAcc: 0.794, mIoU: 0.435
Epoch 40 iteration 0020/1263: training loss 0.441
Epoch 40 iteration 0040/1263: training loss 0.422
Epoch 40 iteration 0060/1263: training loss 0.424
Epoch 40 iteration 0080/1263: training loss 0.418
Epoch 40 iteration 0100/1263: training loss 0.411
Epoch 40 iteration 0120/1263: training loss 0.401
Epoch 40 iteration 0140/1263: training loss 0.399
Epoch 40 iteration 0160/1263: training loss 0.404
Epoch 40 iteration 0180/1263: training loss 0.408
Epoch 40 iteration 0200/1263: training loss 0.410
Epoch 40 iteration 0220/1263: training loss 0.410
Epoch 40 iteration 0240/1263: training loss 0.411
Epoch 40 iteration 0260/1263: training loss 0.409
Epoch 40 iteration 0280/1263: training loss 0.413
Epoch 40 iteration 0300/1263: training loss 0.418
Epoch 40 iteration 0320/1263: training loss 0.416
Epoch 40 iteration 0340/1263: training loss 0.415
Epoch 40 iteration 0360/1263: training loss 0.415
Epoch 40 iteration 0380/1263: training loss 0.415
Epoch 40 iteration 0400/1263: training loss 0.416
Epoch 40 iteration 0420/1263: training loss 0.415
Epoch 40 iteration 0440/1263: training loss 0.414
Epoch 40 iteration 0460/1263: training loss 0.414
Epoch 40 iteration 0480/1263: training loss 0.414
Epoch 40 iteration 0500/1263: training loss 0.415
Epoch 40 iteration 0520/1263: training loss 0.415
Epoch 40 iteration 0540/1263: training loss 0.416
Epoch 40 iteration 0560/1263: training loss 0.416
Epoch 40 iteration 0580/1263: training loss 0.418
Epoch 40 iteration 0600/1263: training loss 0.418
Epoch 40 iteration 0620/1263: training loss 0.419
Epoch 40 iteration 0640/1263: training loss 0.420
Epoch 40 iteration 0660/1263: training loss 0.422
Epoch 40 iteration 0680/1263: training loss 0.425
Epoch 40 iteration 0700/1263: training loss 0.426
Epoch 40 iteration 0720/1263: training loss 0.428
Epoch 40 iteration 0740/1263: training loss 0.430
Epoch 40 iteration 0760/1263: training loss 0.430
Epoch 40 iteration 0780/1263: training loss 0.431
Epoch 40 iteration 0800/1263: training loss 0.432
Epoch 40 iteration 0820/1263: training loss 0.434
Epoch 40 iteration 0840/1263: training loss 0.434
Epoch 40 iteration 0860/1263: training loss 0.433
Epoch 40 iteration 0880/1263: training loss 0.433
Epoch 40 iteration 0900/1263: training loss 0.433
Epoch 40 iteration 0920/1263: training loss 0.432
Epoch 40 iteration 0940/1263: training loss 0.433
Epoch 40 iteration 0960/1263: training loss 0.433
Epoch 40 iteration 0980/1263: training loss 0.433
Epoch 40 iteration 1000/1263: training loss 0.432
Epoch 40 iteration 1020/1263: training loss 0.431
Epoch 40 iteration 1040/1263: training loss 0.431
Epoch 40 iteration 1060/1263: training loss 0.431
Epoch 40 iteration 1080/1263: training loss 0.430
Epoch 40 iteration 1100/1263: training loss 0.431
Epoch 40 iteration 1120/1263: training loss 0.431
Epoch 40 iteration 1140/1263: training loss 0.432
Epoch 40 iteration 1160/1263: training loss 0.432
Epoch 40 iteration 1180/1263: training loss 0.432
Epoch 40 iteration 1200/1263: training loss 0.433
Epoch 40 iteration 1220/1263: training loss 0.434
Epoch 40 iteration 1240/1263: training loss 0.434
Epoch 40 iteration 1260/1263: training loss 0.434
Epoch 40 validation pixAcc: 0.787, mIoU: 0.426
Epoch 41 iteration 0020/1263: training loss 0.432
Epoch 41 iteration 0040/1263: training loss 0.420
Epoch 41 iteration 0060/1263: training loss 0.411
Epoch 41 iteration 0080/1263: training loss 0.418
Epoch 41 iteration 0100/1263: training loss 0.418
Epoch 41 iteration 0120/1263: training loss 0.413
Epoch 41 iteration 0140/1263: training loss 0.410
Epoch 41 iteration 0160/1263: training loss 0.413
Epoch 41 iteration 0180/1263: training loss 0.409
Epoch 41 iteration 0200/1263: training loss 0.408
Epoch 41 iteration 0220/1263: training loss 0.414
Epoch 41 iteration 0240/1263: training loss 0.416
Epoch 41 iteration 0260/1263: training loss 0.416
Epoch 41 iteration 0280/1263: training loss 0.417
Epoch 41 iteration 0300/1263: training loss 0.418
Epoch 41 iteration 0320/1263: training loss 0.418
Epoch 41 iteration 0340/1263: training loss 0.421
Epoch 41 iteration 0360/1263: training loss 0.421
Epoch 41 iteration 0380/1263: training loss 0.420
Epoch 41 iteration 0400/1263: training loss 0.422
Epoch 41 iteration 0420/1263: training loss 0.420
Epoch 41 iteration 0440/1263: training loss 0.418
Epoch 41 iteration 0460/1263: training loss 0.417
Epoch 41 iteration 0480/1263: training loss 0.414
Epoch 41 iteration 0500/1263: training loss 0.413
Epoch 41 iteration 0520/1263: training loss 0.414
Epoch 41 iteration 0540/1263: training loss 0.413
Epoch 41 iteration 0560/1263: training loss 0.413
Epoch 41 iteration 0580/1263: training loss 0.413
Epoch 41 iteration 0600/1263: training loss 0.412
Epoch 41 iteration 0620/1263: training loss 0.412
Epoch 41 iteration 0640/1263: training loss 0.411
Epoch 41 iteration 0660/1263: training loss 0.411
Epoch 41 iteration 0680/1263: training loss 0.411
Epoch 41 iteration 0700/1263: training loss 0.412
Epoch 41 iteration 0720/1263: training loss 0.412
Epoch 41 iteration 0740/1263: training loss 0.413
Epoch 41 iteration 0760/1263: training loss 0.413
Epoch 41 iteration 0780/1263: training loss 0.413
Epoch 41 iteration 0800/1263: training loss 0.412
Epoch 41 iteration 0820/1263: training loss 0.412
Epoch 41 iteration 0840/1263: training loss 0.412
Epoch 41 iteration 0860/1263: training loss 0.413
Epoch 41 iteration 0880/1263: training loss 0.412
Epoch 41 iteration 0900/1263: training loss 0.413
Epoch 41 iteration 0920/1263: training loss 0.414
Epoch 41 iteration 0940/1263: training loss 0.416
Epoch 41 iteration 0960/1263: training loss 0.416
Epoch 41 iteration 0980/1263: training loss 0.415
Epoch 41 iteration 1000/1263: training loss 0.415
Epoch 41 iteration 1020/1263: training loss 0.415
Epoch 41 iteration 1040/1263: training loss 0.415
Epoch 41 iteration 1060/1263: training loss 0.415
Epoch 41 iteration 1080/1263: training loss 0.416
Epoch 41 iteration 1100/1263: training loss 0.416
Epoch 41 iteration 1120/1263: training loss 0.417
Epoch 41 iteration 1140/1263: training loss 0.417
Epoch 41 iteration 1160/1263: training loss 0.418
Epoch 41 iteration 1180/1263: training loss 0.420
Epoch 41 iteration 1200/1263: training loss 0.420
Epoch 41 iteration 1220/1263: training loss 0.420
Epoch 41 iteration 1240/1263: training loss 0.419
Epoch 41 iteration 1260/1263: training loss 0.419
Epoch 41 validation pixAcc: 0.792, mIoU: 0.438
Epoch 42 iteration 0020/1263: training loss 0.416
Epoch 42 iteration 0040/1263: training loss 0.393
Epoch 42 iteration 0060/1263: training loss 0.397
Epoch 42 iteration 0080/1263: training loss 0.402
Epoch 42 iteration 0100/1263: training loss 0.388
Epoch 42 iteration 0120/1263: training loss 0.387
Epoch 42 iteration 0140/1263: training loss 0.384
Epoch 42 iteration 0160/1263: training loss 0.382
Epoch 42 iteration 0180/1263: training loss 0.383
Epoch 42 iteration 0200/1263: training loss 0.383
Epoch 42 iteration 0220/1263: training loss 0.382
Epoch 42 iteration 0240/1263: training loss 0.381
Epoch 42 iteration 0260/1263: training loss 0.381
Epoch 42 iteration 0280/1263: training loss 0.382
Epoch 42 iteration 0300/1263: training loss 0.384
Epoch 42 iteration 0320/1263: training loss 0.384
Epoch 42 iteration 0340/1263: training loss 0.385
Epoch 42 iteration 0360/1263: training loss 0.384
Epoch 42 iteration 0380/1263: training loss 0.384
Epoch 42 iteration 0400/1263: training loss 0.385
Epoch 42 iteration 0420/1263: training loss 0.384
Epoch 42 iteration 0440/1263: training loss 0.384
Epoch 42 iteration 0460/1263: training loss 0.384
Epoch 42 iteration 0480/1263: training loss 0.384
Epoch 42 iteration 0500/1263: training loss 0.385
Epoch 42 iteration 0520/1263: training loss 0.385
Epoch 42 iteration 0540/1263: training loss 0.385
Epoch 42 iteration 0560/1263: training loss 0.386
Epoch 42 iteration 0580/1263: training loss 0.387
Epoch 42 iteration 0600/1263: training loss 0.388
Epoch 42 iteration 0620/1263: training loss 0.388
Epoch 42 iteration 0640/1263: training loss 0.387
Epoch 42 iteration 0660/1263: training loss 0.388
Epoch 42 iteration 0680/1263: training loss 0.389
Epoch 42 iteration 0700/1263: training loss 0.389
Epoch 42 iteration 0720/1263: training loss 0.389
Epoch 42 iteration 0740/1263: training loss 0.390
Epoch 42 iteration 0760/1263: training loss 0.389
Epoch 42 iteration 0780/1263: training loss 0.389
Epoch 42 iteration 0800/1263: training loss 0.391
Epoch 42 iteration 0820/1263: training loss 0.391
Epoch 42 iteration 0840/1263: training loss 0.391
Epoch 42 iteration 0860/1263: training loss 0.393
Epoch 42 iteration 0880/1263: training loss 0.393
Epoch 42 iteration 0900/1263: training loss 0.394
Epoch 42 iteration 0920/1263: training loss 0.394
Epoch 42 iteration 0940/1263: training loss 0.394
Epoch 42 iteration 0960/1263: training loss 0.395
Epoch 42 iteration 0980/1263: training loss 0.395
Epoch 42 iteration 1000/1263: training loss 0.396
Epoch 42 iteration 1020/1263: training loss 0.396
Epoch 42 iteration 1040/1263: training loss 0.395
Epoch 42 iteration 1060/1263: training loss 0.395
Epoch 42 iteration 1080/1263: training loss 0.396
Epoch 42 iteration 1100/1263: training loss 0.396
Epoch 42 iteration 1120/1263: training loss 0.396
Epoch 42 iteration 1140/1263: training loss 0.396
Epoch 42 iteration 1160/1263: training loss 0.397
Epoch 42 iteration 1180/1263: training loss 0.397
Epoch 42 iteration 1200/1263: training loss 0.397
Epoch 42 iteration 1220/1263: training loss 0.397
Epoch 42 iteration 1240/1263: training loss 0.398
Epoch 42 iteration 1260/1263: training loss 0.397
Epoch 42 validation pixAcc: 0.791, mIoU: 0.440
Epoch 43 iteration 0020/1263: training loss 0.377
Epoch 43 iteration 0040/1263: training loss 0.395
Epoch 43 iteration 0060/1263: training loss 0.403
Epoch 43 iteration 0080/1263: training loss 0.411
Epoch 43 iteration 0100/1263: training loss 0.404
Epoch 43 iteration 0120/1263: training loss 0.399
Epoch 43 iteration 0140/1263: training loss 0.391
Epoch 43 iteration 0160/1263: training loss 0.389
Epoch 43 iteration 0180/1263: training loss 0.387
Epoch 43 iteration 0200/1263: training loss 0.383
Epoch 43 iteration 0220/1263: training loss 0.384
Epoch 43 iteration 0240/1263: training loss 0.381
Epoch 43 iteration 0260/1263: training loss 0.383
Epoch 43 iteration 0280/1263: training loss 0.382
Epoch 43 iteration 0300/1263: training loss 0.382
Epoch 43 iteration 0320/1263: training loss 0.382
Epoch 43 iteration 0340/1263: training loss 0.384
Epoch 43 iteration 0360/1263: training loss 0.384
Epoch 43 iteration 0380/1263: training loss 0.382
Epoch 43 iteration 0400/1263: training loss 0.382
Epoch 43 iteration 0420/1263: training loss 0.382
Epoch 43 iteration 0440/1263: training loss 0.383
Epoch 43 iteration 0460/1263: training loss 0.384
Epoch 43 iteration 0480/1263: training loss 0.383
Epoch 43 iteration 0500/1263: training loss 0.384
Epoch 43 iteration 0520/1263: training loss 0.385
Epoch 43 iteration 0540/1263: training loss 0.385
Epoch 43 iteration 0560/1263: training loss 0.385
Epoch 43 iteration 0580/1263: training loss 0.385
Epoch 43 iteration 0600/1263: training loss 0.386
Epoch 43 iteration 0620/1263: training loss 0.386
Epoch 43 iteration 0640/1263: training loss 0.385
Epoch 43 iteration 0660/1263: training loss 0.385
Epoch 43 iteration 0680/1263: training loss 0.385
Epoch 43 iteration 0700/1263: training loss 0.384
Epoch 43 iteration 0720/1263: training loss 0.383
Epoch 43 iteration 0740/1263: training loss 0.383
Epoch 43 iteration 0760/1263: training loss 0.384
Epoch 43 iteration 0780/1263: training loss 0.384
Epoch 43 iteration 0800/1263: training loss 0.385
Epoch 43 iteration 0820/1263: training loss 0.384
Epoch 43 iteration 0840/1263: training loss 0.385
Epoch 43 iteration 0860/1263: training loss 0.386
Epoch 43 iteration 0880/1263: training loss 0.387
Epoch 43 iteration 0900/1263: training loss 0.387
Epoch 43 iteration 0920/1263: training loss 0.388
Epoch 43 iteration 0940/1263: training loss 0.388
Epoch 43 iteration 0960/1263: training loss 0.388
Epoch 43 iteration 0980/1263: training loss 0.388
Epoch 43 iteration 1000/1263: training loss 0.388
Epoch 43 iteration 1020/1263: training loss 0.388
Epoch 43 iteration 1040/1263: training loss 0.388
Epoch 43 iteration 1060/1263: training loss 0.387
Epoch 43 iteration 1080/1263: training loss 0.388
Epoch 43 iteration 1100/1263: training loss 0.388
Epoch 43 iteration 1120/1263: training loss 0.389
Epoch 43 iteration 1140/1263: training loss 0.389
Epoch 43 iteration 1160/1263: training loss 0.390
Epoch 43 iteration 1180/1263: training loss 0.390
Epoch 43 iteration 1200/1263: training loss 0.390
Epoch 43 iteration 1220/1263: training loss 0.390
Epoch 43 iteration 1240/1263: training loss 0.391
Epoch 43 iteration 1260/1263: training loss 0.391
Epoch 43 validation pixAcc: 0.784, mIoU: 0.430
Epoch 44 iteration 0020/1263: training loss 0.383
Epoch 44 iteration 0040/1263: training loss 0.383
Epoch 44 iteration 0060/1263: training loss 0.376
Epoch 44 iteration 0080/1263: training loss 0.381
Epoch 44 iteration 0100/1263: training loss 0.391
Epoch 44 iteration 0120/1263: training loss 0.392
Epoch 44 iteration 0140/1263: training loss 0.390
Epoch 44 iteration 0160/1263: training loss 0.391
Epoch 44 iteration 0180/1263: training loss 0.387
Epoch 44 iteration 0200/1263: training loss 0.385
Epoch 44 iteration 0220/1263: training loss 0.387
Epoch 44 iteration 0240/1263: training loss 0.386
Epoch 44 iteration 0260/1263: training loss 0.386
Epoch 44 iteration 0280/1263: training loss 0.386
Epoch 44 iteration 0300/1263: training loss 0.383
Epoch 44 iteration 0320/1263: training loss 0.384
Epoch 44 iteration 0340/1263: training loss 0.383
Epoch 44 iteration 0360/1263: training loss 0.383
Epoch 44 iteration 0380/1263: training loss 0.381
Epoch 44 iteration 0400/1263: training loss 0.381
Epoch 44 iteration 0420/1263: training loss 0.378
Epoch 44 iteration 0440/1263: training loss 0.378
Epoch 44 iteration 0460/1263: training loss 0.378
Epoch 44 iteration 0480/1263: training loss 0.378
Epoch 44 iteration 0500/1263: training loss 0.376
Epoch 44 iteration 0520/1263: training loss 0.376
Epoch 44 iteration 0540/1263: training loss 0.376
Epoch 44 iteration 0560/1263: training loss 0.377
Epoch 44 iteration 0580/1263: training loss 0.376
Epoch 44 iteration 0600/1263: training loss 0.376
Epoch 44 iteration 0620/1263: training loss 0.375
Epoch 44 iteration 0640/1263: training loss 0.375
Epoch 44 iteration 0660/1263: training loss 0.376
Epoch 44 iteration 0680/1263: training loss 0.376
Epoch 44 iteration 0700/1263: training loss 0.376
Epoch 44 iteration 0720/1263: training loss 0.376
Epoch 44 iteration 0740/1263: training loss 0.378
Epoch 44 iteration 0760/1263: training loss 0.378
Epoch 44 iteration 0780/1263: training loss 0.380
Epoch 44 iteration 0800/1263: training loss 0.380
Epoch 44 iteration 0820/1263: training loss 0.381
Epoch 44 iteration 0840/1263: training loss 0.382
Epoch 44 iteration 0860/1263: training loss 0.383
Epoch 44 iteration 0880/1263: training loss 0.384
Epoch 44 iteration 0900/1263: training loss 0.386
Epoch 44 iteration 0920/1263: training loss 0.387
Epoch 44 iteration 0940/1263: training loss 0.388
Epoch 44 iteration 0960/1263: training loss 0.389
Epoch 44 iteration 0980/1263: training loss 0.390
Epoch 44 iteration 1000/1263: training loss 0.390
Epoch 44 iteration 1020/1263: training loss 0.391
Epoch 44 iteration 1040/1263: training loss 0.392
Epoch 44 iteration 1060/1263: training loss 0.395
Epoch 44 iteration 1080/1263: training loss 0.396
Epoch 44 iteration 1100/1263: training loss 0.397
Epoch 44 iteration 1120/1263: training loss 0.397
Epoch 44 iteration 1140/1263: training loss 0.397
Epoch 44 iteration 1160/1263: training loss 0.399
Epoch 44 iteration 1180/1263: training loss 0.400
Epoch 44 iteration 1200/1263: training loss 0.400
Epoch 44 iteration 1220/1263: training loss 0.401
Epoch 44 iteration 1240/1263: training loss 0.401
Epoch 44 iteration 1260/1263: training loss 0.402
Epoch 44 validation pixAcc: 0.787, mIoU: 0.420
Epoch 45 iteration 0020/1263: training loss 0.388
Epoch 45 iteration 0040/1263: training loss 0.377
Epoch 45 iteration 0060/1263: training loss 0.378
Epoch 45 iteration 0080/1263: training loss 0.375
Epoch 45 iteration 0100/1263: training loss 0.379
Epoch 45 iteration 0120/1263: training loss 0.381
Epoch 45 iteration 0140/1263: training loss 0.377
Epoch 45 iteration 0160/1263: training loss 0.375
Epoch 45 iteration 0180/1263: training loss 0.379
Epoch 45 iteration 0200/1263: training loss 0.384
Epoch 45 iteration 0220/1263: training loss 0.387
Epoch 45 iteration 0240/1263: training loss 0.388
Epoch 45 iteration 0260/1263: training loss 0.389
Epoch 45 iteration 0280/1263: training loss 0.388
Epoch 45 iteration 0300/1263: training loss 0.386
Epoch 45 iteration 0320/1263: training loss 0.385
Epoch 45 iteration 0340/1263: training loss 0.383
Epoch 45 iteration 0360/1263: training loss 0.381
Epoch 45 iteration 0380/1263: training loss 0.381
Epoch 45 iteration 0400/1263: training loss 0.381
Epoch 45 iteration 0420/1263: training loss 0.379
Epoch 45 iteration 0440/1263: training loss 0.379
Epoch 45 iteration 0460/1263: training loss 0.379
Epoch 45 iteration 0480/1263: training loss 0.380
Epoch 45 iteration 0500/1263: training loss 0.381
Epoch 45 iteration 0520/1263: training loss 0.382
Epoch 45 iteration 0540/1263: training loss 0.384
Epoch 45 iteration 0560/1263: training loss 0.384
Epoch 45 iteration 0580/1263: training loss 0.384
Epoch 45 iteration 0600/1263: training loss 0.385
Epoch 45 iteration 0620/1263: training loss 0.387
Epoch 45 iteration 0640/1263: training loss 0.388
Epoch 45 iteration 0660/1263: training loss 0.388
Epoch 45 iteration 0680/1263: training loss 0.387
Epoch 45 iteration 0700/1263: training loss 0.387
Epoch 45 iteration 0720/1263: training loss 0.389
Epoch 45 iteration 0740/1263: training loss 0.390
Epoch 45 iteration 0760/1263: training loss 0.390
Epoch 45 iteration 0780/1263: training loss 0.392
Epoch 45 iteration 0800/1263: training loss 0.392
Epoch 45 iteration 0820/1263: training loss 0.392
Epoch 45 iteration 0840/1263: training loss 0.394
Epoch 45 iteration 0860/1263: training loss 0.396
Epoch 45 iteration 0880/1263: training loss 0.397
Epoch 45 iteration 0900/1263: training loss 0.397
Epoch 45 iteration 0920/1263: training loss 0.397
Epoch 45 iteration 0940/1263: training loss 0.397
Epoch 45 iteration 0960/1263: training loss 0.397
Epoch 45 iteration 0980/1263: training loss 0.397
Epoch 45 iteration 1000/1263: training loss 0.397
Epoch 45 iteration 1020/1263: training loss 0.397
Epoch 45 iteration 1040/1263: training loss 0.398
Epoch 45 iteration 1060/1263: training loss 0.399
Epoch 45 iteration 1080/1263: training loss 0.399
Epoch 45 iteration 1100/1263: training loss 0.399
Epoch 45 iteration 1120/1263: training loss 0.401
Epoch 45 iteration 1140/1263: training loss 0.402
Epoch 45 iteration 1160/1263: training loss 0.403
Epoch 45 iteration 1180/1263: training loss 0.403
Epoch 45 iteration 1200/1263: training loss 0.402
Epoch 45 iteration 1220/1263: training loss 0.402
Epoch 45 iteration 1240/1263: training loss 0.401
Epoch 45 iteration 1260/1263: training loss 0.401
Epoch 45 validation pixAcc: 0.793, mIoU: 0.433
Epoch 46 iteration 0020/1263: training loss 0.350
Epoch 46 iteration 0040/1263: training loss 0.387
Epoch 46 iteration 0060/1263: training loss 0.415
Epoch 46 iteration 0080/1263: training loss 0.417
Epoch 46 iteration 0100/1263: training loss 0.410
Epoch 46 iteration 0120/1263: training loss 0.413
Epoch 46 iteration 0140/1263: training loss 0.416
Epoch 46 iteration 0160/1263: training loss 0.416
Epoch 46 iteration 0180/1263: training loss 0.414
Epoch 46 iteration 0200/1263: training loss 0.416
Epoch 46 iteration 0220/1263: training loss 0.413
Epoch 46 iteration 0240/1263: training loss 0.410
Epoch 46 iteration 0260/1263: training loss 0.409
Epoch 46 iteration 0280/1263: training loss 0.412
Epoch 46 iteration 0300/1263: training loss 0.413
Epoch 46 iteration 0320/1263: training loss 0.414
Epoch 46 iteration 0340/1263: training loss 0.412
Epoch 46 iteration 0360/1263: training loss 0.412
Epoch 46 iteration 0380/1263: training loss 0.408
Epoch 46 iteration 0400/1263: training loss 0.407
Epoch 46 iteration 0420/1263: training loss 0.406
Epoch 46 iteration 0440/1263: training loss 0.406
Epoch 46 iteration 0460/1263: training loss 0.405
Epoch 46 iteration 0480/1263: training loss 0.405
Epoch 46 iteration 0500/1263: training loss 0.406
Epoch 46 iteration 0520/1263: training loss 0.406
Epoch 46 iteration 0540/1263: training loss 0.406
Epoch 46 iteration 0560/1263: training loss 0.406
Epoch 46 iteration 0580/1263: training loss 0.407
Epoch 46 iteration 0600/1263: training loss 0.407
Epoch 46 iteration 0620/1263: training loss 0.406
Epoch 46 iteration 0640/1263: training loss 0.407
Epoch 46 iteration 0660/1263: training loss 0.411
Epoch 46 iteration 0680/1263: training loss 0.410
Epoch 46 iteration 0700/1263: training loss 0.410
Epoch 46 iteration 0720/1263: training loss 0.410
Epoch 46 iteration 0740/1263: training loss 0.411
Epoch 46 iteration 0760/1263: training loss 0.411
Epoch 46 iteration 0780/1263: training loss 0.410
Epoch 46 iteration 0800/1263: training loss 0.411
Epoch 46 iteration 0820/1263: training loss 0.411
Epoch 46 iteration 0840/1263: training loss 0.412
Epoch 46 iteration 0860/1263: training loss 0.412
Epoch 46 iteration 0880/1263: training loss 0.412
Epoch 46 iteration 0900/1263: training loss 0.413
Epoch 46 iteration 0920/1263: training loss 0.414
Epoch 46 iteration 0940/1263: training loss 0.416
Epoch 46 iteration 0960/1263: training loss 0.417
Epoch 46 iteration 0980/1263: training loss 0.417
Epoch 46 iteration 1000/1263: training loss 0.417
Epoch 46 iteration 1020/1263: training loss 0.418
Epoch 46 iteration 1040/1263: training loss 0.418
Epoch 46 iteration 1060/1263: training loss 0.418
Epoch 46 iteration 1080/1263: training loss 0.418
Epoch 46 iteration 1100/1263: training loss 0.418
Epoch 46 iteration 1120/1263: training loss 0.418
Epoch 46 iteration 1140/1263: training loss 0.418
Epoch 46 iteration 1160/1263: training loss 0.419
Epoch 46 iteration 1180/1264: training loss 0.419
Epoch 46 iteration 1200/1264: training loss 0.419
Epoch 46 iteration 1220/1264: training loss 0.419
Epoch 46 iteration 1240/1264: training loss 0.419
Epoch 46 iteration 1260/1264: training loss 0.420
Epoch 46 validation pixAcc: 0.789, mIoU: 0.437
Epoch 47 iteration 0020/1263: training loss 0.400
Epoch 47 iteration 0040/1263: training loss 0.394
Epoch 47 iteration 0060/1263: training loss 0.382
Epoch 47 iteration 0080/1263: training loss 0.382
Epoch 47 iteration 0100/1263: training loss 0.383
Epoch 47 iteration 0120/1263: training loss 0.388
Epoch 47 iteration 0140/1263: training loss 0.388
Epoch 47 iteration 0160/1263: training loss 0.394
Epoch 47 iteration 0180/1263: training loss 0.405
Epoch 47 iteration 0200/1263: training loss 0.409
Epoch 47 iteration 0220/1263: training loss 0.411
Epoch 47 iteration 0240/1263: training loss 0.419
Epoch 47 iteration 0260/1263: training loss 0.422
Epoch 47 iteration 0280/1263: training loss 0.422
Epoch 47 iteration 0300/1263: training loss 0.422
Epoch 47 iteration 0320/1263: training loss 0.423
Epoch 47 iteration 0340/1263: training loss 0.422
Epoch 47 iteration 0360/1263: training loss 0.422
Epoch 47 iteration 0380/1263: training loss 0.421
Epoch 47 iteration 0400/1263: training loss 0.421
Epoch 47 iteration 0420/1263: training loss 0.422
Epoch 47 iteration 0440/1263: training loss 0.421
Epoch 47 iteration 0460/1263: training loss 0.422
Epoch 47 iteration 0480/1263: training loss 0.422
Epoch 47 iteration 0500/1263: training loss 0.422
Epoch 47 iteration 0520/1263: training loss 0.422
Epoch 47 iteration 0540/1263: training loss 0.423
Epoch 47 iteration 0560/1263: training loss 0.423
Epoch 47 iteration 0580/1263: training loss 0.424
Epoch 47 iteration 0600/1263: training loss 0.425
Epoch 47 iteration 0620/1263: training loss 0.427
Epoch 47 iteration 0640/1263: training loss 0.427
Epoch 47 iteration 0660/1263: training loss 0.427
Epoch 47 iteration 0680/1263: training loss 0.427
Epoch 47 iteration 0700/1263: training loss 0.427
Epoch 47 iteration 0720/1263: training loss 0.427
Epoch 47 iteration 0740/1263: training loss 0.428
Epoch 47 iteration 0760/1263: training loss 0.428
Epoch 47 iteration 0780/1263: training loss 0.428
Epoch 47 iteration 0800/1263: training loss 0.428
Epoch 47 iteration 0820/1263: training loss 0.428
Epoch 47 iteration 0840/1263: training loss 0.428
Epoch 47 iteration 0860/1263: training loss 0.428
Epoch 47 iteration 0880/1263: training loss 0.429
Epoch 47 iteration 0900/1263: training loss 0.429
Epoch 47 iteration 0920/1263: training loss 0.429
Epoch 47 iteration 0940/1263: training loss 0.430
Epoch 47 iteration 0960/1263: training loss 0.429
Epoch 47 iteration 0980/1263: training loss 0.428
Epoch 47 iteration 1000/1263: training loss 0.430
Epoch 47 iteration 1020/1263: training loss 0.432
Epoch 47 iteration 1040/1263: training loss 0.432
Epoch 47 iteration 1060/1263: training loss 0.432
Epoch 47 iteration 1080/1263: training loss 0.434
Epoch 47 iteration 1100/1263: training loss 0.435
Epoch 47 iteration 1120/1263: training loss 0.435
Epoch 47 iteration 1140/1263: training loss 0.436
Epoch 47 iteration 1160/1263: training loss 0.435
Epoch 47 iteration 1180/1263: training loss 0.436
Epoch 47 iteration 1200/1263: training loss 0.436
Epoch 47 iteration 1220/1263: training loss 0.436
Epoch 47 iteration 1240/1263: training loss 0.437
Epoch 47 iteration 1260/1263: training loss 0.437
Epoch 47 validation pixAcc: 0.786, mIoU: 0.432
Epoch 48 iteration 0020/1263: training loss 0.461
Epoch 48 iteration 0040/1263: training loss 0.464
Epoch 48 iteration 0060/1263: training loss 0.439
Epoch 48 iteration 0080/1263: training loss 0.439
Epoch 48 iteration 0100/1263: training loss 0.434
Epoch 48 iteration 0120/1263: training loss 0.430
Epoch 48 iteration 0140/1263: training loss 0.423
Epoch 48 iteration 0160/1263: training loss 0.424
Epoch 48 iteration 0180/1263: training loss 0.422
Epoch 48 iteration 0200/1263: training loss 0.419
Epoch 48 iteration 0220/1263: training loss 0.416
Epoch 48 iteration 0240/1263: training loss 0.413
Epoch 48 iteration 0260/1263: training loss 0.409
Epoch 48 iteration 0280/1263: training loss 0.409
Epoch 48 iteration 0300/1263: training loss 0.409
Epoch 48 iteration 0320/1263: training loss 0.408
Epoch 48 iteration 0340/1263: training loss 0.407
Epoch 48 iteration 0360/1263: training loss 0.406
Epoch 48 iteration 0380/1263: training loss 0.405
Epoch 48 iteration 0400/1263: training loss 0.405
Epoch 48 iteration 0420/1263: training loss 0.407
Epoch 48 iteration 0440/1263: training loss 0.407
Epoch 48 iteration 0460/1263: training loss 0.406
Epoch 48 iteration 0480/1263: training loss 0.407
Epoch 48 iteration 0500/1263: training loss 0.407
Epoch 48 iteration 0520/1263: training loss 0.407
Epoch 48 iteration 0540/1263: training loss 0.408
Epoch 48 iteration 0560/1263: training loss 0.407
Epoch 48 iteration 0580/1263: training loss 0.405
Epoch 48 iteration 0600/1263: training loss 0.404
Epoch 48 iteration 0620/1263: training loss 0.404
Epoch 48 iteration 0640/1263: training loss 0.404
Epoch 48 iteration 0660/1263: training loss 0.404
Epoch 48 iteration 0680/1263: training loss 0.402
Epoch 48 iteration 0700/1263: training loss 0.401
Epoch 48 iteration 0720/1263: training loss 0.401
Epoch 48 iteration 0740/1263: training loss 0.401
Epoch 48 iteration 0760/1263: training loss 0.400
Epoch 48 iteration 0780/1263: training loss 0.399
Epoch 48 iteration 0800/1263: training loss 0.399
Epoch 48 iteration 0820/1263: training loss 0.399
Epoch 48 iteration 0840/1263: training loss 0.398
Epoch 48 iteration 0860/1263: training loss 0.397
Epoch 48 iteration 0880/1263: training loss 0.397
Epoch 48 iteration 0900/1263: training loss 0.396
Epoch 48 iteration 0920/1263: training loss 0.395
Epoch 48 iteration 0940/1263: training loss 0.395
Epoch 48 iteration 0960/1263: training loss 0.395
Epoch 48 iteration 0980/1263: training loss 0.394
Epoch 48 iteration 1000/1263: training loss 0.395
Epoch 48 iteration 1020/1263: training loss 0.395
Epoch 48 iteration 1040/1263: training loss 0.395
Epoch 48 iteration 1060/1263: training loss 0.395
Epoch 48 iteration 1080/1263: training loss 0.394
Epoch 48 iteration 1100/1263: training loss 0.394
Epoch 48 iteration 1120/1263: training loss 0.393
Epoch 48 iteration 1140/1263: training loss 0.393
Epoch 48 iteration 1160/1263: training loss 0.393
Epoch 48 iteration 1180/1263: training loss 0.392
Epoch 48 iteration 1200/1263: training loss 0.392
Epoch 48 iteration 1220/1263: training loss 0.391
Epoch 48 iteration 1240/1263: training loss 0.391
Epoch 48 iteration 1260/1263: training loss 0.391
Epoch 48 validation pixAcc: 0.798, mIoU: 0.450
Epoch 49 iteration 0020/1263: training loss 0.388
Epoch 49 iteration 0040/1263: training loss 0.379
Epoch 49 iteration 0060/1263: training loss 0.377
Epoch 49 iteration 0080/1263: training loss 0.377
Epoch 49 iteration 0100/1263: training loss 0.380
Epoch 49 iteration 0120/1263: training loss 0.377
Epoch 49 iteration 0140/1263: training loss 0.372
Epoch 49 iteration 0160/1263: training loss 0.370
Epoch 49 iteration 0180/1263: training loss 0.375
Epoch 49 iteration 0200/1263: training loss 0.375
Epoch 49 iteration 0220/1263: training loss 0.373
Epoch 49 iteration 0240/1263: training loss 0.370
Epoch 49 iteration 0260/1263: training loss 0.368
Epoch 49 iteration 0280/1263: training loss 0.368
Epoch 49 iteration 0300/1263: training loss 0.368
Epoch 49 iteration 0320/1263: training loss 0.367
Epoch 49 iteration 0340/1263: training loss 0.368
Epoch 49 iteration 0360/1263: training loss 0.368
Epoch 49 iteration 0380/1263: training loss 0.368
Epoch 49 iteration 0400/1263: training loss 0.370
Epoch 49 iteration 0420/1263: training loss 0.369
Epoch 49 iteration 0440/1263: training loss 0.368
Epoch 49 iteration 0460/1263: training loss 0.367
Epoch 49 iteration 0480/1263: training loss 0.365
Epoch 49 iteration 0500/1263: training loss 0.365
Epoch 49 iteration 0520/1263: training loss 0.366
Epoch 49 iteration 0540/1263: training loss 0.366
Epoch 49 iteration 0560/1263: training loss 0.366
Epoch 49 iteration 0580/1263: training loss 0.364
Epoch 49 iteration 0600/1263: training loss 0.365
Epoch 49 iteration 0620/1263: training loss 0.367
Epoch 49 iteration 0640/1263: training loss 0.366
Epoch 49 iteration 0660/1263: training loss 0.366
Epoch 49 iteration 0680/1263: training loss 0.367
Epoch 49 iteration 0700/1263: training loss 0.366
Epoch 49 iteration 0720/1263: training loss 0.366
Epoch 49 iteration 0740/1263: training loss 0.367
Epoch 49 iteration 0760/1263: training loss 0.366
Epoch 49 iteration 0780/1263: training loss 0.365
Epoch 49 iteration 0800/1263: training loss 0.366
Epoch 49 iteration 0820/1263: training loss 0.365
Epoch 49 iteration 0840/1263: training loss 0.366
Epoch 49 iteration 0860/1263: training loss 0.365
Epoch 49 iteration 0880/1263: training loss 0.366
Epoch 49 iteration 0900/1263: training loss 0.366
Epoch 49 iteration 0920/1263: training loss 0.366
Epoch 49 iteration 0940/1263: training loss 0.365
Epoch 49 iteration 0960/1263: training loss 0.365
Epoch 49 iteration 0980/1263: training loss 0.366
Epoch 49 iteration 1000/1263: training loss 0.367
Epoch 49 iteration 1020/1263: training loss 0.367
Epoch 49 iteration 1040/1263: training loss 0.367
Epoch 49 iteration 1060/1263: training loss 0.367
Epoch 49 iteration 1080/1263: training loss 0.367
Epoch 49 iteration 1100/1263: training loss 0.367
Epoch 49 iteration 1120/1263: training loss 0.366
Epoch 49 iteration 1140/1263: training loss 0.367
Epoch 49 iteration 1160/1263: training loss 0.366
Epoch 49 iteration 1180/1263: training loss 0.366
Epoch 49 iteration 1200/1263: training loss 0.366
Epoch 49 iteration 1220/1263: training loss 0.366
Epoch 49 iteration 1240/1263: training loss 0.366
Epoch 49 iteration 1260/1263: training loss 0.366
Epoch 49 validation pixAcc: 0.798, mIoU: 0.448
Epoch 50 iteration 0020/1263: training loss 0.385
Epoch 50 iteration 0040/1263: training loss 0.416
Epoch 50 iteration 0060/1263: training loss 0.438
Epoch 50 iteration 0080/1263: training loss 0.429
Epoch 50 iteration 0100/1263: training loss 0.431
Epoch 50 iteration 0120/1263: training loss 0.434
Epoch 50 iteration 0140/1263: training loss 0.430
Epoch 50 iteration 0160/1263: training loss 0.424
Epoch 50 iteration 0180/1263: training loss 0.422
Epoch 50 iteration 0200/1263: training loss 0.415
Epoch 50 iteration 0220/1263: training loss 0.411
Epoch 50 iteration 0240/1263: training loss 0.405
Epoch 50 iteration 0260/1263: training loss 0.402
Epoch 50 iteration 0280/1263: training loss 0.400
Epoch 50 iteration 0300/1263: training loss 0.405
Epoch 50 iteration 0320/1263: training loss 0.404
Epoch 50 iteration 0340/1263: training loss 0.407
Epoch 50 iteration 0360/1263: training loss 0.407
Epoch 50 iteration 0380/1263: training loss 0.407
Epoch 50 iteration 0400/1263: training loss 0.407
Epoch 50 iteration 0420/1263: training loss 0.406
Epoch 50 iteration 0440/1263: training loss 0.408
Epoch 50 iteration 0460/1263: training loss 0.407
Epoch 50 iteration 0480/1263: training loss 0.407
Epoch 50 iteration 0500/1263: training loss 0.407
Epoch 50 iteration 0520/1263: training loss 0.407
Epoch 50 iteration 0540/1263: training loss 0.407
Epoch 50 iteration 0560/1263: training loss 0.406
Epoch 50 iteration 0580/1263: training loss 0.405
Epoch 50 iteration 0600/1263: training loss 0.404
Epoch 50 iteration 0620/1263: training loss 0.403
Epoch 50 iteration 0640/1263: training loss 0.402
Epoch 50 iteration 0660/1263: training loss 0.401
Epoch 50 iteration 0680/1263: training loss 0.401
Epoch 50 iteration 0700/1263: training loss 0.400
Epoch 50 iteration 0720/1263: training loss 0.400
Epoch 50 iteration 0740/1263: training loss 0.400
Epoch 50 iteration 0760/1263: training loss 0.398
Epoch 50 iteration 0780/1263: training loss 0.396
Epoch 50 iteration 0800/1263: training loss 0.395
Epoch 50 iteration 0820/1263: training loss 0.394
Epoch 50 iteration 0840/1263: training loss 0.394
Epoch 50 iteration 0860/1263: training loss 0.394
Epoch 50 iteration 0880/1263: training loss 0.393
Epoch 50 iteration 0900/1263: training loss 0.392
Epoch 50 iteration 0920/1263: training loss 0.392
Epoch 50 iteration 0940/1263: training loss 0.392
Epoch 50 iteration 0960/1263: training loss 0.391
Epoch 50 iteration 0980/1263: training loss 0.391
Epoch 50 iteration 1000/1263: training loss 0.393
Epoch 50 iteration 1020/1263: training loss 0.393
Epoch 50 iteration 1040/1263: training loss 0.393
Epoch 50 iteration 1060/1263: training loss 0.392
Epoch 50 iteration 1080/1263: training loss 0.393
Epoch 50 iteration 1100/1263: training loss 0.393
Epoch 50 iteration 1120/1263: training loss 0.393
Epoch 50 iteration 1140/1263: training loss 0.392
Epoch 50 iteration 1160/1263: training loss 0.392
Epoch 50 iteration 1180/1263: training loss 0.392
Epoch 50 iteration 1200/1263: training loss 0.392
Epoch 50 iteration 1220/1263: training loss 0.392
Epoch 50 iteration 1240/1263: training loss 0.392
Epoch 50 iteration 1260/1263: training loss 0.392
Epoch 50 validation pixAcc: 0.794, mIoU: 0.439
Epoch 51 iteration 0020/1263: training loss 0.365
Epoch 51 iteration 0040/1263: training loss 0.348
Epoch 51 iteration 0060/1263: training loss 0.356
Epoch 51 iteration 0080/1263: training loss 0.364
Epoch 51 iteration 0100/1263: training loss 0.365
Epoch 51 iteration 0120/1263: training loss 0.365
Epoch 51 iteration 0140/1263: training loss 0.363
Epoch 51 iteration 0160/1263: training loss 0.363
Epoch 51 iteration 0180/1263: training loss 0.361
Epoch 51 iteration 0200/1263: training loss 0.363
Epoch 51 iteration 0220/1263: training loss 0.366
Epoch 51 iteration 0240/1263: training loss 0.367
Epoch 51 iteration 0260/1263: training loss 0.365
Epoch 51 iteration 0280/1263: training loss 0.364
Epoch 51 iteration 0300/1263: training loss 0.365
Epoch 51 iteration 0320/1263: training loss 0.364
Epoch 51 iteration 0340/1263: training loss 0.364
Epoch 51 iteration 0360/1263: training loss 0.364
Epoch 51 iteration 0380/1263: training loss 0.362
Epoch 51 iteration 0400/1263: training loss 0.362
Epoch 51 iteration 0420/1263: training loss 0.361
Epoch 51 iteration 0440/1263: training loss 0.360
Epoch 51 iteration 0460/1263: training loss 0.361
Epoch 51 iteration 0480/1263: training loss 0.360
Epoch 51 iteration 0500/1263: training loss 0.359
Epoch 51 iteration 0520/1263: training loss 0.358
Epoch 51 iteration 0540/1263: training loss 0.359
Epoch 51 iteration 0560/1263: training loss 0.358
Epoch 51 iteration 0580/1263: training loss 0.358
Epoch 51 iteration 0600/1263: training loss 0.357
Epoch 51 iteration 0620/1263: training loss 0.357
Epoch 51 iteration 0640/1263: training loss 0.357
Epoch 51 iteration 0660/1263: training loss 0.356
Epoch 51 iteration 0680/1263: training loss 0.356
Epoch 51 iteration 0700/1263: training loss 0.357
Epoch 51 iteration 0720/1263: training loss 0.357
Epoch 51 iteration 0740/1263: training loss 0.358
Epoch 51 iteration 0760/1263: training loss 0.358
Epoch 51 iteration 0780/1263: training loss 0.357
Epoch 51 iteration 0800/1263: training loss 0.357
Epoch 51 iteration 0820/1263: training loss 0.358
Epoch 51 iteration 0840/1263: training loss 0.357
Epoch 51 iteration 0860/1263: training loss 0.357
Epoch 51 iteration 0880/1263: training loss 0.357
Epoch 51 iteration 0900/1263: training loss 0.358
Epoch 51 iteration 0920/1263: training loss 0.358
Epoch 51 iteration 0940/1263: training loss 0.359
Epoch 51 iteration 0960/1263: training loss 0.359
Epoch 51 iteration 0980/1263: training loss 0.359
Epoch 51 iteration 1000/1263: training loss 0.359
Epoch 51 iteration 1020/1263: training loss 0.359
Epoch 51 iteration 1040/1263: training loss 0.360
Epoch 51 iteration 1060/1263: training loss 0.361
Epoch 51 iteration 1080/1263: training loss 0.361
Epoch 51 iteration 1100/1263: training loss 0.360
Epoch 51 iteration 1120/1263: training loss 0.360
Epoch 51 iteration 1140/1263: training loss 0.361
Epoch 51 iteration 1160/1263: training loss 0.361
Epoch 51 iteration 1180/1263: training loss 0.361
Epoch 51 iteration 1200/1263: training loss 0.362
Epoch 51 iteration 1220/1263: training loss 0.361
Epoch 51 iteration 1240/1263: training loss 0.361
Epoch 51 iteration 1260/1263: training loss 0.361
Epoch 51 validation pixAcc: 0.795, mIoU: 0.448
Epoch 52 iteration 0020/1263: training loss 0.385
Epoch 52 iteration 0040/1263: training loss 0.368
Epoch 52 iteration 0060/1263: training loss 0.369
Epoch 52 iteration 0080/1263: training loss 0.361
Epoch 52 iteration 0100/1263: training loss 0.359
Epoch 52 iteration 0120/1263: training loss 0.356
Epoch 52 iteration 0140/1263: training loss 0.359
Epoch 52 iteration 0160/1263: training loss 0.357
Epoch 52 iteration 0180/1263: training loss 0.358
Epoch 52 iteration 0200/1263: training loss 0.360
Epoch 52 iteration 0220/1263: training loss 0.363
Epoch 52 iteration 0240/1263: training loss 0.365
Epoch 52 iteration 0260/1263: training loss 0.366
Epoch 52 iteration 0280/1263: training loss 0.364
Epoch 52 iteration 0300/1263: training loss 0.366
Epoch 52 iteration 0320/1263: training loss 0.366
Epoch 52 iteration 0340/1263: training loss 0.365
Epoch 52 iteration 0360/1263: training loss 0.364
Epoch 52 iteration 0380/1263: training loss 0.366
Epoch 52 iteration 0400/1263: training loss 0.367
Epoch 52 iteration 0420/1263: training loss 0.371
Epoch 52 iteration 0440/1263: training loss 0.371
Epoch 52 iteration 0460/1263: training loss 0.371
Epoch 52 iteration 0480/1263: training loss 0.372
Epoch 52 iteration 0500/1263: training loss 0.372
Epoch 52 iteration 0520/1263: training loss 0.371
Epoch 52 iteration 0540/1263: training loss 0.370
Epoch 52 iteration 0560/1263: training loss 0.369
Epoch 52 iteration 0580/1263: training loss 0.368
Epoch 52 iteration 0600/1263: training loss 0.367
Epoch 52 iteration 0620/1263: training loss 0.367
Epoch 52 iteration 0640/1263: training loss 0.367
Epoch 52 iteration 0660/1263: training loss 0.366
Epoch 52 iteration 0680/1263: training loss 0.364
Epoch 52 iteration 0700/1263: training loss 0.364
Epoch 52 iteration 0720/1263: training loss 0.365
Epoch 52 iteration 0740/1263: training loss 0.365
Epoch 52 iteration 0760/1263: training loss 0.365
Epoch 52 iteration 0780/1263: training loss 0.365
Epoch 52 iteration 0800/1263: training loss 0.364
Epoch 52 iteration 0820/1263: training loss 0.365
Epoch 52 iteration 0840/1263: training loss 0.366
Epoch 52 iteration 0860/1263: training loss 0.367
Epoch 52 iteration 0880/1263: training loss 0.369
Epoch 52 iteration 0900/1263: training loss 0.368
Epoch 52 iteration 0920/1263: training loss 0.369
Epoch 52 iteration 0940/1263: training loss 0.368
Epoch 52 iteration 0960/1263: training loss 0.368
Epoch 52 iteration 0980/1263: training loss 0.367
Epoch 52 iteration 1000/1263: training loss 0.367
Epoch 52 iteration 1020/1263: training loss 0.367
Epoch 52 iteration 1040/1263: training loss 0.367
Epoch 52 iteration 1060/1263: training loss 0.367
Epoch 52 iteration 1080/1263: training loss 0.366
Epoch 52 iteration 1100/1263: training loss 0.367
Epoch 52 iteration 1120/1263: training loss 0.367
Epoch 52 iteration 1140/1263: training loss 0.367
Epoch 52 iteration 1160/1263: training loss 0.368
Epoch 52 iteration 1180/1263: training loss 0.368
Epoch 52 iteration 1200/1263: training loss 0.368
Epoch 52 iteration 1220/1263: training loss 0.368
Epoch 52 iteration 1240/1263: training loss 0.367
Epoch 52 iteration 1260/1263: training loss 0.367
Epoch 52 validation pixAcc: 0.792, mIoU: 0.439
Epoch 53 iteration 0020/1263: training loss 0.352
Epoch 53 iteration 0040/1263: training loss 0.347
Epoch 53 iteration 0060/1263: training loss 0.341
Epoch 53 iteration 0080/1263: training loss 0.341
Epoch 53 iteration 0100/1263: training loss 0.344
Epoch 53 iteration 0120/1263: training loss 0.348
Epoch 53 iteration 0140/1263: training loss 0.348
Epoch 53 iteration 0160/1263: training loss 0.350
Epoch 53 iteration 0180/1263: training loss 0.351
Epoch 53 iteration 0200/1263: training loss 0.353
Epoch 53 iteration 0220/1263: training loss 0.355
Epoch 53 iteration 0240/1263: training loss 0.353
Epoch 53 iteration 0260/1263: training loss 0.355
Epoch 53 iteration 0280/1263: training loss 0.354
Epoch 53 iteration 0300/1263: training loss 0.353
Epoch 53 iteration 0320/1263: training loss 0.349
Epoch 53 iteration 0340/1263: training loss 0.347
Epoch 53 iteration 0360/1263: training loss 0.348
Epoch 53 iteration 0380/1263: training loss 0.347
Epoch 53 iteration 0400/1263: training loss 0.346
Epoch 53 iteration 0420/1263: training loss 0.344
Epoch 53 iteration 0440/1263: training loss 0.344
Epoch 53 iteration 0460/1263: training loss 0.344
Epoch 53 iteration 0480/1263: training loss 0.344
Epoch 53 iteration 0500/1263: training loss 0.345
Epoch 53 iteration 0520/1263: training loss 0.344
Epoch 53 iteration 0540/1263: training loss 0.344
Epoch 53 iteration 0560/1263: training loss 0.343
Epoch 53 iteration 0580/1263: training loss 0.342
Epoch 53 iteration 0600/1263: training loss 0.342
Epoch 53 iteration 0620/1263: training loss 0.342
Epoch 53 iteration 0640/1263: training loss 0.342
Epoch 53 iteration 0660/1263: training loss 0.342
Epoch 53 iteration 0680/1263: training loss 0.341
Epoch 53 iteration 0700/1263: training loss 0.341
Epoch 53 iteration 0720/1263: training loss 0.342
Epoch 53 iteration 0740/1263: training loss 0.342
Epoch 53 iteration 0760/1263: training loss 0.342
Epoch 53 iteration 0780/1263: training loss 0.342
Epoch 53 iteration 0800/1263: training loss 0.342
Epoch 53 iteration 0820/1263: training loss 0.342
Epoch 53 iteration 0840/1263: training loss 0.342
Epoch 53 iteration 0860/1263: training loss 0.341
Epoch 53 iteration 0880/1263: training loss 0.341
Epoch 53 iteration 0900/1263: training loss 0.340
Epoch 53 iteration 0920/1263: training loss 0.339
Epoch 53 iteration 0940/1263: training loss 0.339
Epoch 53 iteration 0960/1263: training loss 0.338
Epoch 53 iteration 0980/1263: training loss 0.339
Epoch 53 iteration 1000/1263: training loss 0.338
Epoch 53 iteration 1020/1263: training loss 0.338
Epoch 53 iteration 1040/1263: training loss 0.338
Epoch 53 iteration 1060/1263: training loss 0.339
Epoch 53 iteration 1080/1263: training loss 0.338
Epoch 53 iteration 1100/1263: training loss 0.340
Epoch 53 iteration 1120/1263: training loss 0.340
Epoch 53 iteration 1140/1263: training loss 0.340
Epoch 53 iteration 1160/1263: training loss 0.340
Epoch 53 iteration 1180/1263: training loss 0.341
Epoch 53 iteration 1200/1263: training loss 0.342
Epoch 53 iteration 1220/1263: training loss 0.343
Epoch 53 iteration 1240/1263: training loss 0.344
Epoch 53 iteration 1260/1263: training loss 0.345
Epoch 53 validation pixAcc: 0.786, mIoU: 0.422
Epoch 54 iteration 0020/1263: training loss 0.366
Epoch 54 iteration 0040/1263: training loss 0.364
Epoch 54 iteration 0060/1263: training loss 0.363
Epoch 54 iteration 0080/1263: training loss 0.357
Epoch 54 iteration 0100/1263: training loss 0.357
Epoch 54 iteration 0120/1263: training loss 0.360
Epoch 54 iteration 0140/1263: training loss 0.356
Epoch 54 iteration 0160/1263: training loss 0.350
Epoch 54 iteration 0180/1263: training loss 0.345
Epoch 54 iteration 0200/1263: training loss 0.344
Epoch 54 iteration 0220/1263: training loss 0.340
Epoch 54 iteration 0240/1263: training loss 0.343
Epoch 54 iteration 0260/1263: training loss 0.346
Epoch 54 iteration 0280/1263: training loss 0.348
Epoch 54 iteration 0300/1263: training loss 0.352
Epoch 54 iteration 0320/1263: training loss 0.352
Epoch 54 iteration 0340/1263: training loss 0.352
Epoch 54 iteration 0360/1263: training loss 0.350
Epoch 54 iteration 0380/1263: training loss 0.349
Epoch 54 iteration 0400/1263: training loss 0.348
Epoch 54 iteration 0420/1263: training loss 0.347
Epoch 54 iteration 0440/1263: training loss 0.346
Epoch 54 iteration 0460/1263: training loss 0.344
Epoch 54 iteration 0480/1263: training loss 0.344
Epoch 54 iteration 0500/1263: training loss 0.342
Epoch 54 iteration 0520/1263: training loss 0.343
Epoch 54 iteration 0540/1263: training loss 0.344
Epoch 54 iteration 0560/1263: training loss 0.345
Epoch 54 iteration 0580/1263: training loss 0.346
Epoch 54 iteration 0600/1263: training loss 0.345
Epoch 54 iteration 0620/1263: training loss 0.344
Epoch 54 iteration 0640/1263: training loss 0.345
Epoch 54 iteration 0660/1263: training loss 0.344
Epoch 54 iteration 0680/1263: training loss 0.344
Epoch 54 iteration 0700/1263: training loss 0.344
Epoch 54 iteration 0720/1263: training loss 0.344
Epoch 54 iteration 0740/1263: training loss 0.345
Epoch 54 iteration 0760/1263: training loss 0.346
Epoch 54 iteration 0780/1263: training loss 0.346
Epoch 54 iteration 0800/1263: training loss 0.346
Epoch 54 iteration 0820/1263: training loss 0.345
Epoch 54 iteration 0840/1263: training loss 0.345
Epoch 54 iteration 0860/1263: training loss 0.345
Epoch 54 iteration 0880/1263: training loss 0.345
Epoch 54 iteration 0900/1263: training loss 0.344
Epoch 54 iteration 0920/1263: training loss 0.344
Epoch 54 iteration 0940/1263: training loss 0.343
Epoch 54 iteration 0960/1263: training loss 0.343
Epoch 54 iteration 0980/1263: training loss 0.343
Epoch 54 iteration 1000/1263: training loss 0.342
Epoch 54 iteration 1020/1263: training loss 0.343
Epoch 54 iteration 1040/1263: training loss 0.342
Epoch 54 iteration 1060/1263: training loss 0.342
Epoch 54 iteration 1080/1263: training loss 0.341
Epoch 54 iteration 1100/1263: training loss 0.341
Epoch 54 iteration 1120/1263: training loss 0.342
Epoch 54 iteration 1140/1263: training loss 0.342
Epoch 54 iteration 1160/1263: training loss 0.342
Epoch 54 iteration 1180/1264: training loss 0.342
Epoch 54 iteration 1200/1264: training loss 0.342
Epoch 54 iteration 1220/1264: training loss 0.342
Epoch 54 iteration 1240/1264: training loss 0.342
Epoch 54 iteration 1260/1264: training loss 0.342
Epoch 54 validation pixAcc: 0.796, mIoU: 0.437
Epoch 55 iteration 0020/1263: training loss 0.315
Epoch 55 iteration 0040/1263: training loss 0.311
Epoch 55 iteration 0060/1263: training loss 0.313
Epoch 55 iteration 0080/1263: training loss 0.308
Epoch 55 iteration 0100/1263: training loss 0.307
Epoch 55 iteration 0120/1263: training loss 0.309
Epoch 55 iteration 0140/1263: training loss 0.308
Epoch 55 iteration 0160/1263: training loss 0.310
Epoch 55 iteration 0180/1263: training loss 0.311
Epoch 55 iteration 0200/1263: training loss 0.314
Epoch 55 iteration 0220/1263: training loss 0.319
Epoch 55 iteration 0240/1263: training loss 0.321
Epoch 55 iteration 0260/1263: training loss 0.321
Epoch 55 iteration 0280/1263: training loss 0.322
Epoch 55 iteration 0300/1263: training loss 0.324
Epoch 55 iteration 0320/1263: training loss 0.326
Epoch 55 iteration 0340/1263: training loss 0.327
Epoch 55 iteration 0360/1263: training loss 0.327
Epoch 55 iteration 0380/1263: training loss 0.329
Epoch 55 iteration 0400/1263: training loss 0.327
Epoch 55 iteration 0420/1263: training loss 0.328
Epoch 55 iteration 0440/1263: training loss 0.327
Epoch 55 iteration 0460/1263: training loss 0.327
Epoch 55 iteration 0480/1263: training loss 0.327
Epoch 55 iteration 0500/1263: training loss 0.327
Epoch 55 iteration 0520/1263: training loss 0.326
Epoch 55 iteration 0540/1263: training loss 0.325
Epoch 55 iteration 0560/1263: training loss 0.324
Epoch 55 iteration 0580/1263: training loss 0.324
Epoch 55 iteration 0600/1263: training loss 0.324
Epoch 55 iteration 0620/1263: training loss 0.325
Epoch 55 iteration 0640/1263: training loss 0.324
Epoch 55 iteration 0660/1263: training loss 0.324
Epoch 55 iteration 0680/1263: training loss 0.324
Epoch 55 iteration 0700/1263: training loss 0.323
Epoch 55 iteration 0720/1263: training loss 0.322
Epoch 55 iteration 0740/1263: training loss 0.323
Epoch 55 iteration 0760/1263: training loss 0.323
Epoch 55 iteration 0780/1263: training loss 0.323
Epoch 55 iteration 0800/1263: training loss 0.323
Epoch 55 iteration 0820/1263: training loss 0.323
Epoch 55 iteration 0840/1263: training loss 0.323
Epoch 55 iteration 0860/1263: training loss 0.324
Epoch 55 iteration 0880/1263: training loss 0.323
Epoch 55 iteration 0900/1263: training loss 0.323
Epoch 55 iteration 0920/1263: training loss 0.323
Epoch 55 iteration 0940/1263: training loss 0.323
Epoch 55 iteration 0960/1263: training loss 0.323
Epoch 55 iteration 0980/1263: training loss 0.323
Epoch 55 iteration 1000/1263: training loss 0.323
Epoch 55 iteration 1020/1263: training loss 0.323
Epoch 55 iteration 1040/1263: training loss 0.323
Epoch 55 iteration 1060/1263: training loss 0.323
Epoch 55 iteration 1080/1263: training loss 0.323
Epoch 55 iteration 1100/1263: training loss 0.323
Epoch 55 iteration 1120/1263: training loss 0.323
Epoch 55 iteration 1140/1263: training loss 0.323
Epoch 55 iteration 1160/1263: training loss 0.324
Epoch 55 iteration 1180/1263: training loss 0.324
Epoch 55 iteration 1200/1263: training loss 0.324
Epoch 55 iteration 1220/1263: training loss 0.324
Epoch 55 iteration 1240/1263: training loss 0.325
Epoch 55 iteration 1260/1263: training loss 0.325
Epoch 55 validation pixAcc: 0.799, mIoU: 0.451
Epoch 56 iteration 0020/1263: training loss 0.306
Epoch 56 iteration 0040/1263: training loss 0.308
Epoch 56 iteration 0060/1263: training loss 0.311
Epoch 56 iteration 0080/1263: training loss 0.313
Epoch 56 iteration 0100/1263: training loss 0.313
Epoch 56 iteration 0120/1263: training loss 0.313
Epoch 56 iteration 0140/1263: training loss 0.321
Epoch 56 iteration 0160/1263: training loss 0.322
Epoch 56 iteration 0180/1263: training loss 0.324
Epoch 56 iteration 0200/1263: training loss 0.325
Epoch 56 iteration 0220/1263: training loss 0.323
Epoch 56 iteration 0240/1263: training loss 0.322
Epoch 56 iteration 0260/1263: training loss 0.319
Epoch 56 iteration 0280/1263: training loss 0.319
Epoch 56 iteration 0300/1263: training loss 0.318
Epoch 56 iteration 0320/1263: training loss 0.317
Epoch 56 iteration 0340/1263: training loss 0.317
Epoch 56 iteration 0360/1263: training loss 0.318
Epoch 56 iteration 0380/1263: training loss 0.317
Epoch 56 iteration 0400/1263: training loss 0.318
Epoch 56 iteration 0420/1263: training loss 0.319
Epoch 56 iteration 0440/1263: training loss 0.320
Epoch 56 iteration 0460/1263: training loss 0.320
Epoch 56 iteration 0480/1263: training loss 0.320
Epoch 56 iteration 0500/1263: training loss 0.319
Epoch 56 iteration 0520/1263: training loss 0.318
Epoch 56 iteration 0540/1263: training loss 0.317
Epoch 56 iteration 0560/1263: training loss 0.318
Epoch 56 iteration 0580/1263: training loss 0.320
Epoch 56 iteration 0600/1263: training loss 0.322
Epoch 56 iteration 0620/1263: training loss 0.324
Epoch 56 iteration 0640/1263: training loss 0.325
Epoch 56 iteration 0660/1263: training loss 0.327
Epoch 56 iteration 0680/1263: training loss 0.328
Epoch 56 iteration 0700/1263: training loss 0.329
Epoch 56 iteration 0720/1263: training loss 0.329
Epoch 56 iteration 0740/1263: training loss 0.330
Epoch 56 iteration 0760/1263: training loss 0.334
Epoch 56 iteration 0780/1263: training loss 0.335
Epoch 56 iteration 0800/1263: training loss 0.337
Epoch 56 iteration 0820/1263: training loss 0.337
Epoch 56 iteration 0840/1263: training loss 0.338
Epoch 56 iteration 0860/1263: training loss 0.338
Epoch 56 iteration 0880/1263: training loss 0.339
Epoch 56 iteration 0900/1263: training loss 0.339
Epoch 56 iteration 0920/1263: training loss 0.341
Epoch 56 iteration 0940/1263: training loss 0.341
Epoch 56 iteration 0960/1263: training loss 0.341
Epoch 56 iteration 0980/1263: training loss 0.341
Epoch 56 iteration 1000/1263: training loss 0.341
Epoch 56 iteration 1020/1263: training loss 0.341
Epoch 56 iteration 1040/1263: training loss 0.340
Epoch 56 iteration 1060/1263: training loss 0.340
Epoch 56 iteration 1080/1263: training loss 0.340
Epoch 56 iteration 1100/1263: training loss 0.341
Epoch 56 iteration 1120/1263: training loss 0.341
Epoch 56 iteration 1140/1263: training loss 0.342
Epoch 56 iteration 1160/1263: training loss 0.341
Epoch 56 iteration 1180/1263: training loss 0.341
Epoch 56 iteration 1200/1263: training loss 0.341
Epoch 56 iteration 1220/1263: training loss 0.342
Epoch 56 iteration 1240/1263: training loss 0.342
Epoch 56 iteration 1260/1263: training loss 0.342
Epoch 56 validation pixAcc: 0.796, mIoU: 0.444
Epoch 57 iteration 0020/1263: training loss 0.343
Epoch 57 iteration 0040/1263: training loss 0.363
Epoch 57 iteration 0060/1263: training loss 0.365
Epoch 57 iteration 0080/1263: training loss 0.361
Epoch 57 iteration 0100/1263: training loss 0.354
Epoch 57 iteration 0120/1263: training loss 0.350
Epoch 57 iteration 0140/1263: training loss 0.346
Epoch 57 iteration 0160/1263: training loss 0.344
Epoch 57 iteration 0180/1263: training loss 0.340
Epoch 57 iteration 0200/1263: training loss 0.339
Epoch 57 iteration 0220/1263: training loss 0.340
Epoch 57 iteration 0240/1263: training loss 0.341
Epoch 57 iteration 0260/1263: training loss 0.339
Epoch 57 iteration 0280/1263: training loss 0.337
Epoch 57 iteration 0300/1263: training loss 0.336
Epoch 57 iteration 0320/1263: training loss 0.337
Epoch 57 iteration 0340/1263: training loss 0.337
Epoch 57 iteration 0360/1263: training loss 0.337
Epoch 57 iteration 0380/1263: training loss 0.336
Epoch 57 iteration 0400/1263: training loss 0.335
Epoch 57 iteration 0420/1263: training loss 0.334
Epoch 57 iteration 0440/1263: training loss 0.334
Epoch 57 iteration 0460/1263: training loss 0.333
Epoch 57 iteration 0480/1263: training loss 0.333
Epoch 57 iteration 0500/1263: training loss 0.336
Epoch 57 iteration 0520/1263: training loss 0.337
Epoch 57 iteration 0540/1263: training loss 0.338
Epoch 57 iteration 0560/1263: training loss 0.338
Epoch 57 iteration 0580/1263: training loss 0.338
Epoch 57 iteration 0600/1263: training loss 0.337
Epoch 57 iteration 0620/1263: training loss 0.336
Epoch 57 iteration 0640/1263: training loss 0.335
Epoch 57 iteration 0660/1263: training loss 0.334
Epoch 57 iteration 0680/1263: training loss 0.333
Epoch 57 iteration 0700/1263: training loss 0.333
Epoch 57 iteration 0720/1263: training loss 0.333
Epoch 57 iteration 0740/1263: training loss 0.333
Epoch 57 iteration 0760/1263: training loss 0.333
Epoch 57 iteration 0780/1263: training loss 0.332
Epoch 57 iteration 0800/1263: training loss 0.333
Epoch 57 iteration 0820/1263: training loss 0.333
Epoch 57 iteration 0840/1263: training loss 0.334
Epoch 57 iteration 0860/1263: training loss 0.333
Epoch 57 iteration 0880/1263: training loss 0.334
Epoch 57 iteration 0900/1263: training loss 0.334
Epoch 57 iteration 0920/1263: training loss 0.333
Epoch 57 iteration 0940/1263: training loss 0.333
Epoch 57 iteration 0960/1263: training loss 0.333
Epoch 57 iteration 0980/1263: training loss 0.333
Epoch 57 iteration 1000/1263: training loss 0.333
Epoch 57 iteration 1020/1263: training loss 0.333
Epoch 57 iteration 1040/1263: training loss 0.333
Epoch 57 iteration 1060/1263: training loss 0.333
Epoch 57 iteration 1080/1263: training loss 0.333
Epoch 57 iteration 1100/1263: training loss 0.333
Epoch 57 iteration 1120/1263: training loss 0.333
Epoch 57 iteration 1140/1263: training loss 0.334
Epoch 57 iteration 1160/1263: training loss 0.334
Epoch 57 iteration 1180/1263: training loss 0.334
Epoch 57 iteration 1200/1263: training loss 0.334
Epoch 57 iteration 1220/1263: training loss 0.335
Epoch 57 iteration 1240/1263: training loss 0.335
Epoch 57 iteration 1260/1263: training loss 0.334
Epoch 57 validation pixAcc: 0.795, mIoU: 0.442
Epoch 58 iteration 0020/1263: training loss 0.318
Epoch 58 iteration 0040/1263: training loss 0.306
Epoch 58 iteration 0060/1263: training loss 0.314
Epoch 58 iteration 0080/1263: training loss 0.321
Epoch 58 iteration 0100/1263: training loss 0.331
Epoch 58 iteration 0120/1263: training loss 0.334
Epoch 58 iteration 0140/1263: training loss 0.333
Epoch 58 iteration 0160/1263: training loss 0.331
Epoch 58 iteration 0180/1263: training loss 0.328
Epoch 58 iteration 0200/1263: training loss 0.327
Epoch 58 iteration 0220/1263: training loss 0.327
Epoch 58 iteration 0240/1263: training loss 0.330
Epoch 58 iteration 0260/1263: training loss 0.331
Epoch 58 iteration 0280/1263: training loss 0.333
Epoch 58 iteration 0300/1263: training loss 0.329
Epoch 58 iteration 0320/1263: training loss 0.329
Epoch 58 iteration 0340/1263: training loss 0.328
Epoch 58 iteration 0360/1263: training loss 0.327
Epoch 58 iteration 0380/1263: training loss 0.327
Epoch 58 iteration 0400/1263: training loss 0.325
Epoch 58 iteration 0420/1263: training loss 0.324
Epoch 58 iteration 0440/1263: training loss 0.324
Epoch 58 iteration 0460/1263: training loss 0.324
Epoch 58 iteration 0480/1263: training loss 0.325
Epoch 58 iteration 0500/1263: training loss 0.325
Epoch 58 iteration 0520/1263: training loss 0.324
Epoch 58 iteration 0540/1263: training loss 0.324
Epoch 58 iteration 0560/1263: training loss 0.325
Epoch 58 iteration 0580/1263: training loss 0.324
Epoch 58 iteration 0600/1263: training loss 0.324
Epoch 58 iteration 0620/1263: training loss 0.325
Epoch 58 iteration 0640/1263: training loss 0.325
Epoch 58 iteration 0660/1263: training loss 0.325
Epoch 58 iteration 0680/1263: training loss 0.325
Epoch 58 iteration 0700/1263: training loss 0.326
Epoch 58 iteration 0720/1263: training loss 0.327
Epoch 58 iteration 0740/1263: training loss 0.327
Epoch 58 iteration 0760/1263: training loss 0.328
Epoch 58 iteration 0780/1263: training loss 0.328
Epoch 58 iteration 0800/1263: training loss 0.328
Epoch 58 iteration 0820/1263: training loss 0.327
Epoch 58 iteration 0840/1263: training loss 0.327
Epoch 58 iteration 0860/1263: training loss 0.327
Epoch 58 iteration 0880/1263: training loss 0.328
Epoch 58 iteration 0900/1263: training loss 0.327
Epoch 58 iteration 0920/1263: training loss 0.326
Epoch 58 iteration 0940/1263: training loss 0.326
Epoch 58 iteration 0960/1263: training loss 0.326
Epoch 58 iteration 0980/1263: training loss 0.326
Epoch 58 iteration 1000/1263: training loss 0.326
Epoch 58 iteration 1020/1263: training loss 0.326
Epoch 58 iteration 1040/1263: training loss 0.326
Epoch 58 iteration 1060/1263: training loss 0.327
Epoch 58 iteration 1080/1263: training loss 0.326
Epoch 58 iteration 1100/1263: training loss 0.327
Epoch 58 iteration 1120/1263: training loss 0.326
Epoch 58 iteration 1140/1263: training loss 0.326
Epoch 58 iteration 1160/1263: training loss 0.326
Epoch 58 iteration 1180/1263: training loss 0.325
Epoch 58 iteration 1200/1263: training loss 0.326
Epoch 58 iteration 1220/1263: training loss 0.326
Epoch 58 iteration 1240/1263: training loss 0.326
Epoch 58 iteration 1260/1263: training loss 0.326
Epoch 58 validation pixAcc: 0.801, mIoU: 0.456
Epoch 59 iteration 0020/1263: training loss 0.315
Epoch 59 iteration 0040/1263: training loss 0.299
Epoch 59 iteration 0060/1263: training loss 0.312
Epoch 59 iteration 0080/1263: training loss 0.315
Epoch 59 iteration 0100/1263: training loss 0.321
Epoch 59 iteration 0120/1263: training loss 0.330
Epoch 59 iteration 0140/1263: training loss 0.334
Epoch 59 iteration 0160/1263: training loss 0.333
Epoch 59 iteration 0180/1263: training loss 0.330
Epoch 59 iteration 0200/1263: training loss 0.331
Epoch 59 iteration 0220/1263: training loss 0.329
Epoch 59 iteration 0240/1263: training loss 0.329
Epoch 59 iteration 0260/1263: training loss 0.330
Epoch 59 iteration 0280/1263: training loss 0.330
Epoch 59 iteration 0300/1263: training loss 0.330
Epoch 59 iteration 0320/1263: training loss 0.329
Epoch 59 iteration 0340/1263: training loss 0.331
Epoch 59 iteration 0360/1263: training loss 0.334
Epoch 59 iteration 0380/1263: training loss 0.335
Epoch 59 iteration 0400/1263: training loss 0.334
Epoch 59 iteration 0420/1263: training loss 0.333
Epoch 59 iteration 0440/1263: training loss 0.333
Epoch 59 iteration 0460/1263: training loss 0.333
Epoch 59 iteration 0480/1263: training loss 0.332
Epoch 59 iteration 0500/1263: training loss 0.332
Epoch 59 iteration 0520/1263: training loss 0.331
Epoch 59 iteration 0540/1263: training loss 0.330
Epoch 59 iteration 0560/1263: training loss 0.332
Epoch 59 iteration 0580/1263: training loss 0.332
Epoch 59 iteration 0600/1263: training loss 0.332
Epoch 59 iteration 0620/1263: training loss 0.332
Epoch 59 iteration 0640/1263: training loss 0.332
Epoch 59 iteration 0660/1263: training loss 0.333
Epoch 59 iteration 0680/1263: training loss 0.332
Epoch 59 iteration 0700/1263: training loss 0.332
Epoch 59 iteration 0720/1263: training loss 0.331
Epoch 59 iteration 0740/1263: training loss 0.330
Epoch 59 iteration 0760/1263: training loss 0.329
Epoch 59 iteration 0780/1263: training loss 0.328
Epoch 59 iteration 0800/1263: training loss 0.328
Epoch 59 iteration 0820/1263: training loss 0.329
Epoch 59 iteration 0840/1263: training loss 0.329
Epoch 59 iteration 0860/1263: training loss 0.329
Epoch 59 iteration 0880/1263: training loss 0.329
Epoch 59 iteration 0900/1263: training loss 0.329
Epoch 59 iteration 0920/1263: training loss 0.329
Epoch 59 iteration 0940/1263: training loss 0.329
Epoch 59 iteration 0960/1263: training loss 0.329
Epoch 59 iteration 0980/1263: training loss 0.329
Epoch 59 iteration 1000/1263: training loss 0.329
Epoch 59 iteration 1020/1263: training loss 0.329
Epoch 59 iteration 1040/1263: training loss 0.330
Epoch 59 iteration 1060/1263: training loss 0.331
Epoch 59 iteration 1080/1263: training loss 0.330
Epoch 59 iteration 1100/1263: training loss 0.330
Epoch 59 iteration 1120/1263: training loss 0.330
Epoch 59 iteration 1140/1263: training loss 0.330
Epoch 59 iteration 1160/1263: training loss 0.331
Epoch 59 iteration 1180/1263: training loss 0.330
Epoch 59 iteration 1200/1263: training loss 0.331
Epoch 59 iteration 1220/1263: training loss 0.331
Epoch 59 iteration 1240/1263: training loss 0.330
Epoch 59 iteration 1260/1263: training loss 0.330
Epoch 59 validation pixAcc: 0.796, mIoU: 0.450
Epoch 60 iteration 0020/1263: training loss 0.302
Epoch 60 iteration 0040/1263: training loss 0.303
Epoch 60 iteration 0060/1263: training loss 0.308
Epoch 60 iteration 0080/1263: training loss 0.313
Epoch 60 iteration 0100/1263: training loss 0.310
Epoch 60 iteration 0120/1263: training loss 0.313
Epoch 60 iteration 0140/1263: training loss 0.319
Epoch 60 iteration 0160/1263: training loss 0.320
Epoch 60 iteration 0180/1263: training loss 0.319
Epoch 60 iteration 0200/1263: training loss 0.319
Epoch 60 iteration 0220/1263: training loss 0.317
Epoch 60 iteration 0240/1263: training loss 0.317
Epoch 60 iteration 0260/1263: training loss 0.318
Epoch 60 iteration 0280/1263: training loss 0.319
Epoch 60 iteration 0300/1263: training loss 0.317
Epoch 60 iteration 0320/1263: training loss 0.316
Epoch 60 iteration 0340/1263: training loss 0.317
Epoch 60 iteration 0360/1263: training loss 0.318
Epoch 60 iteration 0380/1263: training loss 0.319
Epoch 60 iteration 0400/1263: training loss 0.319
Epoch 60 iteration 0420/1263: training loss 0.320
Epoch 60 iteration 0440/1263: training loss 0.320
Epoch 60 iteration 0460/1263: training loss 0.320
Epoch 60 iteration 0480/1263: training loss 0.323
Epoch 60 iteration 0500/1263: training loss 0.321
Epoch 60 iteration 0520/1263: training loss 0.321
Epoch 60 iteration 0540/1263: training loss 0.321
Epoch 60 iteration 0560/1263: training loss 0.320
Epoch 60 iteration 0580/1263: training loss 0.319
Epoch 60 iteration 0600/1263: training loss 0.319
Epoch 60 iteration 0620/1263: training loss 0.318
Epoch 60 iteration 0640/1263: training loss 0.316
Epoch 60 iteration 0660/1263: training loss 0.316
Epoch 60 iteration 0680/1263: training loss 0.316
Epoch 60 iteration 0700/1263: training loss 0.315
Epoch 60 iteration 0720/1263: training loss 0.315
Epoch 60 iteration 0740/1263: training loss 0.315
Epoch 60 iteration 0760/1263: training loss 0.315
Epoch 60 iteration 0780/1263: training loss 0.315
Epoch 60 iteration 0800/1263: training loss 0.315
Epoch 60 iteration 0820/1263: training loss 0.315
Epoch 60 iteration 0840/1263: training loss 0.316
Epoch 60 iteration 0860/1263: training loss 0.315
Epoch 60 iteration 0880/1263: training loss 0.316
Epoch 60 iteration 0900/1263: training loss 0.315
Epoch 60 iteration 0920/1263: training loss 0.315
Epoch 60 iteration 0940/1263: training loss 0.316
Epoch 60 iteration 0960/1263: training loss 0.315
Epoch 60 iteration 0980/1263: training loss 0.315
Epoch 60 iteration 1000/1263: training loss 0.315
Epoch 60 iteration 1020/1263: training loss 0.316
Epoch 60 iteration 1040/1263: training loss 0.316
Epoch 60 iteration 1060/1263: training loss 0.316
Epoch 60 iteration 1080/1263: training loss 0.317
Epoch 60 iteration 1100/1263: training loss 0.317
Epoch 60 iteration 1120/1263: training loss 0.318
Epoch 60 iteration 1140/1263: training loss 0.319
Epoch 60 iteration 1160/1263: training loss 0.318
Epoch 60 iteration 1180/1263: training loss 0.319
Epoch 60 iteration 1200/1263: training loss 0.318
Epoch 60 iteration 1220/1263: training loss 0.318
Epoch 60 iteration 1240/1263: training loss 0.319
Epoch 60 iteration 1260/1263: training loss 0.320
Epoch 60 validation pixAcc: 0.796, mIoU: 0.437
Epoch 61 iteration 0020/1263: training loss 0.291
Epoch 61 iteration 0040/1263: training loss 0.309
Epoch 61 iteration 0060/1263: training loss 0.326
Epoch 61 iteration 0080/1263: training loss 0.333
Epoch 61 iteration 0100/1263: training loss 0.328
Epoch 61 iteration 0120/1263: training loss 0.328
Epoch 61 iteration 0140/1263: training loss 0.329
Epoch 61 iteration 0160/1263: training loss 0.326
Epoch 61 iteration 0180/1263: training loss 0.333
Epoch 61 iteration 0200/1263: training loss 0.331
Epoch 61 iteration 0220/1263: training loss 0.328
Epoch 61 iteration 0240/1263: training loss 0.327
Epoch 61 iteration 0260/1263: training loss 0.325
Epoch 61 iteration 0280/1263: training loss 0.323
Epoch 61 iteration 0300/1263: training loss 0.322
Epoch 61 iteration 0320/1263: training loss 0.322
Epoch 61 iteration 0340/1263: training loss 0.323
Epoch 61 iteration 0360/1263: training loss 0.323
Epoch 61 iteration 0380/1263: training loss 0.323
Epoch 61 iteration 0400/1263: training loss 0.324
Epoch 61 iteration 0420/1263: training loss 0.323
Epoch 61 iteration 0440/1263: training loss 0.323
Epoch 61 iteration 0460/1263: training loss 0.322
Epoch 61 iteration 0480/1263: training loss 0.321
Epoch 61 iteration 0500/1263: training loss 0.322
Epoch 61 iteration 0520/1263: training loss 0.321
Epoch 61 iteration 0540/1263: training loss 0.320
Epoch 61 iteration 0560/1263: training loss 0.320
Epoch 61 iteration 0580/1263: training loss 0.320
Epoch 61 iteration 0600/1263: training loss 0.319
Epoch 61 iteration 0620/1263: training loss 0.319
Epoch 61 iteration 0640/1263: training loss 0.319
Epoch 61 iteration 0660/1263: training loss 0.319
Epoch 61 iteration 0680/1263: training loss 0.318
Epoch 61 iteration 0700/1263: training loss 0.318
Epoch 61 iteration 0720/1263: training loss 0.318
Epoch 61 iteration 0740/1263: training loss 0.317
Epoch 61 iteration 0760/1263: training loss 0.318
Epoch 61 iteration 0780/1263: training loss 0.318
Epoch 61 iteration 0800/1263: training loss 0.318
Epoch 61 iteration 0820/1263: training loss 0.318
Epoch 61 iteration 0840/1263: training loss 0.317
Epoch 61 iteration 0860/1263: training loss 0.317
Epoch 61 iteration 0880/1263: training loss 0.318
Epoch 61 iteration 0900/1263: training loss 0.317
Epoch 61 iteration 0920/1263: training loss 0.318
Epoch 61 iteration 0940/1263: training loss 0.317
Epoch 61 iteration 0960/1263: training loss 0.318
Epoch 61 iteration 0980/1263: training loss 0.317
Epoch 61 iteration 1000/1263: training loss 0.317
Epoch 61 iteration 1020/1263: training loss 0.317
Epoch 61 iteration 1040/1263: training loss 0.317
Epoch 61 iteration 1060/1263: training loss 0.317
Epoch 61 iteration 1080/1263: training loss 0.317
Epoch 61 iteration 1100/1263: training loss 0.318
Epoch 61 iteration 1120/1263: training loss 0.318
Epoch 61 iteration 1140/1263: training loss 0.318
Epoch 61 iteration 1160/1263: training loss 0.318
Epoch 61 iteration 1180/1263: training loss 0.318
Epoch 61 iteration 1200/1263: training loss 0.318
Epoch 61 iteration 1220/1263: training loss 0.319
Epoch 61 iteration 1240/1263: training loss 0.319
Epoch 61 iteration 1260/1263: training loss 0.319
Epoch 61 validation pixAcc: 0.796, mIoU: 0.448
Epoch 62 iteration 0020/1263: training loss 0.304
Epoch 62 iteration 0040/1263: training loss 0.304
Epoch 62 iteration 0060/1263: training loss 0.297
Epoch 62 iteration 0080/1263: training loss 0.299
Epoch 62 iteration 0100/1263: training loss 0.299
Epoch 62 iteration 0120/1263: training loss 0.300
Epoch 62 iteration 0140/1263: training loss 0.296
Epoch 62 iteration 0160/1263: training loss 0.295
Epoch 62 iteration 0180/1263: training loss 0.300
Epoch 62 iteration 0200/1263: training loss 0.301
Epoch 62 iteration 0220/1263: training loss 0.300
Epoch 62 iteration 0240/1263: training loss 0.298
Epoch 62 iteration 0260/1263: training loss 0.296
Epoch 62 iteration 0280/1263: training loss 0.295
Epoch 62 iteration 0300/1263: training loss 0.300
Epoch 62 iteration 0320/1263: training loss 0.302
Epoch 62 iteration 0340/1263: training loss 0.303
Epoch 62 iteration 0360/1263: training loss 0.303
Epoch 62 iteration 0380/1263: training loss 0.303
Epoch 62 iteration 0400/1263: training loss 0.302
Epoch 62 iteration 0420/1263: training loss 0.302
Epoch 62 iteration 0440/1263: training loss 0.302
Epoch 62 iteration 0460/1263: training loss 0.301
Epoch 62 iteration 0480/1263: training loss 0.300
Epoch 62 iteration 0500/1263: training loss 0.299
Epoch 62 iteration 0520/1263: training loss 0.299
Epoch 62 iteration 0540/1263: training loss 0.300
Epoch 62 iteration 0560/1263: training loss 0.302
Epoch 62 iteration 0580/1263: training loss 0.302
Epoch 62 iteration 0600/1263: training loss 0.302
Epoch 62 iteration 0620/1263: training loss 0.302
Epoch 62 iteration 0640/1263: training loss 0.303
Epoch 62 iteration 0660/1263: training loss 0.302
Epoch 62 iteration 0680/1263: training loss 0.302
Epoch 62 iteration 0700/1263: training loss 0.302
Epoch 62 iteration 0720/1263: training loss 0.302
Epoch 62 iteration 0740/1263: training loss 0.301
Epoch 62 iteration 0760/1263: training loss 0.301
Epoch 62 iteration 0780/1263: training loss 0.301
Epoch 62 iteration 0800/1263: training loss 0.300
Epoch 62 iteration 0820/1263: training loss 0.300
Epoch 62 iteration 0840/1263: training loss 0.300
Epoch 62 iteration 0860/1263: training loss 0.301
Epoch 62 iteration 0880/1263: training loss 0.300
Epoch 62 iteration 0900/1263: training loss 0.301
Epoch 62 iteration 0920/1263: training loss 0.301
Epoch 62 iteration 0940/1263: training loss 0.301
Epoch 62 iteration 0960/1263: training loss 0.301
Epoch 62 iteration 0980/1263: training loss 0.301
Epoch 62 iteration 1000/1263: training loss 0.300
Epoch 62 iteration 1020/1263: training loss 0.300
Epoch 62 iteration 1040/1263: training loss 0.300
Epoch 62 iteration 1060/1263: training loss 0.299
Epoch 62 iteration 1080/1263: training loss 0.299
Epoch 62 iteration 1100/1263: training loss 0.299
Epoch 62 iteration 1120/1263: training loss 0.300
Epoch 62 iteration 1140/1263: training loss 0.300
Epoch 62 iteration 1160/1263: training loss 0.301
Epoch 62 iteration 1180/1264: training loss 0.301
Epoch 62 iteration 1200/1264: training loss 0.301
Epoch 62 iteration 1220/1264: training loss 0.301
Epoch 62 iteration 1240/1264: training loss 0.302
Epoch 62 iteration 1260/1264: training loss 0.301
Epoch 62 validation pixAcc: 0.799, mIoU: 0.450
Epoch 63 iteration 0020/1263: training loss 0.305
Epoch 63 iteration 0040/1263: training loss 0.307
Epoch 63 iteration 0060/1263: training loss 0.303
Epoch 63 iteration 0080/1263: training loss 0.296
Epoch 63 iteration 0100/1263: training loss 0.290
Epoch 63 iteration 0120/1263: training loss 0.288
Epoch 63 iteration 0140/1263: training loss 0.288
Epoch 63 iteration 0160/1263: training loss 0.292
Epoch 63 iteration 0180/1263: training loss 0.287
Epoch 63 iteration 0200/1263: training loss 0.286
Epoch 63 iteration 0220/1263: training loss 0.286
Epoch 63 iteration 0240/1263: training loss 0.285
Epoch 63 iteration 0260/1263: training loss 0.286
Epoch 63 iteration 0280/1263: training loss 0.288
Epoch 63 iteration 0300/1263: training loss 0.287
Epoch 63 iteration 0320/1263: training loss 0.287
Epoch 63 iteration 0340/1263: training loss 0.289
Epoch 63 iteration 0360/1263: training loss 0.289
Epoch 63 iteration 0380/1263: training loss 0.290
Epoch 63 iteration 0400/1263: training loss 0.292
Epoch 63 iteration 0420/1263: training loss 0.293
Epoch 63 iteration 0440/1263: training loss 0.293
Epoch 63 iteration 0460/1263: training loss 0.293
Epoch 63 iteration 0480/1263: training loss 0.294
Epoch 63 iteration 0500/1263: training loss 0.294
Epoch 63 iteration 0520/1263: training loss 0.296
Epoch 63 iteration 0540/1263: training loss 0.295
Epoch 63 iteration 0560/1263: training loss 0.295
Epoch 63 iteration 0580/1263: training loss 0.296
Epoch 63 iteration 0600/1263: training loss 0.295
Epoch 63 iteration 0620/1263: training loss 0.296
Epoch 63 iteration 0640/1263: training loss 0.297
Epoch 63 iteration 0660/1263: training loss 0.296
Epoch 63 iteration 0680/1263: training loss 0.297
Epoch 63 iteration 0700/1263: training loss 0.296
Epoch 63 iteration 0720/1263: training loss 0.296
Epoch 63 iteration 0740/1263: training loss 0.296
Epoch 63 iteration 0760/1263: training loss 0.296
Epoch 63 iteration 0780/1263: training loss 0.296
Epoch 63 iteration 0800/1263: training loss 0.296
Epoch 63 iteration 0820/1263: training loss 0.296
Epoch 63 iteration 0840/1263: training loss 0.296
Epoch 63 iteration 0860/1263: training loss 0.295
Epoch 63 iteration 0880/1263: training loss 0.295
Epoch 63 iteration 0900/1263: training loss 0.295
Epoch 63 iteration 0920/1263: training loss 0.295
Epoch 63 iteration 0940/1263: training loss 0.295
Epoch 63 iteration 0960/1263: training loss 0.294
Epoch 63 iteration 0980/1263: training loss 0.294
Epoch 63 iteration 1000/1263: training loss 0.295
Epoch 63 iteration 1020/1263: training loss 0.295
Epoch 63 iteration 1040/1263: training loss 0.295
Epoch 63 iteration 1060/1263: training loss 0.296
Epoch 63 iteration 1080/1263: training loss 0.295
Epoch 63 iteration 1100/1263: training loss 0.296
Epoch 63 iteration 1120/1263: training loss 0.296
Epoch 63 iteration 1140/1263: training loss 0.296
Epoch 63 iteration 1160/1263: training loss 0.297
Epoch 63 iteration 1180/1263: training loss 0.298
Epoch 63 iteration 1200/1263: training loss 0.298
Epoch 63 iteration 1220/1263: training loss 0.298
Epoch 63 iteration 1240/1263: training loss 0.298
Epoch 63 iteration 1260/1263: training loss 0.298
Epoch 63 validation pixAcc: 0.798, mIoU: 0.449
Epoch 64 iteration 0020/1263: training loss 0.274
Epoch 64 iteration 0040/1263: training loss 0.276
Epoch 64 iteration 0060/1263: training loss 0.286
Epoch 64 iteration 0080/1263: training loss 0.287
Epoch 64 iteration 0100/1263: training loss 0.283
Epoch 64 iteration 0120/1263: training loss 0.287
Epoch 64 iteration 0140/1263: training loss 0.287
Epoch 64 iteration 0160/1263: training loss 0.289
Epoch 64 iteration 0180/1263: training loss 0.290
Epoch 64 iteration 0200/1263: training loss 0.289
Epoch 64 iteration 0220/1263: training loss 0.288
Epoch 64 iteration 0240/1263: training loss 0.287
Epoch 64 iteration 0260/1263: training loss 0.286
Epoch 64 iteration 0280/1263: training loss 0.285
Epoch 64 iteration 0300/1263: training loss 0.284
Epoch 64 iteration 0320/1263: training loss 0.283
Epoch 64 iteration 0340/1263: training loss 0.284
Epoch 64 iteration 0360/1263: training loss 0.284
Epoch 64 iteration 0380/1263: training loss 0.284
Epoch 64 iteration 0400/1263: training loss 0.283
Epoch 64 iteration 0420/1263: training loss 0.283
Epoch 64 iteration 0440/1263: training loss 0.282
Epoch 64 iteration 0460/1263: training loss 0.283
Epoch 64 iteration 0480/1263: training loss 0.283
Epoch 64 iteration 0500/1263: training loss 0.283
Epoch 64 iteration 0520/1263: training loss 0.283
Epoch 64 iteration 0540/1263: training loss 0.284
Epoch 64 iteration 0560/1263: training loss 0.286
Epoch 64 iteration 0580/1263: training loss 0.285
Epoch 64 iteration 0600/1263: training loss 0.285
Epoch 64 iteration 0620/1263: training loss 0.285
Epoch 64 iteration 0640/1263: training loss 0.284
Epoch 64 iteration 0660/1263: training loss 0.285
Epoch 64 iteration 0680/1263: training loss 0.286
Epoch 64 iteration 0700/1263: training loss 0.288
Epoch 64 iteration 0720/1263: training loss 0.290
Epoch 64 iteration 0740/1263: training loss 0.290
Epoch 64 iteration 0760/1263: training loss 0.291
Epoch 64 iteration 0780/1263: training loss 0.291
Epoch 64 iteration 0800/1263: training loss 0.290
Epoch 64 iteration 0820/1263: training loss 0.291
Epoch 64 iteration 0840/1263: training loss 0.291
Epoch 64 iteration 0860/1263: training loss 0.290
Epoch 64 iteration 0880/1263: training loss 0.290
Epoch 64 iteration 0900/1263: training loss 0.290
Epoch 64 iteration 0920/1263: training loss 0.291
Epoch 64 iteration 0940/1263: training loss 0.291
Epoch 64 iteration 0960/1263: training loss 0.291
Epoch 64 iteration 0980/1263: training loss 0.291
Epoch 64 iteration 1000/1263: training loss 0.292
Epoch 64 iteration 1020/1263: training loss 0.293
Epoch 64 iteration 1040/1263: training loss 0.293
Epoch 64 iteration 1060/1263: training loss 0.294
Epoch 64 iteration 1080/1263: training loss 0.295
Epoch 64 iteration 1100/1263: training loss 0.295
Epoch 64 iteration 1120/1263: training loss 0.294
Epoch 64 iteration 1140/1263: training loss 0.294
Epoch 64 iteration 1160/1263: training loss 0.295
Epoch 64 iteration 1180/1263: training loss 0.294
Epoch 64 iteration 1200/1263: training loss 0.295
Epoch 64 iteration 1220/1263: training loss 0.294
Epoch 64 iteration 1240/1263: training loss 0.294
Epoch 64 iteration 1260/1263: training loss 0.294
Epoch 64 validation pixAcc: 0.803, mIoU: 0.456
Epoch 65 iteration 0020/1263: training loss 0.253
Epoch 65 iteration 0040/1263: training loss 0.265
Epoch 65 iteration 0060/1263: training loss 0.264
Epoch 65 iteration 0080/1263: training loss 0.269
Epoch 65 iteration 0100/1263: training loss 0.275
Epoch 65 iteration 0120/1263: training loss 0.280
Epoch 65 iteration 0140/1263: training loss 0.281
Epoch 65 iteration 0160/1263: training loss 0.286
Epoch 65 iteration 0180/1263: training loss 0.285
Epoch 65 iteration 0200/1263: training loss 0.287
Epoch 65 iteration 0220/1263: training loss 0.286
Epoch 65 iteration 0240/1263: training loss 0.289
Epoch 65 iteration 0260/1263: training loss 0.291
Epoch 65 iteration 0280/1263: training loss 0.292
Epoch 65 iteration 0300/1263: training loss 0.292
Epoch 65 iteration 0320/1263: training loss 0.297
Epoch 65 iteration 0340/1263: training loss 0.299
Epoch 65 iteration 0360/1263: training loss 0.299
Epoch 65 iteration 0380/1263: training loss 0.298
Epoch 65 iteration 0400/1263: training loss 0.298
Epoch 65 iteration 0420/1263: training loss 0.298
Epoch 65 iteration 0440/1263: training loss 0.298
Epoch 65 iteration 0460/1263: training loss 0.298
Epoch 65 iteration 0480/1263: training loss 0.298
Epoch 65 iteration 0500/1263: training loss 0.298
Epoch 65 iteration 0520/1263: training loss 0.298
Epoch 65 iteration 0540/1263: training loss 0.298
Epoch 65 iteration 0560/1263: training loss 0.298
Epoch 65 iteration 0580/1263: training loss 0.298
Epoch 65 iteration 0600/1263: training loss 0.298
Epoch 65 iteration 0620/1263: training loss 0.297
Epoch 65 iteration 0640/1263: training loss 0.297
Epoch 65 iteration 0660/1263: training loss 0.296
Epoch 65 iteration 0680/1263: training loss 0.295
Epoch 65 iteration 0700/1263: training loss 0.297
Epoch 65 iteration 0720/1263: training loss 0.298
Epoch 65 iteration 0740/1263: training loss 0.298
Epoch 65 iteration 0760/1263: training loss 0.298
Epoch 65 iteration 0780/1263: training loss 0.298
Epoch 65 iteration 0800/1263: training loss 0.298
Epoch 65 iteration 0820/1263: training loss 0.298
Epoch 65 iteration 0840/1263: training loss 0.297
Epoch 65 iteration 0860/1263: training loss 0.297
Epoch 65 iteration 0880/1263: training loss 0.297
Epoch 65 iteration 0900/1263: training loss 0.298
Epoch 65 iteration 0920/1263: training loss 0.298
Epoch 65 iteration 0940/1263: training loss 0.298
Epoch 65 iteration 0960/1263: training loss 0.298
Epoch 65 iteration 0980/1263: training loss 0.298
Epoch 65 iteration 1000/1263: training loss 0.297
Epoch 65 iteration 1020/1263: training loss 0.297
Epoch 65 iteration 1040/1263: training loss 0.298
Epoch 65 iteration 1060/1263: training loss 0.298
Epoch 65 iteration 1080/1263: training loss 0.298
Epoch 65 iteration 1100/1263: training loss 0.297
Epoch 65 iteration 1120/1263: training loss 0.297
Epoch 65 iteration 1140/1263: training loss 0.298
Epoch 65 iteration 1160/1263: training loss 0.298
Epoch 65 iteration 1180/1263: training loss 0.298
Epoch 65 iteration 1200/1263: training loss 0.298
Epoch 65 iteration 1220/1263: training loss 0.298
Epoch 65 iteration 1240/1263: training loss 0.299
Epoch 65 iteration 1260/1263: training loss 0.299
Epoch 65 validation pixAcc: 0.799, mIoU: 0.444
Epoch 66 iteration 0020/1263: training loss 0.277
Epoch 66 iteration 0040/1263: training loss 0.284
Epoch 66 iteration 0060/1263: training loss 0.291
Epoch 66 iteration 0080/1263: training loss 0.290
Epoch 66 iteration 0100/1263: training loss 0.290
Epoch 66 iteration 0120/1263: training loss 0.286
Epoch 66 iteration 0140/1263: training loss 0.288
Epoch 66 iteration 0160/1263: training loss 0.287
Epoch 66 iteration 0180/1263: training loss 0.286
Epoch 66 iteration 0200/1263: training loss 0.281
Epoch 66 iteration 0220/1263: training loss 0.284
Epoch 66 iteration 0240/1263: training loss 0.283
Epoch 66 iteration 0260/1263: training loss 0.285
Epoch 66 iteration 0280/1263: training loss 0.284
Epoch 66 iteration 0300/1263: training loss 0.284
Epoch 66 iteration 0320/1263: training loss 0.284
Epoch 66 iteration 0340/1263: training loss 0.285
Epoch 66 iteration 0360/1263: training loss 0.285
Epoch 66 iteration 0380/1263: training loss 0.286
Epoch 66 iteration 0400/1263: training loss 0.289
Epoch 66 iteration 0420/1263: training loss 0.290
Epoch 66 iteration 0440/1263: training loss 0.289
Epoch 66 iteration 0460/1263: training loss 0.290
Epoch 66 iteration 0480/1263: training loss 0.291
Epoch 66 iteration 0500/1263: training loss 0.290
Epoch 66 iteration 0520/1263: training loss 0.291
Epoch 66 iteration 0540/1263: training loss 0.290
Epoch 66 iteration 0560/1263: training loss 0.291
Epoch 66 iteration 0580/1263: training loss 0.290
Epoch 66 iteration 0600/1263: training loss 0.290
Epoch 66 iteration 0620/1263: training loss 0.289
Epoch 66 iteration 0640/1263: training loss 0.289
Epoch 66 iteration 0660/1263: training loss 0.289
Epoch 66 iteration 0680/1263: training loss 0.290
Epoch 66 iteration 0700/1263: training loss 0.290
Epoch 66 iteration 0720/1263: training loss 0.289
Epoch 66 iteration 0740/1263: training loss 0.289
Epoch 66 iteration 0760/1263: training loss 0.289
Epoch 66 iteration 0780/1263: training loss 0.289
Epoch 66 iteration 0800/1263: training loss 0.289
Epoch 66 iteration 0820/1263: training loss 0.290
Epoch 66 iteration 0840/1263: training loss 0.290
Epoch 66 iteration 0860/1263: training loss 0.289
Epoch 66 iteration 0880/1263: training loss 0.290
Epoch 66 iteration 0900/1263: training loss 0.291
Epoch 66 iteration 0920/1263: training loss 0.293
Epoch 66 iteration 0940/1263: training loss 0.294
Epoch 66 iteration 0960/1263: training loss 0.295
Epoch 66 iteration 0980/1263: training loss 0.295
Epoch 66 iteration 1000/1263: training loss 0.295
Epoch 66 iteration 1020/1263: training loss 0.295
Epoch 66 iteration 1040/1263: training loss 0.295
Epoch 66 iteration 1060/1263: training loss 0.295
Epoch 66 iteration 1080/1263: training loss 0.295
Epoch 66 iteration 1100/1263: training loss 0.296
Epoch 66 iteration 1120/1263: training loss 0.296
Epoch 66 iteration 1140/1263: training loss 0.296
Epoch 66 iteration 1160/1263: training loss 0.295
Epoch 66 iteration 1180/1263: training loss 0.295
Epoch 66 iteration 1200/1263: training loss 0.295
Epoch 66 iteration 1220/1263: training loss 0.295
Epoch 66 iteration 1240/1263: training loss 0.295
Epoch 66 iteration 1260/1263: training loss 0.294
Epoch 66 validation pixAcc: 0.794, mIoU: 0.449
Epoch 67 iteration 0020/1263: training loss 0.292
Epoch 67 iteration 0040/1263: training loss 0.305
Epoch 67 iteration 0060/1263: training loss 0.294
Epoch 67 iteration 0080/1263: training loss 0.297
Epoch 67 iteration 0100/1263: training loss 0.297
Epoch 67 iteration 0120/1263: training loss 0.302
Epoch 67 iteration 0140/1263: training loss 0.299
Epoch 67 iteration 0160/1263: training loss 0.296
Epoch 67 iteration 0180/1263: training loss 0.298
Epoch 67 iteration 0200/1263: training loss 0.298
Epoch 67 iteration 0220/1263: training loss 0.300
Epoch 67 iteration 0240/1263: training loss 0.299
Epoch 67 iteration 0260/1263: training loss 0.299
Epoch 67 iteration 0280/1263: training loss 0.297
Epoch 67 iteration 0300/1263: training loss 0.296
Epoch 67 iteration 0320/1263: training loss 0.295
Epoch 67 iteration 0340/1263: training loss 0.295
Epoch 67 iteration 0360/1263: training loss 0.294
Epoch 67 iteration 0380/1263: training loss 0.294
Epoch 67 iteration 0400/1263: training loss 0.295
Epoch 67 iteration 0420/1263: training loss 0.296
Epoch 67 iteration 0440/1263: training loss 0.295
Epoch 67 iteration 0460/1263: training loss 0.294
Epoch 67 iteration 0480/1263: training loss 0.293
Epoch 67 iteration 0500/1263: training loss 0.292
Epoch 67 iteration 0520/1263: training loss 0.293
Epoch 67 iteration 0540/1263: training loss 0.292
Epoch 67 iteration 0560/1263: training loss 0.292
Epoch 67 iteration 0580/1263: training loss 0.291
Epoch 67 iteration 0600/1263: training loss 0.290
Epoch 67 iteration 0620/1263: training loss 0.289
Epoch 67 iteration 0640/1263: training loss 0.290
Epoch 67 iteration 0660/1263: training loss 0.289
Epoch 67 iteration 0680/1263: training loss 0.289
Epoch 67 iteration 0700/1263: training loss 0.288
Epoch 67 iteration 0720/1263: training loss 0.288
Epoch 67 iteration 0740/1263: training loss 0.288
Epoch 67 iteration 0760/1263: training loss 0.288
Epoch 67 iteration 0780/1263: training loss 0.288
Epoch 67 iteration 0800/1263: training loss 0.288
Epoch 67 iteration 0820/1263: training loss 0.288
Epoch 67 iteration 0840/1263: training loss 0.289
Epoch 67 iteration 0860/1263: training loss 0.289
Epoch 67 iteration 0880/1263: training loss 0.289
Epoch 67 iteration 0900/1263: training loss 0.289
Epoch 67 iteration 0920/1263: training loss 0.289
Epoch 67 iteration 0940/1263: training loss 0.289
Epoch 67 iteration 0960/1263: training loss 0.289
Epoch 67 iteration 0980/1263: training loss 0.289
Epoch 67 iteration 1000/1263: training loss 0.289
Epoch 67 iteration 1020/1263: training loss 0.289
Epoch 67 iteration 1040/1263: training loss 0.289
Epoch 67 iteration 1060/1263: training loss 0.289
Epoch 67 iteration 1080/1263: training loss 0.289
Epoch 67 iteration 1100/1263: training loss 0.288
Epoch 67 iteration 1120/1263: training loss 0.288
Epoch 67 iteration 1140/1263: training loss 0.289
Epoch 67 iteration 1160/1263: training loss 0.290
Epoch 67 iteration 1180/1263: training loss 0.291
Epoch 67 iteration 1200/1263: training loss 0.291
Epoch 67 iteration 1220/1263: training loss 0.291
Epoch 67 iteration 1240/1263: training loss 0.292
Epoch 67 iteration 1260/1263: training loss 0.292
Epoch 67 validation pixAcc: 0.798, mIoU: 0.448
Epoch 68 iteration 0020/1263: training loss 0.292
Epoch 68 iteration 0040/1263: training loss 0.286
Epoch 68 iteration 0060/1263: training loss 0.282
Epoch 68 iteration 0080/1263: training loss 0.282
Epoch 68 iteration 0100/1263: training loss 0.280
Epoch 68 iteration 0120/1263: training loss 0.278
Epoch 68 iteration 0140/1263: training loss 0.278
Epoch 68 iteration 0160/1263: training loss 0.281
Epoch 68 iteration 0180/1263: training loss 0.284
Epoch 68 iteration 0200/1263: training loss 0.283
Epoch 68 iteration 0220/1263: training loss 0.282
Epoch 68 iteration 0240/1263: training loss 0.285
Epoch 68 iteration 0260/1263: training loss 0.283
Epoch 68 iteration 0280/1263: training loss 0.283
Epoch 68 iteration 0300/1263: training loss 0.284
Epoch 68 iteration 0320/1263: training loss 0.286
Epoch 68 iteration 0340/1263: training loss 0.286
Epoch 68 iteration 0360/1263: training loss 0.286
Epoch 68 iteration 0380/1263: training loss 0.285
Epoch 68 iteration 0400/1263: training loss 0.284
Epoch 68 iteration 0420/1263: training loss 0.287
Epoch 68 iteration 0440/1263: training loss 0.287
Epoch 68 iteration 0460/1263: training loss 0.287
Epoch 68 iteration 0480/1263: training loss 0.288
Epoch 68 iteration 0500/1263: training loss 0.289
Epoch 68 iteration 0520/1263: training loss 0.289
Epoch 68 iteration 0540/1263: training loss 0.290
Epoch 68 iteration 0560/1263: training loss 0.291
Epoch 68 iteration 0580/1263: training loss 0.290
Epoch 68 iteration 0600/1263: training loss 0.289
Epoch 68 iteration 0620/1263: training loss 0.289
Epoch 68 iteration 0640/1263: training loss 0.289
Epoch 68 iteration 0660/1263: training loss 0.288
Epoch 68 iteration 0680/1263: training loss 0.289
Epoch 68 iteration 0700/1263: training loss 0.288
Epoch 68 iteration 0720/1263: training loss 0.289
Epoch 68 iteration 0740/1263: training loss 0.290
Epoch 68 iteration 0760/1263: training loss 0.290
Epoch 68 iteration 0780/1263: training loss 0.290
Epoch 68 iteration 0800/1263: training loss 0.290
Epoch 68 iteration 0820/1263: training loss 0.290
Epoch 68 iteration 0840/1263: training loss 0.290
Epoch 68 iteration 0860/1263: training loss 0.290
Epoch 68 iteration 0880/1263: training loss 0.290
Epoch 68 iteration 0900/1263: training loss 0.290
Epoch 68 iteration 0920/1263: training loss 0.290
Epoch 68 iteration 0940/1263: training loss 0.289
Epoch 68 iteration 0960/1263: training loss 0.288
Epoch 68 iteration 0980/1263: training loss 0.288
Epoch 68 iteration 1000/1263: training loss 0.288
Epoch 68 iteration 1020/1263: training loss 0.288
Epoch 68 iteration 1040/1263: training loss 0.288
Epoch 68 iteration 1060/1263: training loss 0.288
Epoch 68 iteration 1080/1263: training loss 0.289
Epoch 68 iteration 1100/1263: training loss 0.289
Epoch 68 iteration 1120/1263: training loss 0.289
Epoch 68 iteration 1140/1263: training loss 0.289
Epoch 68 iteration 1160/1263: training loss 0.289
Epoch 68 iteration 1180/1263: training loss 0.290
Epoch 68 iteration 1200/1263: training loss 0.290
Epoch 68 iteration 1220/1263: training loss 0.290
Epoch 68 iteration 1240/1263: training loss 0.290
Epoch 68 iteration 1260/1263: training loss 0.290
Epoch 68 validation pixAcc: 0.802, mIoU: 0.456
Epoch 69 iteration 0020/1263: training loss 0.263
Epoch 69 iteration 0040/1263: training loss 0.275
Epoch 69 iteration 0060/1263: training loss 0.275
Epoch 69 iteration 0080/1263: training loss 0.277
Epoch 69 iteration 0100/1263: training loss 0.276
Epoch 69 iteration 0120/1263: training loss 0.277
Epoch 69 iteration 0140/1263: training loss 0.274
Epoch 69 iteration 0160/1263: training loss 0.273
Epoch 69 iteration 0180/1263: training loss 0.274
Epoch 69 iteration 0200/1263: training loss 0.272
Epoch 69 iteration 0220/1263: training loss 0.271
Epoch 69 iteration 0240/1263: training loss 0.271
Epoch 69 iteration 0260/1263: training loss 0.271
Epoch 69 iteration 0280/1263: training loss 0.272
Epoch 69 iteration 0300/1263: training loss 0.274
Epoch 69 iteration 0320/1263: training loss 0.272
Epoch 69 iteration 0340/1263: training loss 0.273
Epoch 69 iteration 0360/1263: training loss 0.274
Epoch 69 iteration 0380/1263: training loss 0.275
Epoch 69 iteration 0400/1263: training loss 0.274
Epoch 69 iteration 0420/1263: training loss 0.272
Epoch 69 iteration 0440/1263: training loss 0.273
Epoch 69 iteration 0460/1263: training loss 0.273
Epoch 69 iteration 0480/1263: training loss 0.272
Epoch 69 iteration 0500/1263: training loss 0.272
Epoch 69 iteration 0520/1263: training loss 0.272
Epoch 69 iteration 0540/1263: training loss 0.272
Epoch 69 iteration 0560/1263: training loss 0.271
Epoch 69 iteration 0580/1263: training loss 0.272
Epoch 69 iteration 0600/1263: training loss 0.272
Epoch 69 iteration 0620/1263: training loss 0.274
Epoch 69 iteration 0640/1263: training loss 0.274
Epoch 69 iteration 0660/1263: training loss 0.275
Epoch 69 iteration 0680/1263: training loss 0.275
Epoch 69 iteration 0700/1263: training loss 0.275
Epoch 69 iteration 0720/1263: training loss 0.275
Epoch 69 iteration 0740/1263: training loss 0.275
Epoch 69 iteration 0760/1263: training loss 0.275
Epoch 69 iteration 0780/1263: training loss 0.276
Epoch 69 iteration 0800/1263: training loss 0.275
Epoch 69 iteration 0820/1263: training loss 0.275
Epoch 69 iteration 0840/1263: training loss 0.275
Epoch 69 iteration 0860/1263: training loss 0.275
Epoch 69 iteration 0880/1263: training loss 0.275
Epoch 69 iteration 0900/1263: training loss 0.275
Epoch 69 iteration 0920/1263: training loss 0.275
Epoch 69 iteration 0940/1263: training loss 0.275
Epoch 69 iteration 0960/1263: training loss 0.274
Epoch 69 iteration 0980/1263: training loss 0.275
Epoch 69 iteration 1000/1263: training loss 0.275
Epoch 69 iteration 1020/1263: training loss 0.275
Epoch 69 iteration 1040/1263: training loss 0.275
Epoch 69 iteration 1060/1263: training loss 0.275
Epoch 69 iteration 1080/1263: training loss 0.275
Epoch 69 iteration 1100/1263: training loss 0.276
Epoch 69 iteration 1120/1263: training loss 0.276
Epoch 69 iteration 1140/1263: training loss 0.276
Epoch 69 iteration 1160/1263: training loss 0.276
Epoch 69 iteration 1180/1263: training loss 0.276
Epoch 69 iteration 1200/1263: training loss 0.276
Epoch 69 iteration 1220/1263: training loss 0.276
Epoch 69 iteration 1240/1263: training loss 0.276
Epoch 69 iteration 1260/1263: training loss 0.276
Epoch 69 validation pixAcc: 0.782, mIoU: 0.432
Epoch 70 iteration 0020/1263: training loss 0.315
Epoch 70 iteration 0040/1263: training loss 0.307
Epoch 70 iteration 0060/1263: training loss 0.296
Epoch 70 iteration 0080/1263: training loss 0.292
Epoch 70 iteration 0100/1263: training loss 0.293
Epoch 70 iteration 0120/1263: training loss 0.289
Epoch 70 iteration 0140/1263: training loss 0.285
Epoch 70 iteration 0160/1263: training loss 0.287
Epoch 70 iteration 0180/1263: training loss 0.286
Epoch 70 iteration 0200/1263: training loss 0.284
Epoch 70 iteration 0220/1263: training loss 0.284
Epoch 70 iteration 0240/1263: training loss 0.283
Epoch 70 iteration 0260/1263: training loss 0.284
Epoch 70 iteration 0280/1263: training loss 0.283
Epoch 70 iteration 0300/1263: training loss 0.284
Epoch 70 iteration 0320/1263: training loss 0.284
Epoch 70 iteration 0340/1263: training loss 0.282
Epoch 70 iteration 0360/1263: training loss 0.281
Epoch 70 iteration 0380/1263: training loss 0.279
Epoch 70 iteration 0400/1263: training loss 0.279
Epoch 70 iteration 0420/1263: training loss 0.278
Epoch 70 iteration 0440/1263: training loss 0.277
Epoch 70 iteration 0460/1263: training loss 0.277
Epoch 70 iteration 0480/1263: training loss 0.277
Epoch 70 iteration 0500/1263: training loss 0.276
Epoch 70 iteration 0520/1263: training loss 0.276
Epoch 70 iteration 0540/1263: training loss 0.274
Epoch 70 iteration 0560/1263: training loss 0.272
Epoch 70 iteration 0580/1263: training loss 0.273
Epoch 70 iteration 0600/1263: training loss 0.272
Epoch 70 iteration 0620/1263: training loss 0.271
Epoch 70 iteration 0640/1263: training loss 0.271
Epoch 70 iteration 0660/1263: training loss 0.272
Epoch 70 iteration 0680/1263: training loss 0.271
Epoch 70 iteration 0700/1263: training loss 0.271
Epoch 70 iteration 0720/1263: training loss 0.271
Epoch 70 iteration 0740/1263: training loss 0.271
Epoch 70 iteration 0760/1263: training loss 0.270
Epoch 70 iteration 0780/1263: training loss 0.270
Epoch 70 iteration 0800/1263: training loss 0.270
Epoch 70 iteration 0820/1263: training loss 0.269
Epoch 70 iteration 0840/1263: training loss 0.270
Epoch 70 iteration 0860/1263: training loss 0.270
Epoch 70 iteration 0880/1263: training loss 0.270
Epoch 70 iteration 0900/1263: training loss 0.270
Epoch 70 iteration 0920/1263: training loss 0.270
Epoch 70 iteration 0940/1263: training loss 0.270
Epoch 70 iteration 0960/1263: training loss 0.269
Epoch 70 iteration 0980/1263: training loss 0.269
Epoch 70 iteration 1000/1263: training loss 0.269
Epoch 70 iteration 1020/1263: training loss 0.269
Epoch 70 iteration 1040/1263: training loss 0.269
Epoch 70 iteration 1060/1263: training loss 0.269
Epoch 70 iteration 1080/1263: training loss 0.269
Epoch 70 iteration 1100/1263: training loss 0.270
Epoch 70 iteration 1120/1263: training loss 0.269
Epoch 70 iteration 1140/1263: training loss 0.270
Epoch 70 iteration 1160/1263: training loss 0.270
Epoch 70 iteration 1180/1264: training loss 0.270
Epoch 70 iteration 1200/1264: training loss 0.270
Epoch 70 iteration 1220/1264: training loss 0.270
Epoch 70 iteration 1240/1264: training loss 0.270
Epoch 70 iteration 1260/1264: training loss 0.270
Epoch 70 validation pixAcc: 0.799, mIoU: 0.449
Epoch 71 iteration 0020/1263: training loss 0.244
Epoch 71 iteration 0040/1263: training loss 0.252
Epoch 71 iteration 0060/1263: training loss 0.257
Epoch 71 iteration 0080/1263: training loss 0.260
Epoch 71 iteration 0100/1263: training loss 0.253
Epoch 71 iteration 0120/1263: training loss 0.253
Epoch 71 iteration 0140/1263: training loss 0.253
Epoch 71 iteration 0160/1263: training loss 0.254
Epoch 71 iteration 0180/1263: training loss 0.256
Epoch 71 iteration 0200/1263: training loss 0.257
Epoch 71 iteration 0220/1263: training loss 0.262
Epoch 71 iteration 0240/1263: training loss 0.267
Epoch 71 iteration 0260/1263: training loss 0.269
Epoch 71 iteration 0280/1263: training loss 0.270
Epoch 71 iteration 0300/1263: training loss 0.270
Epoch 71 iteration 0320/1263: training loss 0.271
Epoch 71 iteration 0340/1263: training loss 0.270
Epoch 71 iteration 0360/1263: training loss 0.271
Epoch 71 iteration 0380/1263: training loss 0.270
Epoch 71 iteration 0400/1263: training loss 0.269
Epoch 71 iteration 0420/1263: training loss 0.270
Epoch 71 iteration 0440/1263: training loss 0.270
Epoch 71 iteration 0460/1263: training loss 0.270
Epoch 71 iteration 0480/1263: training loss 0.271
Epoch 71 iteration 0500/1263: training loss 0.271
Epoch 71 iteration 0520/1263: training loss 0.271
Epoch 71 iteration 0540/1263: training loss 0.271
Epoch 71 iteration 0560/1263: training loss 0.270
Epoch 71 iteration 0580/1263: training loss 0.271
Epoch 71 iteration 0600/1263: training loss 0.272
Epoch 71 iteration 0620/1263: training loss 0.272
Epoch 71 iteration 0640/1263: training loss 0.272
Epoch 71 iteration 0660/1263: training loss 0.273
Epoch 71 iteration 0680/1263: training loss 0.273
Epoch 71 iteration 0700/1263: training loss 0.273
Epoch 71 iteration 0720/1263: training loss 0.273
Epoch 71 iteration 0740/1263: training loss 0.273
Epoch 71 iteration 0760/1263: training loss 0.273
Epoch 71 iteration 0780/1263: training loss 0.274
Epoch 71 iteration 0800/1263: training loss 0.274
Epoch 71 iteration 0820/1263: training loss 0.274
Epoch 71 iteration 0840/1263: training loss 0.274
Epoch 71 iteration 0860/1263: training loss 0.274
Epoch 71 iteration 0880/1263: training loss 0.274
Epoch 71 iteration 0900/1263: training loss 0.274
Epoch 71 iteration 0920/1263: training loss 0.274
Epoch 71 iteration 0940/1263: training loss 0.273
Epoch 71 iteration 0960/1263: training loss 0.273
Epoch 71 iteration 0980/1263: training loss 0.273
Epoch 71 iteration 1000/1263: training loss 0.273
Epoch 71 iteration 1020/1263: training loss 0.272
Epoch 71 iteration 1040/1263: training loss 0.272
Epoch 71 iteration 1060/1263: training loss 0.272
Epoch 71 iteration 1080/1263: training loss 0.271
Epoch 71 iteration 1100/1263: training loss 0.271
Epoch 71 iteration 1120/1263: training loss 0.271
Epoch 71 iteration 1140/1263: training loss 0.270
Epoch 71 iteration 1160/1263: training loss 0.270
Epoch 71 iteration 1180/1263: training loss 0.270
Epoch 71 iteration 1200/1263: training loss 0.270
Epoch 71 iteration 1220/1263: training loss 0.270
Epoch 71 iteration 1240/1263: training loss 0.270
Epoch 71 iteration 1260/1263: training loss 0.270
Epoch 71 validation pixAcc: 0.803, mIoU: 0.453
Epoch 72 iteration 0020/1263: training loss 0.282
Epoch 72 iteration 0040/1263: training loss 0.255
Epoch 72 iteration 0060/1263: training loss 0.256
Epoch 72 iteration 0080/1263: training loss 0.255
Epoch 72 iteration 0100/1263: training loss 0.256
Epoch 72 iteration 0120/1263: training loss 0.253
Epoch 72 iteration 0140/1263: training loss 0.255
Epoch 72 iteration 0160/1263: training loss 0.258
Epoch 72 iteration 0180/1263: training loss 0.259
Epoch 72 iteration 0200/1263: training loss 0.258
Epoch 72 iteration 0220/1263: training loss 0.260
Epoch 72 iteration 0240/1263: training loss 0.260
Epoch 72 iteration 0260/1263: training loss 0.261
Epoch 72 iteration 0280/1263: training loss 0.262
Epoch 72 iteration 0300/1263: training loss 0.262
Epoch 72 iteration 0320/1263: training loss 0.261
Epoch 72 iteration 0340/1263: training loss 0.261
Epoch 72 iteration 0360/1263: training loss 0.262
Epoch 72 iteration 0380/1263: training loss 0.261
Epoch 72 iteration 0400/1263: training loss 0.262
Epoch 72 iteration 0420/1263: training loss 0.262
Epoch 72 iteration 0440/1263: training loss 0.262
Epoch 72 iteration 0460/1263: training loss 0.261
Epoch 72 iteration 0480/1263: training loss 0.261
Epoch 72 iteration 0500/1263: training loss 0.262
Epoch 72 iteration 0520/1263: training loss 0.261
Epoch 72 iteration 0540/1263: training loss 0.262
Epoch 72 iteration 0560/1263: training loss 0.263
Epoch 72 iteration 0580/1263: training loss 0.261
Epoch 72 iteration 0600/1263: training loss 0.261
Epoch 72 iteration 0620/1263: training loss 0.260
Epoch 72 iteration 0640/1263: training loss 0.260
Epoch 72 iteration 0660/1263: training loss 0.259
Epoch 72 iteration 0680/1263: training loss 0.259
Epoch 72 iteration 0700/1263: training loss 0.259
Epoch 72 iteration 0720/1263: training loss 0.258
Epoch 72 iteration 0740/1263: training loss 0.259
Epoch 72 iteration 0760/1263: training loss 0.259
Epoch 72 iteration 0780/1263: training loss 0.258
Epoch 72 iteration 0800/1263: training loss 0.258
Epoch 72 iteration 0820/1263: training loss 0.257
Epoch 72 iteration 0840/1263: training loss 0.257
Epoch 72 iteration 0860/1263: training loss 0.257
Epoch 72 iteration 0880/1263: training loss 0.257
Epoch 72 iteration 0900/1263: training loss 0.257
Epoch 72 iteration 0920/1263: training loss 0.258
Epoch 72 iteration 0940/1263: training loss 0.258
Epoch 72 iteration 0960/1263: training loss 0.258
Epoch 72 iteration 0980/1263: training loss 0.258
Epoch 72 iteration 1000/1263: training loss 0.258
Epoch 72 iteration 1020/1263: training loss 0.258
Epoch 72 iteration 1040/1263: training loss 0.258
Epoch 72 iteration 1060/1263: training loss 0.258
Epoch 72 iteration 1080/1263: training loss 0.259
Epoch 72 iteration 1100/1263: training loss 0.259
Epoch 72 iteration 1120/1263: training loss 0.259
Epoch 72 iteration 1140/1263: training loss 0.259
Epoch 72 iteration 1160/1263: training loss 0.259
Epoch 72 iteration 1180/1263: training loss 0.259
Epoch 72 iteration 1200/1263: training loss 0.259
Epoch 72 iteration 1220/1263: training loss 0.259
Epoch 72 iteration 1240/1263: training loss 0.259
Epoch 72 iteration 1260/1263: training loss 0.259
Epoch 72 validation pixAcc: 0.802, mIoU: 0.454
Epoch 73 iteration 0020/1263: training loss 0.257
Epoch 73 iteration 0040/1263: training loss 0.254
Epoch 73 iteration 0060/1263: training loss 0.255
Epoch 73 iteration 0080/1263: training loss 0.255
Epoch 73 iteration 0100/1263: training loss 0.251
Epoch 73 iteration 0120/1263: training loss 0.252
Epoch 73 iteration 0140/1263: training loss 0.249
Epoch 73 iteration 0160/1263: training loss 0.249
Epoch 73 iteration 0180/1263: training loss 0.249
Epoch 73 iteration 0200/1263: training loss 0.251
Epoch 73 iteration 0220/1263: training loss 0.250
Epoch 73 iteration 0240/1263: training loss 0.250
Epoch 73 iteration 0260/1263: training loss 0.250
Epoch 73 iteration 0280/1263: training loss 0.250
Epoch 73 iteration 0300/1263: training loss 0.248
Epoch 73 iteration 0320/1263: training loss 0.248
Epoch 73 iteration 0340/1263: training loss 0.248
Epoch 73 iteration 0360/1263: training loss 0.248
Epoch 73 iteration 0380/1263: training loss 0.249
Epoch 73 iteration 0400/1263: training loss 0.248
Epoch 73 iteration 0420/1263: training loss 0.248
Epoch 73 iteration 0440/1263: training loss 0.247
Epoch 73 iteration 0460/1263: training loss 0.246
Epoch 73 iteration 0480/1263: training loss 0.246
Epoch 73 iteration 0500/1263: training loss 0.246
Epoch 73 iteration 0520/1263: training loss 0.246
Epoch 73 iteration 0540/1263: training loss 0.245
Epoch 73 iteration 0560/1263: training loss 0.245
Epoch 73 iteration 0580/1263: training loss 0.246
Epoch 73 iteration 0600/1263: training loss 0.246
Epoch 73 iteration 0620/1263: training loss 0.246
Epoch 73 iteration 0640/1263: training loss 0.246
Epoch 73 iteration 0660/1263: training loss 0.247
Epoch 73 iteration 0680/1263: training loss 0.246
Epoch 73 iteration 0700/1263: training loss 0.246
Epoch 73 iteration 0720/1263: training loss 0.246
Epoch 73 iteration 0740/1263: training loss 0.246
Epoch 73 iteration 0760/1263: training loss 0.245
Epoch 73 iteration 0780/1263: training loss 0.246
Epoch 73 iteration 0800/1263: training loss 0.246
Epoch 73 iteration 0820/1263: training loss 0.246
Epoch 73 iteration 0840/1263: training loss 0.246
Epoch 73 iteration 0860/1263: training loss 0.246
Epoch 73 iteration 0880/1263: training loss 0.247
Epoch 73 iteration 0900/1263: training loss 0.248
Epoch 73 iteration 0920/1263: training loss 0.248
Epoch 73 iteration 0940/1263: training loss 0.248
Epoch 73 iteration 0960/1263: training loss 0.249
Epoch 73 iteration 0980/1263: training loss 0.249
Epoch 73 iteration 1000/1263: training loss 0.250
Epoch 73 iteration 1020/1263: training loss 0.250
Epoch 73 iteration 1040/1263: training loss 0.250
Epoch 73 iteration 1060/1263: training loss 0.250
Epoch 73 iteration 1080/1263: training loss 0.251
Epoch 73 iteration 1100/1263: training loss 0.251
Epoch 73 iteration 1120/1263: training loss 0.251
Epoch 73 iteration 1140/1263: training loss 0.252
Epoch 73 iteration 1160/1263: training loss 0.252
Epoch 73 iteration 1180/1263: training loss 0.252
Epoch 73 iteration 1200/1263: training loss 0.253
Epoch 73 iteration 1220/1263: training loss 0.253
Epoch 73 iteration 1240/1263: training loss 0.253
Epoch 73 iteration 1260/1263: training loss 0.252
Epoch 73 validation pixAcc: 0.802, mIoU: 0.454
Epoch 74 iteration 0020/1263: training loss 0.242
Epoch 74 iteration 0040/1263: training loss 0.246
Epoch 74 iteration 0060/1263: training loss 0.250
Epoch 74 iteration 0080/1263: training loss 0.246
Epoch 74 iteration 0100/1263: training loss 0.246
Epoch 74 iteration 0120/1263: training loss 0.246
Epoch 74 iteration 0140/1263: training loss 0.244
Epoch 74 iteration 0160/1263: training loss 0.245
Epoch 74 iteration 0180/1263: training loss 0.245
Epoch 74 iteration 0200/1263: training loss 0.245
Epoch 74 iteration 0220/1263: training loss 0.244
Epoch 74 iteration 0240/1263: training loss 0.244
Epoch 74 iteration 0260/1263: training loss 0.245
Epoch 74 iteration 0280/1263: training loss 0.245
Epoch 74 iteration 0300/1263: training loss 0.245
Epoch 74 iteration 0320/1263: training loss 0.245
Epoch 74 iteration 0340/1263: training loss 0.245
Epoch 74 iteration 0360/1263: training loss 0.246
Epoch 74 iteration 0380/1263: training loss 0.246
Epoch 74 iteration 0400/1263: training loss 0.247
Epoch 74 iteration 0420/1263: training loss 0.248
Epoch 74 iteration 0440/1263: training loss 0.251
Epoch 74 iteration 0460/1263: training loss 0.253
Epoch 74 iteration 0480/1263: training loss 0.255
Epoch 74 iteration 0500/1263: training loss 0.257
Epoch 74 iteration 0520/1263: training loss 0.258
Epoch 74 iteration 0540/1263: training loss 0.259
Epoch 74 iteration 0560/1263: training loss 0.259
Epoch 74 iteration 0580/1263: training loss 0.259
Epoch 74 iteration 0600/1263: training loss 0.260
Epoch 74 iteration 0620/1263: training loss 0.261
Epoch 74 iteration 0640/1263: training loss 0.262
Epoch 74 iteration 0660/1263: training loss 0.262
Epoch 74 iteration 0680/1263: training loss 0.263
Epoch 74 iteration 0700/1263: training loss 0.263
Epoch 74 iteration 0720/1263: training loss 0.263
Epoch 74 iteration 0740/1263: training loss 0.263
Epoch 74 iteration 0760/1263: training loss 0.263
Epoch 74 iteration 0780/1263: training loss 0.263
Epoch 74 iteration 0800/1263: training loss 0.264
Epoch 74 iteration 0820/1263: training loss 0.263
Epoch 74 iteration 0840/1263: training loss 0.263
Epoch 74 iteration 0860/1263: training loss 0.263
Epoch 74 iteration 0880/1263: training loss 0.263
Epoch 74 iteration 0900/1263: training loss 0.263
Epoch 74 iteration 0920/1263: training loss 0.263
Epoch 74 iteration 0940/1263: training loss 0.264
Epoch 74 iteration 0960/1263: training loss 0.264
Epoch 74 iteration 0980/1263: training loss 0.264
Epoch 74 iteration 1000/1263: training loss 0.264
Epoch 74 iteration 1020/1263: training loss 0.264
Epoch 74 iteration 1040/1263: training loss 0.265
Epoch 74 iteration 1060/1263: training loss 0.265
Epoch 74 iteration 1080/1263: training loss 0.264
Epoch 74 iteration 1100/1263: training loss 0.264
Epoch 74 iteration 1120/1263: training loss 0.263
Epoch 74 iteration 1140/1263: training loss 0.263
Epoch 74 iteration 1160/1263: training loss 0.263
Epoch 74 iteration 1180/1263: training loss 0.263
Epoch 74 iteration 1200/1263: training loss 0.263
Epoch 74 iteration 1220/1263: training loss 0.263
Epoch 74 iteration 1240/1263: training loss 0.262
Epoch 74 iteration 1260/1263: training loss 0.263
Epoch 74 validation pixAcc: 0.798, mIoU: 0.454
Epoch 75 iteration 0020/1263: training loss 0.293
Epoch 75 iteration 0040/1263: training loss 0.276
Epoch 75 iteration 0060/1263: training loss 0.267
Epoch 75 iteration 0080/1263: training loss 0.264
Epoch 75 iteration 0100/1263: training loss 0.265
Epoch 75 iteration 0120/1263: training loss 0.264
Epoch 75 iteration 0140/1263: training loss 0.263
Epoch 75 iteration 0160/1263: training loss 0.260
Epoch 75 iteration 0180/1263: training loss 0.258
Epoch 75 iteration 0200/1263: training loss 0.259
Epoch 75 iteration 0220/1263: training loss 0.262
Epoch 75 iteration 0240/1263: training loss 0.260
Epoch 75 iteration 0260/1263: training loss 0.261
Epoch 75 iteration 0280/1263: training loss 0.261
Epoch 75 iteration 0300/1263: training loss 0.261
Epoch 75 iteration 0320/1263: training loss 0.261
Epoch 75 iteration 0340/1263: training loss 0.261
Epoch 75 iteration 0360/1263: training loss 0.260
Epoch 75 iteration 0380/1263: training loss 0.261
Epoch 75 iteration 0400/1263: training loss 0.260
Epoch 75 iteration 0420/1263: training loss 0.260
Epoch 75 iteration 0440/1263: training loss 0.260
Epoch 75 iteration 0460/1263: training loss 0.260
Epoch 75 iteration 0480/1263: training loss 0.260
Epoch 75 iteration 0500/1263: training loss 0.259
Epoch 75 iteration 0520/1263: training loss 0.258
Epoch 75 iteration 0540/1263: training loss 0.258
Epoch 75 iteration 0560/1263: training loss 0.257
Epoch 75 iteration 0580/1263: training loss 0.257
Epoch 75 iteration 0600/1263: training loss 0.257
Epoch 75 iteration 0620/1263: training loss 0.256
Epoch 75 iteration 0640/1263: training loss 0.256
Epoch 75 iteration 0660/1263: training loss 0.256
Epoch 75 iteration 0680/1263: training loss 0.255
Epoch 75 iteration 0700/1263: training loss 0.255
Epoch 75 iteration 0720/1263: training loss 0.254
Epoch 75 iteration 0740/1263: training loss 0.254
Epoch 75 iteration 0760/1263: training loss 0.253
Epoch 75 iteration 0780/1263: training loss 0.253
Epoch 75 iteration 0800/1263: training loss 0.253
Epoch 75 iteration 0820/1263: training loss 0.253
Epoch 75 iteration 0840/1263: training loss 0.253
Epoch 75 iteration 0860/1263: training loss 0.253
Epoch 75 iteration 0880/1263: training loss 0.253
Epoch 75 iteration 0900/1263: training loss 0.253
Epoch 75 iteration 0920/1263: training loss 0.253
Epoch 75 iteration 0940/1263: training loss 0.254
Epoch 75 iteration 0960/1263: training loss 0.254
Epoch 75 iteration 0980/1263: training loss 0.254
Epoch 75 iteration 1000/1263: training loss 0.254
Epoch 75 iteration 1020/1263: training loss 0.254
Epoch 75 iteration 1040/1263: training loss 0.254
Epoch 75 iteration 1060/1263: training loss 0.254
Epoch 75 iteration 1080/1263: training loss 0.254
Epoch 75 iteration 1100/1263: training loss 0.254
Epoch 75 iteration 1120/1263: training loss 0.254
Epoch 75 iteration 1140/1263: training loss 0.255
Epoch 75 iteration 1160/1263: training loss 0.254
Epoch 75 iteration 1180/1263: training loss 0.255
Epoch 75 iteration 1200/1263: training loss 0.255
Epoch 75 iteration 1220/1263: training loss 0.254
Epoch 75 iteration 1240/1263: training loss 0.255
Epoch 75 iteration 1260/1263: training loss 0.255
Epoch 75 validation pixAcc: 0.799, mIoU: 0.455
Epoch 76 iteration 0020/1263: training loss 0.247
Epoch 76 iteration 0040/1263: training loss 0.253
Epoch 76 iteration 0060/1263: training loss 0.248
Epoch 76 iteration 0080/1263: training loss 0.249
Epoch 76 iteration 0100/1263: training loss 0.244
Epoch 76 iteration 0120/1263: training loss 0.242
Epoch 76 iteration 0140/1263: training loss 0.242
Epoch 76 iteration 0160/1263: training loss 0.248
Epoch 76 iteration 0180/1263: training loss 0.247
Epoch 76 iteration 0200/1263: training loss 0.249
Epoch 76 iteration 0220/1263: training loss 0.250
Epoch 76 iteration 0240/1263: training loss 0.252
Epoch 76 iteration 0260/1263: training loss 0.254
Epoch 76 iteration 0280/1263: training loss 0.253
Epoch 76 iteration 0300/1263: training loss 0.254
Epoch 76 iteration 0320/1263: training loss 0.253
Epoch 76 iteration 0340/1263: training loss 0.253
Epoch 76 iteration 0360/1263: training loss 0.252
Epoch 76 iteration 0380/1263: training loss 0.251
Epoch 76 iteration 0400/1263: training loss 0.249
Epoch 76 iteration 0420/1263: training loss 0.250
Epoch 76 iteration 0440/1263: training loss 0.249
Epoch 76 iteration 0460/1263: training loss 0.249
Epoch 76 iteration 0480/1263: training loss 0.248
Epoch 76 iteration 0500/1263: training loss 0.248
Epoch 76 iteration 0520/1263: training loss 0.247
Epoch 76 iteration 0540/1263: training loss 0.247
Epoch 76 iteration 0560/1263: training loss 0.247
Epoch 76 iteration 0580/1263: training loss 0.247
Epoch 76 iteration 0600/1263: training loss 0.247
Epoch 76 iteration 0620/1263: training loss 0.247
Epoch 76 iteration 0640/1263: training loss 0.247
Epoch 76 iteration 0660/1263: training loss 0.246
Epoch 76 iteration 0680/1263: training loss 0.246
Epoch 76 iteration 0700/1263: training loss 0.247
Epoch 76 iteration 0720/1263: training loss 0.247
Epoch 76 iteration 0740/1263: training loss 0.247
Epoch 76 iteration 0760/1263: training loss 0.247
Epoch 76 iteration 0780/1263: training loss 0.247
Epoch 76 iteration 0800/1263: training loss 0.247
Epoch 76 iteration 0820/1263: training loss 0.247
Epoch 76 iteration 0840/1263: training loss 0.247
Epoch 76 iteration 0860/1263: training loss 0.247
Epoch 76 iteration 0880/1263: training loss 0.247
Epoch 76 iteration 0900/1263: training loss 0.247
Epoch 76 iteration 0920/1263: training loss 0.247
Epoch 76 iteration 0940/1263: training loss 0.247
Epoch 76 iteration 0960/1263: training loss 0.247
Epoch 76 iteration 0980/1263: training loss 0.246
Epoch 76 iteration 1000/1263: training loss 0.246
Epoch 76 iteration 1020/1263: training loss 0.246
Epoch 76 iteration 1040/1263: training loss 0.246
Epoch 76 iteration 1060/1263: training loss 0.246
Epoch 76 iteration 1080/1263: training loss 0.246
Epoch 76 iteration 1100/1263: training loss 0.246
Epoch 76 iteration 1120/1263: training loss 0.246
Epoch 76 iteration 1140/1263: training loss 0.246
Epoch 76 iteration 1160/1263: training loss 0.246
Epoch 76 iteration 1180/1263: training loss 0.246
Epoch 76 iteration 1200/1263: training loss 0.246
Epoch 76 iteration 1220/1263: training loss 0.246
Epoch 76 iteration 1240/1263: training loss 0.246
Epoch 76 iteration 1260/1263: training loss 0.246
Epoch 76 validation pixAcc: 0.802, mIoU: 0.459
Epoch 77 iteration 0020/1263: training loss 0.215
Epoch 77 iteration 0040/1263: training loss 0.230
Epoch 77 iteration 0060/1263: training loss 0.231
Epoch 77 iteration 0080/1263: training loss 0.232
Epoch 77 iteration 0100/1263: training loss 0.232
Epoch 77 iteration 0120/1263: training loss 0.228
Epoch 77 iteration 0140/1263: training loss 0.231
Epoch 77 iteration 0160/1263: training loss 0.233
Epoch 77 iteration 0180/1263: training loss 0.232
Epoch 77 iteration 0200/1263: training loss 0.234
Epoch 77 iteration 0220/1263: training loss 0.237
Epoch 77 iteration 0240/1263: training loss 0.237
Epoch 77 iteration 0260/1263: training loss 0.238
Epoch 77 iteration 0280/1263: training loss 0.240
Epoch 77 iteration 0300/1263: training loss 0.239
Epoch 77 iteration 0320/1263: training loss 0.241
Epoch 77 iteration 0340/1263: training loss 0.242
Epoch 77 iteration 0360/1263: training loss 0.242
Epoch 77 iteration 0380/1263: training loss 0.241
Epoch 77 iteration 0400/1263: training loss 0.240
Epoch 77 iteration 0420/1263: training loss 0.240
Epoch 77 iteration 0440/1263: training loss 0.240
Epoch 77 iteration 0460/1263: training loss 0.240
Epoch 77 iteration 0480/1263: training loss 0.240
Epoch 77 iteration 0500/1263: training loss 0.240
Epoch 77 iteration 0520/1263: training loss 0.240
Epoch 77 iteration 0540/1263: training loss 0.239
Epoch 77 iteration 0560/1263: training loss 0.240
Epoch 77 iteration 0580/1263: training loss 0.239
Epoch 77 iteration 0600/1263: training loss 0.239
Epoch 77 iteration 0620/1263: training loss 0.238
Epoch 77 iteration 0640/1263: training loss 0.238
Epoch 77 iteration 0660/1263: training loss 0.238
Epoch 77 iteration 0680/1263: training loss 0.238
Epoch 77 iteration 0700/1263: training loss 0.238
Epoch 77 iteration 0720/1263: training loss 0.238
Epoch 77 iteration 0740/1263: training loss 0.238
Epoch 77 iteration 0760/1263: training loss 0.238
Epoch 77 iteration 0780/1263: training loss 0.239
Epoch 77 iteration 0800/1263: training loss 0.239
Epoch 77 iteration 0820/1263: training loss 0.238
Epoch 77 iteration 0840/1263: training loss 0.238
Epoch 77 iteration 0860/1263: training loss 0.238
Epoch 77 iteration 0880/1263: training loss 0.238
Epoch 77 iteration 0900/1263: training loss 0.238
Epoch 77 iteration 0920/1263: training loss 0.238
Epoch 77 iteration 0940/1263: training loss 0.238
Epoch 77 iteration 0960/1263: training loss 0.238
Epoch 77 iteration 0980/1263: training loss 0.238
Epoch 77 iteration 1000/1263: training loss 0.238
Epoch 77 iteration 1020/1263: training loss 0.238
Epoch 77 iteration 1040/1263: training loss 0.238
Epoch 77 iteration 1060/1263: training loss 0.238
Epoch 77 iteration 1080/1263: training loss 0.238
Epoch 77 iteration 1100/1263: training loss 0.237
Epoch 77 iteration 1120/1263: training loss 0.237
Epoch 77 iteration 1140/1263: training loss 0.237
Epoch 77 iteration 1160/1263: training loss 0.237
Epoch 77 iteration 1180/1263: training loss 0.237
Epoch 77 iteration 1200/1263: training loss 0.237
Epoch 77 iteration 1220/1263: training loss 0.237
Epoch 77 iteration 1240/1263: training loss 0.237
Epoch 77 iteration 1260/1263: training loss 0.237
Epoch 77 validation pixAcc: 0.806, mIoU: 0.459
Epoch 78 iteration 0020/1263: training loss 0.223
Epoch 78 iteration 0040/1263: training loss 0.222
Epoch 78 iteration 0060/1263: training loss 0.220
Epoch 78 iteration 0080/1263: training loss 0.223
Epoch 78 iteration 0100/1263: training loss 0.223
Epoch 78 iteration 0120/1263: training loss 0.224
Epoch 78 iteration 0140/1263: training loss 0.223
Epoch 78 iteration 0160/1263: training loss 0.224
Epoch 78 iteration 0180/1263: training loss 0.224
Epoch 78 iteration 0200/1263: training loss 0.224
Epoch 78 iteration 0220/1263: training loss 0.224
Epoch 78 iteration 0240/1263: training loss 0.226
Epoch 78 iteration 0260/1263: training loss 0.226
Epoch 78 iteration 0280/1263: training loss 0.226
Epoch 78 iteration 0300/1263: training loss 0.226
Epoch 78 iteration 0320/1263: training loss 0.226
Epoch 78 iteration 0340/1263: training loss 0.226
Epoch 78 iteration 0360/1263: training loss 0.227
Epoch 78 iteration 0380/1263: training loss 0.227
Epoch 78 iteration 0400/1263: training loss 0.226
Epoch 78 iteration 0420/1263: training loss 0.226
Epoch 78 iteration 0440/1263: training loss 0.226
Epoch 78 iteration 0460/1263: training loss 0.226
Epoch 78 iteration 0480/1263: training loss 0.227
Epoch 78 iteration 0500/1263: training loss 0.227
Epoch 78 iteration 0520/1263: training loss 0.228
Epoch 78 iteration 0540/1263: training loss 0.229
Epoch 78 iteration 0560/1263: training loss 0.229
Epoch 78 iteration 0580/1263: training loss 0.229
Epoch 78 iteration 0600/1263: training loss 0.229
Epoch 78 iteration 0620/1263: training loss 0.230
Epoch 78 iteration 0640/1263: training loss 0.232
Epoch 78 iteration 0660/1263: training loss 0.232
Epoch 78 iteration 0680/1263: training loss 0.232
Epoch 78 iteration 0700/1263: training loss 0.233
Epoch 78 iteration 0720/1263: training loss 0.233
Epoch 78 iteration 0740/1263: training loss 0.234
Epoch 78 iteration 0760/1263: training loss 0.234
Epoch 78 iteration 0780/1263: training loss 0.235
Epoch 78 iteration 0800/1263: training loss 0.235
Epoch 78 iteration 0820/1263: training loss 0.235
Epoch 78 iteration 0840/1263: training loss 0.235
Epoch 78 iteration 0860/1263: training loss 0.235
Epoch 78 iteration 0880/1263: training loss 0.235
Epoch 78 iteration 0900/1263: training loss 0.235
Epoch 78 iteration 0920/1263: training loss 0.235
Epoch 78 iteration 0940/1263: training loss 0.235
Epoch 78 iteration 0960/1263: training loss 0.235
Epoch 78 iteration 0980/1263: training loss 0.235
Epoch 78 iteration 1000/1263: training loss 0.235
Epoch 78 iteration 1020/1263: training loss 0.235
Epoch 78 iteration 1040/1263: training loss 0.235
Epoch 78 iteration 1060/1263: training loss 0.235
Epoch 78 iteration 1080/1263: training loss 0.236
Epoch 78 iteration 1100/1263: training loss 0.235
Epoch 78 iteration 1120/1263: training loss 0.235
Epoch 78 iteration 1140/1263: training loss 0.235
Epoch 78 iteration 1160/1263: training loss 0.235
Epoch 78 iteration 1180/1264: training loss 0.236
Epoch 78 iteration 1200/1264: training loss 0.236
Epoch 78 iteration 1220/1264: training loss 0.237
Epoch 78 iteration 1240/1264: training loss 0.237
Epoch 78 iteration 1260/1264: training loss 0.238
Epoch 78 validation pixAcc: 0.801, mIoU: 0.456
Epoch 79 iteration 0020/1263: training loss 0.252
Epoch 79 iteration 0040/1263: training loss 0.256
Epoch 79 iteration 0060/1263: training loss 0.264
Epoch 79 iteration 0080/1263: training loss 0.261
Epoch 79 iteration 0100/1263: training loss 0.259
Epoch 79 iteration 0120/1263: training loss 0.260
Epoch 79 iteration 0140/1263: training loss 0.259
Epoch 79 iteration 0160/1263: training loss 0.263
Epoch 79 iteration 0180/1263: training loss 0.263
Epoch 79 iteration 0200/1263: training loss 0.260
Epoch 79 iteration 0220/1263: training loss 0.258
Epoch 79 iteration 0240/1263: training loss 0.255
Epoch 79 iteration 0260/1263: training loss 0.254
Epoch 79 iteration 0280/1263: training loss 0.253
Epoch 79 iteration 0300/1263: training loss 0.252
Epoch 79 iteration 0320/1263: training loss 0.253
Epoch 79 iteration 0340/1263: training loss 0.252
Epoch 79 iteration 0360/1263: training loss 0.250
Epoch 79 iteration 0380/1263: training loss 0.248
Epoch 79 iteration 0400/1263: training loss 0.247
Epoch 79 iteration 0420/1263: training loss 0.247
Epoch 79 iteration 0440/1263: training loss 0.245
Epoch 79 iteration 0460/1263: training loss 0.245
Epoch 79 iteration 0480/1263: training loss 0.245
Epoch 79 iteration 0500/1263: training loss 0.245
Epoch 79 iteration 0520/1263: training loss 0.243
Epoch 79 iteration 0540/1263: training loss 0.243
Epoch 79 iteration 0560/1263: training loss 0.244
Epoch 79 iteration 0580/1263: training loss 0.243
Epoch 79 iteration 0600/1263: training loss 0.243
Epoch 79 iteration 0620/1263: training loss 0.243
Epoch 79 iteration 0640/1263: training loss 0.244
Epoch 79 iteration 0660/1263: training loss 0.244
Epoch 79 iteration 0680/1263: training loss 0.243
Epoch 79 iteration 0700/1263: training loss 0.243
Epoch 79 iteration 0720/1263: training loss 0.242
Epoch 79 iteration 0740/1263: training loss 0.242
Epoch 79 iteration 0760/1263: training loss 0.242
Epoch 79 iteration 0780/1263: training loss 0.242
Epoch 79 iteration 0800/1263: training loss 0.242
Epoch 79 iteration 0820/1263: training loss 0.242
Epoch 79 iteration 0840/1263: training loss 0.242
Epoch 79 iteration 0860/1263: training loss 0.243
Epoch 79 iteration 0880/1263: training loss 0.243
Epoch 79 iteration 0900/1263: training loss 0.243
Epoch 79 iteration 0920/1263: training loss 0.242
Epoch 79 iteration 0940/1263: training loss 0.242
Epoch 79 iteration 0960/1263: training loss 0.242
Epoch 79 iteration 0980/1263: training loss 0.243
Epoch 79 iteration 1000/1263: training loss 0.242
Epoch 79 iteration 1020/1263: training loss 0.243
Epoch 79 iteration 1040/1263: training loss 0.243
Epoch 79 iteration 1060/1263: training loss 0.244
Epoch 79 iteration 1080/1263: training loss 0.244
Epoch 79 iteration 1100/1263: training loss 0.244
Epoch 79 iteration 1120/1263: training loss 0.245
Epoch 79 iteration 1140/1263: training loss 0.245
Epoch 79 iteration 1160/1263: training loss 0.245
Epoch 79 iteration 1180/1263: training loss 0.245
Epoch 79 iteration 1200/1263: training loss 0.246
Epoch 79 iteration 1220/1263: training loss 0.246
Epoch 79 iteration 1240/1263: training loss 0.247
Epoch 79 iteration 1260/1263: training loss 0.247
Epoch 79 validation pixAcc: 0.800, mIoU: 0.462
Epoch 80 iteration 0020/1263: training loss 0.272
Epoch 80 iteration 0040/1263: training loss 0.276
Epoch 80 iteration 0060/1263: training loss 0.268
Epoch 80 iteration 0080/1263: training loss 0.261
Epoch 80 iteration 0100/1263: training loss 0.260
Epoch 80 iteration 0120/1263: training loss 0.255
Epoch 80 iteration 0140/1263: training loss 0.252
Epoch 80 iteration 0160/1263: training loss 0.251
Epoch 80 iteration 0180/1263: training loss 0.249
Epoch 80 iteration 0200/1263: training loss 0.249
Epoch 80 iteration 0220/1263: training loss 0.247
Epoch 80 iteration 0240/1263: training loss 0.246
Epoch 80 iteration 0260/1263: training loss 0.244
Epoch 80 iteration 0280/1263: training loss 0.242
Epoch 80 iteration 0300/1263: training loss 0.241
Epoch 80 iteration 0320/1263: training loss 0.240
Epoch 80 iteration 0340/1263: training loss 0.239
Epoch 80 iteration 0360/1263: training loss 0.239
Epoch 80 iteration 0380/1263: training loss 0.238
Epoch 80 iteration 0400/1263: training loss 0.238
Epoch 80 iteration 0420/1263: training loss 0.238
Epoch 80 iteration 0440/1263: training loss 0.238
Epoch 80 iteration 0460/1263: training loss 0.237
Epoch 80 iteration 0480/1263: training loss 0.237
Epoch 80 iteration 0500/1263: training loss 0.237
Epoch 80 iteration 0520/1263: training loss 0.236
Epoch 80 iteration 0540/1263: training loss 0.236
Epoch 80 iteration 0560/1263: training loss 0.236
Epoch 80 iteration 0580/1263: training loss 0.235
Epoch 80 iteration 0600/1263: training loss 0.235
Epoch 80 iteration 0620/1263: training loss 0.235
Epoch 80 iteration 0640/1263: training loss 0.235
Epoch 80 iteration 0660/1263: training loss 0.235
Epoch 80 iteration 0680/1263: training loss 0.235
Epoch 80 iteration 0700/1263: training loss 0.235
Epoch 80 iteration 0720/1263: training loss 0.235
Epoch 80 iteration 0740/1263: training loss 0.235
Epoch 80 iteration 0760/1263: training loss 0.236
Epoch 80 iteration 0780/1263: training loss 0.235
Epoch 80 iteration 0800/1263: training loss 0.236
Epoch 80 iteration 0820/1263: training loss 0.236
Epoch 80 iteration 0840/1263: training loss 0.237
Epoch 80 iteration 0860/1263: training loss 0.236
Epoch 80 iteration 0880/1263: training loss 0.237
Epoch 80 iteration 0900/1263: training loss 0.237
Epoch 80 iteration 0920/1263: training loss 0.237
Epoch 80 iteration 0940/1263: training loss 0.238
Epoch 80 iteration 0960/1263: training loss 0.237
Epoch 80 iteration 0980/1263: training loss 0.237
Epoch 80 iteration 1000/1263: training loss 0.238
Epoch 80 iteration 1020/1263: training loss 0.238
Epoch 80 iteration 1040/1263: training loss 0.238
Epoch 80 iteration 1060/1263: training loss 0.238
Epoch 80 iteration 1080/1263: training loss 0.238
Epoch 80 iteration 1100/1263: training loss 0.238
Epoch 80 iteration 1120/1263: training loss 0.238
Epoch 80 iteration 1140/1263: training loss 0.238
Epoch 80 iteration 1160/1263: training loss 0.238
Epoch 80 iteration 1180/1263: training loss 0.238
Epoch 80 iteration 1200/1263: training loss 0.238
Epoch 80 iteration 1220/1263: training loss 0.237
Epoch 80 iteration 1240/1263: training loss 0.238
Epoch 80 iteration 1260/1263: training loss 0.238
Epoch 80 validation pixAcc: 0.803, mIoU: 0.456
Epoch 81 iteration 0020/1263: training loss 0.227
Epoch 81 iteration 0040/1263: training loss 0.238
Epoch 81 iteration 0060/1263: training loss 0.234
Epoch 81 iteration 0080/1263: training loss 0.232
Epoch 81 iteration 0100/1263: training loss 0.231
Epoch 81 iteration 0120/1263: training loss 0.228
Epoch 81 iteration 0140/1263: training loss 0.228
Epoch 81 iteration 0160/1263: training loss 0.229
Epoch 81 iteration 0180/1263: training loss 0.227
Epoch 81 iteration 0200/1263: training loss 0.229
Epoch 81 iteration 0220/1263: training loss 0.231
Epoch 81 iteration 0240/1263: training loss 0.230
Epoch 81 iteration 0260/1263: training loss 0.231
Epoch 81 iteration 0280/1263: training loss 0.230
Epoch 81 iteration 0300/1263: training loss 0.231
Epoch 81 iteration 0320/1263: training loss 0.231
Epoch 81 iteration 0340/1263: training loss 0.231
Epoch 81 iteration 0360/1263: training loss 0.230
Epoch 81 iteration 0380/1263: training loss 0.228
Epoch 81 iteration 0400/1263: training loss 0.228
Epoch 81 iteration 0420/1263: training loss 0.227
Epoch 81 iteration 0440/1263: training loss 0.227
Epoch 81 iteration 0460/1263: training loss 0.227
Epoch 81 iteration 0480/1263: training loss 0.227
Epoch 81 iteration 0500/1263: training loss 0.228
Epoch 81 iteration 0520/1263: training loss 0.227
Epoch 81 iteration 0540/1263: training loss 0.228
Epoch 81 iteration 0560/1263: training loss 0.227
Epoch 81 iteration 0580/1263: training loss 0.227
Epoch 81 iteration 0600/1263: training loss 0.227
Epoch 81 iteration 0620/1263: training loss 0.227
Epoch 81 iteration 0640/1263: training loss 0.227
Epoch 81 iteration 0660/1263: training loss 0.227
Epoch 81 iteration 0680/1263: training loss 0.227
Epoch 81 iteration 0700/1263: training loss 0.227
Epoch 81 iteration 0720/1263: training loss 0.227
Epoch 81 iteration 0740/1263: training loss 0.227
Epoch 81 iteration 0760/1263: training loss 0.227
Epoch 81 iteration 0780/1263: training loss 0.228
Epoch 81 iteration 0800/1263: training loss 0.228
Epoch 81 iteration 0820/1263: training loss 0.228
Epoch 81 iteration 0840/1263: training loss 0.228
Epoch 81 iteration 0860/1263: training loss 0.228
Epoch 81 iteration 0880/1263: training loss 0.228
Epoch 81 iteration 0900/1263: training loss 0.228
Epoch 81 iteration 0920/1263: training loss 0.228
Epoch 81 iteration 0940/1263: training loss 0.228
Epoch 81 iteration 0960/1263: training loss 0.228
Epoch 81 iteration 0980/1263: training loss 0.228
Epoch 81 iteration 1000/1263: training loss 0.228
Epoch 81 iteration 1020/1263: training loss 0.228
Epoch 81 iteration 1040/1263: training loss 0.228
Epoch 81 iteration 1060/1263: training loss 0.227
Epoch 81 iteration 1080/1263: training loss 0.228
Epoch 81 iteration 1100/1263: training loss 0.228
Epoch 81 iteration 1120/1263: training loss 0.229
Epoch 81 iteration 1140/1263: training loss 0.229
Epoch 81 iteration 1160/1263: training loss 0.229
Epoch 81 iteration 1180/1263: training loss 0.229
Epoch 81 iteration 1200/1263: training loss 0.229
Epoch 81 iteration 1220/1263: training loss 0.229
Epoch 81 iteration 1240/1263: training loss 0.229
Epoch 81 iteration 1260/1263: training loss 0.228
Epoch 81 validation pixAcc: 0.804, mIoU: 0.470
Epoch 82 iteration 0020/1263: training loss 0.198
Epoch 82 iteration 0040/1263: training loss 0.207
Epoch 82 iteration 0060/1263: training loss 0.206
Epoch 82 iteration 0080/1263: training loss 0.212
Epoch 82 iteration 0100/1263: training loss 0.212
Epoch 82 iteration 0120/1263: training loss 0.217
Epoch 82 iteration 0140/1263: training loss 0.219
Epoch 82 iteration 0160/1263: training loss 0.218
Epoch 82 iteration 0180/1263: training loss 0.219
Epoch 82 iteration 0200/1263: training loss 0.220
Epoch 82 iteration 0220/1263: training loss 0.223
Epoch 82 iteration 0240/1263: training loss 0.225
Epoch 82 iteration 0260/1263: training loss 0.225
Epoch 82 iteration 0280/1263: training loss 0.225
Epoch 82 iteration 0300/1263: training loss 0.224
Epoch 82 iteration 0320/1263: training loss 0.225
Epoch 82 iteration 0340/1263: training loss 0.225
Epoch 82 iteration 0360/1263: training loss 0.228
Epoch 82 iteration 0380/1263: training loss 0.228
Epoch 82 iteration 0400/1263: training loss 0.227
Epoch 82 iteration 0420/1263: training loss 0.229
Epoch 82 iteration 0440/1263: training loss 0.229
Epoch 82 iteration 0460/1263: training loss 0.229
Epoch 82 iteration 0480/1263: training loss 0.230
Epoch 82 iteration 0500/1263: training loss 0.231
Epoch 82 iteration 0520/1263: training loss 0.231
Epoch 82 iteration 0540/1263: training loss 0.231
Epoch 82 iteration 0560/1263: training loss 0.231
Epoch 82 iteration 0580/1263: training loss 0.231
Epoch 82 iteration 0600/1263: training loss 0.231
Epoch 82 iteration 0620/1263: training loss 0.231
Epoch 82 iteration 0640/1263: training loss 0.231
Epoch 82 iteration 0660/1263: training loss 0.231
Epoch 82 iteration 0680/1263: training loss 0.231
Epoch 82 iteration 0700/1263: training loss 0.231
Epoch 82 iteration 0720/1263: training loss 0.232
Epoch 82 iteration 0740/1263: training loss 0.232
Epoch 82 iteration 0760/1263: training loss 0.231
Epoch 82 iteration 0780/1263: training loss 0.232
Epoch 82 iteration 0800/1263: training loss 0.232
Epoch 82 iteration 0820/1263: training loss 0.232
Epoch 82 iteration 0840/1263: training loss 0.231
Epoch 82 iteration 0860/1263: training loss 0.231
Epoch 82 iteration 0880/1263: training loss 0.231
Epoch 82 iteration 0900/1263: training loss 0.232
Epoch 82 iteration 0920/1263: training loss 0.232
Epoch 82 iteration 0940/1263: training loss 0.233
Epoch 82 iteration 0960/1263: training loss 0.233
Epoch 82 iteration 0980/1263: training loss 0.233
Epoch 82 iteration 1000/1263: training loss 0.233
Epoch 82 iteration 1020/1263: training loss 0.233
Epoch 82 iteration 1040/1263: training loss 0.233
Epoch 82 iteration 1060/1263: training loss 0.233
Epoch 82 iteration 1080/1263: training loss 0.233
Epoch 82 iteration 1100/1263: training loss 0.233
Epoch 82 iteration 1120/1263: training loss 0.233
Epoch 82 iteration 1140/1263: training loss 0.233
Epoch 82 iteration 1160/1263: training loss 0.234
Epoch 82 iteration 1180/1263: training loss 0.234
Epoch 82 iteration 1200/1263: training loss 0.234
Epoch 82 iteration 1220/1263: training loss 0.234
Epoch 82 iteration 1240/1263: training loss 0.234
Epoch 82 iteration 1260/1263: training loss 0.234
Epoch 82 validation pixAcc: 0.800, mIoU: 0.462
Epoch 83 iteration 0020/1263: training loss 0.210
Epoch 83 iteration 0040/1263: training loss 0.212
Epoch 83 iteration 0060/1263: training loss 0.214
Epoch 83 iteration 0080/1263: training loss 0.222
Epoch 83 iteration 0100/1263: training loss 0.222
Epoch 83 iteration 0120/1263: training loss 0.222
Epoch 83 iteration 0140/1263: training loss 0.220
Epoch 83 iteration 0160/1263: training loss 0.222
Epoch 83 iteration 0180/1263: training loss 0.224
Epoch 83 iteration 0200/1263: training loss 0.227
Epoch 83 iteration 0220/1263: training loss 0.226
Epoch 83 iteration 0240/1263: training loss 0.225
Epoch 83 iteration 0260/1263: training loss 0.225
Epoch 83 iteration 0280/1263: training loss 0.224
Epoch 83 iteration 0300/1263: training loss 0.224
Epoch 83 iteration 0320/1263: training loss 0.224
Epoch 83 iteration 0340/1263: training loss 0.226
Epoch 83 iteration 0360/1263: training loss 0.226
Epoch 83 iteration 0380/1263: training loss 0.225
Epoch 83 iteration 0400/1263: training loss 0.224
Epoch 83 iteration 0420/1263: training loss 0.225
Epoch 83 iteration 0440/1263: training loss 0.225
Epoch 83 iteration 0460/1263: training loss 0.225
Epoch 83 iteration 0480/1263: training loss 0.225
Epoch 83 iteration 0500/1263: training loss 0.227
Epoch 83 iteration 0520/1263: training loss 0.227
Epoch 83 iteration 0540/1263: training loss 0.227
Epoch 83 iteration 0560/1263: training loss 0.227
Epoch 83 iteration 0580/1263: training loss 0.228
Epoch 83 iteration 0600/1263: training loss 0.228
Epoch 83 iteration 0620/1263: training loss 0.229
Epoch 83 iteration 0640/1263: training loss 0.228
Epoch 83 iteration 0660/1263: training loss 0.229
Epoch 83 iteration 0680/1263: training loss 0.229
Epoch 83 iteration 0700/1263: training loss 0.229
Epoch 83 iteration 0720/1263: training loss 0.230
Epoch 83 iteration 0740/1263: training loss 0.230
Epoch 83 iteration 0760/1263: training loss 0.230
Epoch 83 iteration 0780/1263: training loss 0.230
Epoch 83 iteration 0800/1263: training loss 0.231
Epoch 83 iteration 0820/1263: training loss 0.231
Epoch 83 iteration 0840/1263: training loss 0.231
Epoch 83 iteration 0860/1263: training loss 0.230
Epoch 83 iteration 0880/1263: training loss 0.230
Epoch 83 iteration 0900/1263: training loss 0.230
Epoch 83 iteration 0920/1263: training loss 0.230
Epoch 83 iteration 0940/1263: training loss 0.231
Epoch 83 iteration 0960/1263: training loss 0.230
Epoch 83 iteration 0980/1263: training loss 0.230
Epoch 83 iteration 1000/1263: training loss 0.231
Epoch 83 iteration 1020/1263: training loss 0.231
Epoch 83 iteration 1040/1263: training loss 0.231
Epoch 83 iteration 1060/1263: training loss 0.231
Epoch 83 iteration 1080/1263: training loss 0.231
Epoch 83 iteration 1100/1263: training loss 0.231
Epoch 83 iteration 1120/1263: training loss 0.231
Epoch 83 iteration 1140/1263: training loss 0.231
Epoch 83 iteration 1160/1263: training loss 0.231
Epoch 83 iteration 1180/1263: training loss 0.231
Epoch 83 iteration 1200/1263: training loss 0.232
Epoch 83 iteration 1220/1263: training loss 0.232
Epoch 83 iteration 1240/1263: training loss 0.232
Epoch 83 iteration 1260/1263: training loss 0.232
Epoch 83 validation pixAcc: 0.802, mIoU: 0.459
Epoch 84 iteration 0020/1263: training loss 0.269
Epoch 84 iteration 0040/1263: training loss 0.247
Epoch 84 iteration 0060/1263: training loss 0.240
Epoch 84 iteration 0080/1263: training loss 0.237
Epoch 84 iteration 0100/1263: training loss 0.229
Epoch 84 iteration 0120/1263: training loss 0.230
Epoch 84 iteration 0140/1263: training loss 0.228
Epoch 84 iteration 0160/1263: training loss 0.228
Epoch 84 iteration 0180/1263: training loss 0.232
Epoch 84 iteration 0200/1263: training loss 0.232
Epoch 84 iteration 0220/1263: training loss 0.229
Epoch 84 iteration 0240/1263: training loss 0.230
Epoch 84 iteration 0260/1263: training loss 0.229
Epoch 84 iteration 0280/1263: training loss 0.230
Epoch 84 iteration 0300/1263: training loss 0.230
Epoch 84 iteration 0320/1263: training loss 0.230
Epoch 84 iteration 0340/1263: training loss 0.230
Epoch 84 iteration 0360/1263: training loss 0.230
Epoch 84 iteration 0380/1263: training loss 0.230
Epoch 84 iteration 0400/1263: training loss 0.231
Epoch 84 iteration 0420/1263: training loss 0.232
Epoch 84 iteration 0440/1263: training loss 0.233
Epoch 84 iteration 0460/1263: training loss 0.234
Epoch 84 iteration 0480/1263: training loss 0.234
Epoch 84 iteration 0500/1263: training loss 0.235
Epoch 84 iteration 0520/1263: training loss 0.236
Epoch 84 iteration 0540/1263: training loss 0.236
Epoch 84 iteration 0560/1263: training loss 0.236
Epoch 84 iteration 0580/1263: training loss 0.237
Epoch 84 iteration 0600/1263: training loss 0.237
Epoch 84 iteration 0620/1263: training loss 0.237
Epoch 84 iteration 0640/1263: training loss 0.236
Epoch 84 iteration 0660/1263: training loss 0.237
Epoch 84 iteration 0680/1263: training loss 0.236
Epoch 84 iteration 0700/1263: training loss 0.236
Epoch 84 iteration 0720/1263: training loss 0.236
Epoch 84 iteration 0740/1263: training loss 0.237
Epoch 84 iteration 0760/1263: training loss 0.237
Epoch 84 iteration 0780/1263: training loss 0.238
Epoch 84 iteration 0800/1263: training loss 0.238
Epoch 84 iteration 0820/1263: training loss 0.237
Epoch 84 iteration 0840/1263: training loss 0.237
Epoch 84 iteration 0860/1263: training loss 0.236
Epoch 84 iteration 0880/1263: training loss 0.236
Epoch 84 iteration 0900/1263: training loss 0.236
Epoch 84 iteration 0920/1263: training loss 0.237
Epoch 84 iteration 0940/1263: training loss 0.237
Epoch 84 iteration 0960/1263: training loss 0.236
Epoch 84 iteration 0980/1263: training loss 0.236
Epoch 84 iteration 1000/1263: training loss 0.236
Epoch 84 iteration 1020/1263: training loss 0.236
Epoch 84 iteration 1040/1263: training loss 0.236
Epoch 84 iteration 1060/1263: training loss 0.236
Epoch 84 iteration 1080/1263: training loss 0.236
Epoch 84 iteration 1100/1263: training loss 0.236
Epoch 84 iteration 1120/1263: training loss 0.236
Epoch 84 iteration 1140/1263: training loss 0.236
Epoch 84 iteration 1160/1263: training loss 0.236
Epoch 84 iteration 1180/1263: training loss 0.235
Epoch 84 iteration 1200/1263: training loss 0.235
Epoch 84 iteration 1220/1263: training loss 0.235
Epoch 84 iteration 1240/1263: training loss 0.235
Epoch 84 iteration 1260/1263: training loss 0.235
Epoch 84 validation pixAcc: 0.805, mIoU: 0.458
Epoch 85 iteration 0020/1263: training loss 0.228
Epoch 85 iteration 0040/1263: training loss 0.220
Epoch 85 iteration 0060/1263: training loss 0.216
Epoch 85 iteration 0080/1263: training loss 0.219
Epoch 85 iteration 0100/1263: training loss 0.219
Epoch 85 iteration 0120/1263: training loss 0.220
Epoch 85 iteration 0140/1263: training loss 0.221
Epoch 85 iteration 0160/1263: training loss 0.222
Epoch 85 iteration 0180/1263: training loss 0.226
Epoch 85 iteration 0200/1263: training loss 0.226
Epoch 85 iteration 0220/1263: training loss 0.224
Epoch 85 iteration 0240/1263: training loss 0.224
Epoch 85 iteration 0260/1263: training loss 0.223
Epoch 85 iteration 0280/1263: training loss 0.222
Epoch 85 iteration 0300/1263: training loss 0.223
Epoch 85 iteration 0320/1263: training loss 0.222
Epoch 85 iteration 0340/1263: training loss 0.223
Epoch 85 iteration 0360/1263: training loss 0.223
Epoch 85 iteration 0380/1263: training loss 0.222
Epoch 85 iteration 0400/1263: training loss 0.222
Epoch 85 iteration 0420/1263: training loss 0.221
Epoch 85 iteration 0440/1263: training loss 0.221
Epoch 85 iteration 0460/1263: training loss 0.221
Epoch 85 iteration 0480/1263: training loss 0.222
Epoch 85 iteration 0500/1263: training loss 0.222
Epoch 85 iteration 0520/1263: training loss 0.223
Epoch 85 iteration 0540/1263: training loss 0.223
Epoch 85 iteration 0560/1263: training loss 0.224
Epoch 85 iteration 0580/1263: training loss 0.224
Epoch 85 iteration 0600/1263: training loss 0.224
Epoch 85 iteration 0620/1263: training loss 0.224
Epoch 85 iteration 0640/1263: training loss 0.224
Epoch 85 iteration 0660/1263: training loss 0.224
Epoch 85 iteration 0680/1263: training loss 0.224
Epoch 85 iteration 0700/1263: training loss 0.224
Epoch 85 iteration 0720/1263: training loss 0.224
Epoch 85 iteration 0740/1263: training loss 0.224
Epoch 85 iteration 0760/1263: training loss 0.224
Epoch 85 iteration 0780/1263: training loss 0.224
Epoch 85 iteration 0800/1263: training loss 0.224
Epoch 85 iteration 0820/1263: training loss 0.224
Epoch 85 iteration 0840/1263: training loss 0.224
Epoch 85 iteration 0860/1263: training loss 0.224
Epoch 85 iteration 0880/1263: training loss 0.224
Epoch 85 iteration 0900/1263: training loss 0.224
Epoch 85 iteration 0920/1263: training loss 0.224
Epoch 85 iteration 0940/1263: training loss 0.224
Epoch 85 iteration 0960/1263: training loss 0.225
Epoch 85 iteration 0980/1263: training loss 0.225
Epoch 85 iteration 1000/1263: training loss 0.225
Epoch 85 iteration 1020/1263: training loss 0.225
Epoch 85 iteration 1040/1263: training loss 0.226
Epoch 85 iteration 1060/1263: training loss 0.225
Epoch 85 iteration 1080/1263: training loss 0.226
Epoch 85 iteration 1100/1263: training loss 0.226
Epoch 85 iteration 1120/1263: training loss 0.226
Epoch 85 iteration 1140/1263: training loss 0.225
Epoch 85 iteration 1160/1263: training loss 0.225
Epoch 85 iteration 1180/1263: training loss 0.226
Epoch 85 iteration 1200/1263: training loss 0.226
Epoch 85 iteration 1220/1263: training loss 0.226
Epoch 85 iteration 1240/1263: training loss 0.226
Epoch 85 iteration 1260/1263: training loss 0.226
Epoch 85 validation pixAcc: 0.805, mIoU: 0.470
Epoch 86 iteration 0020/1263: training loss 0.218
Epoch 86 iteration 0040/1263: training loss 0.210
Epoch 86 iteration 0060/1263: training loss 0.207
Epoch 86 iteration 0080/1263: training loss 0.209
Epoch 86 iteration 0100/1263: training loss 0.212
Epoch 86 iteration 0120/1263: training loss 0.212
Epoch 86 iteration 0140/1263: training loss 0.208
Epoch 86 iteration 0160/1263: training loss 0.207
Epoch 86 iteration 0180/1263: training loss 0.207
Epoch 86 iteration 0200/1263: training loss 0.208
Epoch 86 iteration 0220/1263: training loss 0.209
Epoch 86 iteration 0240/1263: training loss 0.209
Epoch 86 iteration 0260/1263: training loss 0.211
Epoch 86 iteration 0280/1263: training loss 0.212
Epoch 86 iteration 0300/1263: training loss 0.213
Epoch 86 iteration 0320/1263: training loss 0.213
Epoch 86 iteration 0340/1263: training loss 0.213
Epoch 86 iteration 0360/1263: training loss 0.214
Epoch 86 iteration 0380/1263: training loss 0.214
Epoch 86 iteration 0400/1263: training loss 0.214
Epoch 86 iteration 0420/1263: training loss 0.214
Epoch 86 iteration 0440/1263: training loss 0.215
Epoch 86 iteration 0460/1263: training loss 0.215
Epoch 86 iteration 0480/1263: training loss 0.214
Epoch 86 iteration 0500/1263: training loss 0.215
Epoch 86 iteration 0520/1263: training loss 0.215
Epoch 86 iteration 0540/1263: training loss 0.215
Epoch 86 iteration 0560/1263: training loss 0.215
Epoch 86 iteration 0580/1263: training loss 0.215
Epoch 86 iteration 0600/1263: training loss 0.215
Epoch 86 iteration 0620/1263: training loss 0.215
Epoch 86 iteration 0640/1263: training loss 0.216
Epoch 86 iteration 0660/1263: training loss 0.216
Epoch 86 iteration 0680/1263: training loss 0.216
Epoch 86 iteration 0700/1263: training loss 0.216
Epoch 86 iteration 0720/1263: training loss 0.217
Epoch 86 iteration 0740/1263: training loss 0.216
Epoch 86 iteration 0760/1263: training loss 0.216
Epoch 86 iteration 0780/1263: training loss 0.218
Epoch 86 iteration 0800/1263: training loss 0.218
Epoch 86 iteration 0820/1263: training loss 0.218
Epoch 86 iteration 0840/1263: training loss 0.217
Epoch 86 iteration 0860/1263: training loss 0.217
Epoch 86 iteration 0880/1263: training loss 0.217
Epoch 86 iteration 0900/1263: training loss 0.218
Epoch 86 iteration 0920/1263: training loss 0.218
Epoch 86 iteration 0940/1263: training loss 0.218
Epoch 86 iteration 0960/1263: training loss 0.218
Epoch 86 iteration 0980/1263: training loss 0.219
Epoch 86 iteration 1000/1263: training loss 0.219
Epoch 86 iteration 1020/1263: training loss 0.219
Epoch 86 iteration 1040/1263: training loss 0.220
Epoch 86 iteration 1060/1263: training loss 0.221
Epoch 86 iteration 1080/1263: training loss 0.221
Epoch 86 iteration 1100/1263: training loss 0.221
Epoch 86 iteration 1120/1263: training loss 0.221
Epoch 86 iteration 1140/1263: training loss 0.222
Epoch 86 iteration 1160/1263: training loss 0.222
Epoch 86 iteration 1180/1264: training loss 0.222
Epoch 86 iteration 1200/1264: training loss 0.222
Epoch 86 iteration 1220/1264: training loss 0.222
Epoch 86 iteration 1240/1264: training loss 0.222
Epoch 86 iteration 1260/1264: training loss 0.222
Epoch 86 validation pixAcc: 0.806, mIoU: 0.471
Epoch 87 iteration 0020/1263: training loss 0.209
Epoch 87 iteration 0040/1263: training loss 0.219
Epoch 87 iteration 0060/1263: training loss 0.214
Epoch 87 iteration 0080/1263: training loss 0.216
Epoch 87 iteration 0100/1263: training loss 0.214
Epoch 87 iteration 0120/1263: training loss 0.216
Epoch 87 iteration 0140/1263: training loss 0.215
Epoch 87 iteration 0160/1263: training loss 0.216
Epoch 87 iteration 0180/1263: training loss 0.216
Epoch 87 iteration 0200/1263: training loss 0.221
Epoch 87 iteration 0220/1263: training loss 0.221
Epoch 87 iteration 0240/1263: training loss 0.221
Epoch 87 iteration 0260/1263: training loss 0.221
Epoch 87 iteration 0280/1263: training loss 0.220
Epoch 87 iteration 0300/1263: training loss 0.220
Epoch 87 iteration 0320/1263: training loss 0.219
Epoch 87 iteration 0340/1263: training loss 0.220
Epoch 87 iteration 0360/1263: training loss 0.219
Epoch 87 iteration 0380/1263: training loss 0.218
Epoch 87 iteration 0400/1263: training loss 0.218
Epoch 87 iteration 0420/1263: training loss 0.217
Epoch 87 iteration 0440/1263: training loss 0.217
Epoch 87 iteration 0460/1263: training loss 0.217
Epoch 87 iteration 0480/1263: training loss 0.218
Epoch 87 iteration 0500/1263: training loss 0.219
Epoch 87 iteration 0520/1263: training loss 0.220
Epoch 87 iteration 0540/1263: training loss 0.220
Epoch 87 iteration 0560/1263: training loss 0.221
Epoch 87 iteration 0580/1263: training loss 0.221
Epoch 87 iteration 0600/1263: training loss 0.221
Epoch 87 iteration 0620/1263: training loss 0.222
Epoch 87 iteration 0640/1263: training loss 0.223
Epoch 87 iteration 0660/1263: training loss 0.223
Epoch 87 iteration 0680/1263: training loss 0.223
Epoch 87 iteration 0700/1263: training loss 0.224
Epoch 87 iteration 0720/1263: training loss 0.224
Epoch 87 iteration 0740/1263: training loss 0.224
Epoch 87 iteration 0760/1263: training loss 0.224
Epoch 87 iteration 0780/1263: training loss 0.223
Epoch 87 iteration 0800/1263: training loss 0.223
Epoch 87 iteration 0820/1263: training loss 0.223
Epoch 87 iteration 0840/1263: training loss 0.224
Epoch 87 iteration 0860/1263: training loss 0.224
Epoch 87 iteration 0880/1263: training loss 0.224
Epoch 87 iteration 0900/1263: training loss 0.224
Epoch 87 iteration 0920/1263: training loss 0.224
Epoch 87 iteration 0940/1263: training loss 0.224
Epoch 87 iteration 0960/1263: training loss 0.224
Epoch 87 iteration 0980/1263: training loss 0.224
Epoch 87 iteration 1000/1263: training loss 0.224
Epoch 87 iteration 1020/1263: training loss 0.224
Epoch 87 iteration 1040/1263: training loss 0.224
Epoch 87 iteration 1060/1263: training loss 0.224
Epoch 87 iteration 1080/1263: training loss 0.224
Epoch 87 iteration 1100/1263: training loss 0.223
Epoch 87 iteration 1120/1263: training loss 0.224
Epoch 87 iteration 1140/1263: training loss 0.223
Epoch 87 iteration 1160/1263: training loss 0.224
Epoch 87 iteration 1180/1263: training loss 0.224
Epoch 87 iteration 1200/1263: training loss 0.224
Epoch 87 iteration 1220/1263: training loss 0.223
Epoch 87 iteration 1240/1263: training loss 0.223
Epoch 87 iteration 1260/1263: training loss 0.223
Epoch 87 validation pixAcc: 0.806, mIoU: 0.471
Epoch 88 iteration 0020/1263: training loss 0.202
Epoch 88 iteration 0040/1263: training loss 0.204
Epoch 88 iteration 0060/1263: training loss 0.203
Epoch 88 iteration 0080/1263: training loss 0.209
Epoch 88 iteration 0100/1263: training loss 0.210
Epoch 88 iteration 0120/1263: training loss 0.213
Epoch 88 iteration 0140/1263: training loss 0.212
Epoch 88 iteration 0160/1263: training loss 0.214
Epoch 88 iteration 0180/1263: training loss 0.214
Epoch 88 iteration 0200/1263: training loss 0.213
Epoch 88 iteration 0220/1263: training loss 0.213
Epoch 88 iteration 0240/1263: training loss 0.213
Epoch 88 iteration 0260/1263: training loss 0.213
Epoch 88 iteration 0280/1263: training loss 0.216
Epoch 88 iteration 0300/1263: training loss 0.217
Epoch 88 iteration 0320/1263: training loss 0.216
Epoch 88 iteration 0340/1263: training loss 0.216
Epoch 88 iteration 0360/1263: training loss 0.216
Epoch 88 iteration 0380/1263: training loss 0.216
Epoch 88 iteration 0400/1263: training loss 0.215
Epoch 88 iteration 0420/1263: training loss 0.216
Epoch 88 iteration 0440/1263: training loss 0.216
Epoch 88 iteration 0460/1263: training loss 0.216
Epoch 88 iteration 0480/1263: training loss 0.215
Epoch 88 iteration 0500/1263: training loss 0.215
Epoch 88 iteration 0520/1263: training loss 0.216
Epoch 88 iteration 0540/1263: training loss 0.215
Epoch 88 iteration 0560/1263: training loss 0.215
Epoch 88 iteration 0580/1263: training loss 0.214
Epoch 88 iteration 0600/1263: training loss 0.213
Epoch 88 iteration 0620/1263: training loss 0.212
Epoch 88 iteration 0640/1263: training loss 0.213
Epoch 88 iteration 0660/1263: training loss 0.212
Epoch 88 iteration 0680/1263: training loss 0.213
Epoch 88 iteration 0700/1263: training loss 0.213
Epoch 88 iteration 0720/1263: training loss 0.214
Epoch 88 iteration 0740/1263: training loss 0.213
Epoch 88 iteration 0760/1263: training loss 0.213
Epoch 88 iteration 0780/1263: training loss 0.213
Epoch 88 iteration 0800/1263: training loss 0.213
Epoch 88 iteration 0820/1263: training loss 0.213
Epoch 88 iteration 0840/1263: training loss 0.213
Epoch 88 iteration 0860/1263: training loss 0.213
Epoch 88 iteration 0880/1263: training loss 0.212
Epoch 88 iteration 0900/1263: training loss 0.212
Epoch 88 iteration 0920/1263: training loss 0.213
Epoch 88 iteration 0940/1263: training loss 0.213
Epoch 88 iteration 0960/1263: training loss 0.213
Epoch 88 iteration 0980/1263: training loss 0.212
Epoch 88 iteration 1000/1263: training loss 0.212
Epoch 88 iteration 1020/1263: training loss 0.213
Epoch 88 iteration 1040/1263: training loss 0.213
Epoch 88 iteration 1060/1263: training loss 0.213
Epoch 88 iteration 1080/1263: training loss 0.212
Epoch 88 iteration 1100/1263: training loss 0.212
Epoch 88 iteration 1120/1263: training loss 0.212
Epoch 88 iteration 1140/1263: training loss 0.212
Epoch 88 iteration 1160/1263: training loss 0.212
Epoch 88 iteration 1180/1263: training loss 0.212
Epoch 88 iteration 1200/1263: training loss 0.212
Epoch 88 iteration 1220/1263: training loss 0.212
Epoch 88 iteration 1240/1263: training loss 0.212
Epoch 88 iteration 1260/1263: training loss 0.212
Epoch 88 validation pixAcc: 0.804, mIoU: 0.463
Epoch 89 iteration 0020/1263: training loss 0.220
Epoch 89 iteration 0040/1263: training loss 0.209
Epoch 89 iteration 0060/1263: training loss 0.206
Epoch 89 iteration 0080/1263: training loss 0.201
Epoch 89 iteration 0100/1263: training loss 0.201
Epoch 89 iteration 0120/1263: training loss 0.203
Epoch 89 iteration 0140/1263: training loss 0.204
Epoch 89 iteration 0160/1263: training loss 0.205
Epoch 89 iteration 0180/1263: training loss 0.204
Epoch 89 iteration 0200/1263: training loss 0.205
Epoch 89 iteration 0220/1263: training loss 0.204
Epoch 89 iteration 0240/1263: training loss 0.204
Epoch 89 iteration 0260/1263: training loss 0.204
Epoch 89 iteration 0280/1263: training loss 0.204
Epoch 89 iteration 0300/1263: training loss 0.203
Epoch 89 iteration 0320/1263: training loss 0.204
Epoch 89 iteration 0340/1263: training loss 0.204
Epoch 89 iteration 0360/1263: training loss 0.204
Epoch 89 iteration 0380/1263: training loss 0.204
Epoch 89 iteration 0400/1263: training loss 0.204
Epoch 89 iteration 0420/1263: training loss 0.203
Epoch 89 iteration 0440/1263: training loss 0.203
Epoch 89 iteration 0460/1263: training loss 0.203
Epoch 89 iteration 0480/1263: training loss 0.204
Epoch 89 iteration 0500/1263: training loss 0.204
Epoch 89 iteration 0520/1263: training loss 0.204
Epoch 89 iteration 0540/1263: training loss 0.204
Epoch 89 iteration 0560/1263: training loss 0.203
Epoch 89 iteration 0580/1263: training loss 0.203
Epoch 89 iteration 0600/1263: training loss 0.203
Epoch 89 iteration 0620/1263: training loss 0.204
Epoch 89 iteration 0640/1263: training loss 0.204
Epoch 89 iteration 0660/1263: training loss 0.204
Epoch 89 iteration 0680/1263: training loss 0.204
Epoch 89 iteration 0700/1263: training loss 0.204
Epoch 89 iteration 0720/1263: training loss 0.204
Epoch 89 iteration 0740/1263: training loss 0.204
Epoch 89 iteration 0760/1263: training loss 0.204
Epoch 89 iteration 0780/1263: training loss 0.204
Epoch 89 iteration 0800/1263: training loss 0.204
Epoch 89 iteration 0820/1263: training loss 0.204
Epoch 89 iteration 0840/1263: training loss 0.204
Epoch 89 iteration 0860/1263: training loss 0.204
Epoch 89 iteration 0880/1263: training loss 0.204
Epoch 89 iteration 0900/1263: training loss 0.205
Epoch 89 iteration 0920/1263: training loss 0.205
Epoch 89 iteration 0940/1263: training loss 0.205
Epoch 89 iteration 0960/1263: training loss 0.205
Epoch 89 iteration 0980/1263: training loss 0.205
Epoch 89 iteration 1000/1263: training loss 0.205
Epoch 89 iteration 1020/1263: training loss 0.205
Epoch 89 iteration 1040/1263: training loss 0.205
Epoch 89 iteration 1060/1263: training loss 0.205
Epoch 89 iteration 1080/1263: training loss 0.205
Epoch 89 iteration 1100/1263: training loss 0.205
Epoch 89 iteration 1120/1263: training loss 0.205
Epoch 89 iteration 1140/1263: training loss 0.205
Epoch 89 iteration 1160/1263: training loss 0.205
Epoch 89 iteration 1180/1263: training loss 0.205
Epoch 89 iteration 1200/1263: training loss 0.205
Epoch 89 iteration 1220/1263: training loss 0.205
Epoch 89 iteration 1240/1263: training loss 0.205
Epoch 89 iteration 1260/1263: training loss 0.205
Epoch 89 validation pixAcc: 0.807, mIoU: 0.467
Epoch 90 iteration 0020/1263: training loss 0.209
Epoch 90 iteration 0040/1263: training loss 0.207
Epoch 90 iteration 0060/1263: training loss 0.203
Epoch 90 iteration 0080/1263: training loss 0.202
Epoch 90 iteration 0100/1263: training loss 0.204
Epoch 90 iteration 0120/1263: training loss 0.201
Epoch 90 iteration 0140/1263: training loss 0.200
Epoch 90 iteration 0160/1263: training loss 0.202
Epoch 90 iteration 0180/1263: training loss 0.202
Epoch 90 iteration 0200/1263: training loss 0.203
Epoch 90 iteration 0220/1263: training loss 0.203
Epoch 90 iteration 0240/1263: training loss 0.202
Epoch 90 iteration 0260/1263: training loss 0.202
Epoch 90 iteration 0280/1263: training loss 0.201
Epoch 90 iteration 0300/1263: training loss 0.200
Epoch 90 iteration 0320/1263: training loss 0.200
Epoch 90 iteration 0340/1263: training loss 0.201
Epoch 90 iteration 0360/1263: training loss 0.201
Epoch 90 iteration 0380/1263: training loss 0.201
Epoch 90 iteration 0400/1263: training loss 0.202
Epoch 90 iteration 0420/1263: training loss 0.202
Epoch 90 iteration 0440/1263: training loss 0.202
Epoch 90 iteration 0460/1263: training loss 0.202
Epoch 90 iteration 0480/1263: training loss 0.202
Epoch 90 iteration 0500/1263: training loss 0.202
Epoch 90 iteration 0520/1263: training loss 0.203
Epoch 90 iteration 0540/1263: training loss 0.203
Epoch 90 iteration 0560/1263: training loss 0.203
Epoch 90 iteration 0580/1263: training loss 0.203
Epoch 90 iteration 0600/1263: training loss 0.203
Epoch 90 iteration 0620/1263: training loss 0.204
Epoch 90 iteration 0640/1263: training loss 0.204
Epoch 90 iteration 0660/1263: training loss 0.205
Epoch 90 iteration 0680/1263: training loss 0.205
Epoch 90 iteration 0700/1263: training loss 0.205
Epoch 90 iteration 0720/1263: training loss 0.205
Epoch 90 iteration 0740/1263: training loss 0.204
Epoch 90 iteration 0760/1263: training loss 0.205
Epoch 90 iteration 0780/1263: training loss 0.205
Epoch 90 iteration 0800/1263: training loss 0.205
Epoch 90 iteration 0820/1263: training loss 0.206
Epoch 90 iteration 0840/1263: training loss 0.205
Epoch 90 iteration 0860/1263: training loss 0.206
Epoch 90 iteration 0880/1263: training loss 0.206
Epoch 90 iteration 0900/1263: training loss 0.206
Epoch 90 iteration 0920/1263: training loss 0.206
Epoch 90 iteration 0940/1263: training loss 0.206
Epoch 90 iteration 0960/1263: training loss 0.207
Epoch 90 iteration 0980/1263: training loss 0.207
Epoch 90 iteration 1000/1263: training loss 0.207
Epoch 90 iteration 1020/1263: training loss 0.207
Epoch 90 iteration 1040/1263: training loss 0.207
Epoch 90 iteration 1060/1263: training loss 0.207
Epoch 90 iteration 1080/1263: training loss 0.207
Epoch 90 iteration 1100/1263: training loss 0.208
Epoch 90 iteration 1120/1263: training loss 0.208
Epoch 90 iteration 1140/1263: training loss 0.208
Epoch 90 iteration 1160/1263: training loss 0.208
Epoch 90 iteration 1180/1263: training loss 0.208
Epoch 90 iteration 1200/1263: training loss 0.208
Epoch 90 iteration 1220/1263: training loss 0.208
Epoch 90 iteration 1240/1263: training loss 0.209
Epoch 90 iteration 1260/1263: training loss 0.209
Epoch 90 validation pixAcc: 0.804, mIoU: 0.459
Epoch 91 iteration 0020/1263: training loss 0.209
Epoch 91 iteration 0040/1263: training loss 0.210
Epoch 91 iteration 0060/1263: training loss 0.211
Epoch 91 iteration 0080/1263: training loss 0.210
Epoch 91 iteration 0100/1263: training loss 0.209
Epoch 91 iteration 0120/1263: training loss 0.206
Epoch 91 iteration 0140/1263: training loss 0.209
Epoch 91 iteration 0160/1263: training loss 0.208
Epoch 91 iteration 0180/1263: training loss 0.208
Epoch 91 iteration 0200/1263: training loss 0.208
Epoch 91 iteration 0220/1263: training loss 0.207
Epoch 91 iteration 0240/1263: training loss 0.206
Epoch 91 iteration 0260/1263: training loss 0.207
Epoch 91 iteration 0280/1263: training loss 0.207
Epoch 91 iteration 0300/1263: training loss 0.207
Epoch 91 iteration 0320/1263: training loss 0.205
Epoch 91 iteration 0340/1263: training loss 0.204
Epoch 91 iteration 0360/1263: training loss 0.204
Epoch 91 iteration 0380/1263: training loss 0.203
Epoch 91 iteration 0400/1263: training loss 0.203
Epoch 91 iteration 0420/1263: training loss 0.203
Epoch 91 iteration 0440/1263: training loss 0.202
Epoch 91 iteration 0460/1263: training loss 0.202
Epoch 91 iteration 0480/1263: training loss 0.202
Epoch 91 iteration 0500/1263: training loss 0.203
Epoch 91 iteration 0520/1263: training loss 0.202
Epoch 91 iteration 0540/1263: training loss 0.203
Epoch 91 iteration 0560/1263: training loss 0.203
Epoch 91 iteration 0580/1263: training loss 0.202
Epoch 91 iteration 0600/1263: training loss 0.202
Epoch 91 iteration 0620/1263: training loss 0.203
Epoch 91 iteration 0640/1263: training loss 0.202
Epoch 91 iteration 0660/1263: training loss 0.202
Epoch 91 iteration 0680/1263: training loss 0.203
Epoch 91 iteration 0700/1263: training loss 0.203
Epoch 91 iteration 0720/1263: training loss 0.203
Epoch 91 iteration 0740/1263: training loss 0.204
Epoch 91 iteration 0760/1263: training loss 0.204
Epoch 91 iteration 0780/1263: training loss 0.204
Epoch 91 iteration 0800/1263: training loss 0.204
Epoch 91 iteration 0820/1263: training loss 0.204
Epoch 91 iteration 0840/1263: training loss 0.204
Epoch 91 iteration 0860/1263: training loss 0.204
Epoch 91 iteration 0880/1263: training loss 0.204
Epoch 91 iteration 0900/1263: training loss 0.204
Epoch 91 iteration 0920/1263: training loss 0.204
Epoch 91 iteration 0940/1263: training loss 0.204
Epoch 91 iteration 0960/1263: training loss 0.204
Epoch 91 iteration 0980/1263: training loss 0.204
Epoch 91 iteration 1000/1263: training loss 0.204
Epoch 91 iteration 1020/1263: training loss 0.204
Epoch 91 iteration 1040/1263: training loss 0.204
Epoch 91 iteration 1060/1263: training loss 0.204
Epoch 91 iteration 1080/1263: training loss 0.204
Epoch 91 iteration 1100/1263: training loss 0.204
Epoch 91 iteration 1120/1263: training loss 0.204
Epoch 91 iteration 1140/1263: training loss 0.204
Epoch 91 iteration 1160/1263: training loss 0.204
Epoch 91 iteration 1180/1263: training loss 0.203
Epoch 91 iteration 1200/1263: training loss 0.203
Epoch 91 iteration 1220/1263: training loss 0.203
Epoch 91 iteration 1240/1263: training loss 0.203
Epoch 91 iteration 1260/1263: training loss 0.203
Epoch 91 validation pixAcc: 0.808, mIoU: 0.469
Epoch 92 iteration 0020/1263: training loss 0.183
Epoch 92 iteration 0040/1263: training loss 0.183
Epoch 92 iteration 0060/1263: training loss 0.190
Epoch 92 iteration 0080/1263: training loss 0.188
Epoch 92 iteration 0100/1263: training loss 0.189
Epoch 92 iteration 0120/1263: training loss 0.191
Epoch 92 iteration 0140/1263: training loss 0.190
Epoch 92 iteration 0160/1263: training loss 0.191
Epoch 92 iteration 0180/1263: training loss 0.192
Epoch 92 iteration 0200/1263: training loss 0.194
Epoch 92 iteration 0220/1263: training loss 0.193
Epoch 92 iteration 0240/1263: training loss 0.192
Epoch 92 iteration 0260/1263: training loss 0.191
Epoch 92 iteration 0280/1263: training loss 0.192
Epoch 92 iteration 0300/1263: training loss 0.191
Epoch 92 iteration 0320/1263: training loss 0.191
Epoch 92 iteration 0340/1263: training loss 0.191
Epoch 92 iteration 0360/1263: training loss 0.191
Epoch 92 iteration 0380/1263: training loss 0.191
Epoch 92 iteration 0400/1263: training loss 0.192
Epoch 92 iteration 0420/1263: training loss 0.192
Epoch 92 iteration 0440/1263: training loss 0.192
Epoch 92 iteration 0460/1263: training loss 0.192
Epoch 92 iteration 0480/1263: training loss 0.193
Epoch 92 iteration 0500/1263: training loss 0.193
Epoch 92 iteration 0520/1263: training loss 0.194
Epoch 92 iteration 0540/1263: training loss 0.194
Epoch 92 iteration 0560/1263: training loss 0.193
Epoch 92 iteration 0580/1263: training loss 0.193
Epoch 92 iteration 0600/1263: training loss 0.194
Epoch 92 iteration 0620/1263: training loss 0.194
Epoch 92 iteration 0640/1263: training loss 0.195
Epoch 92 iteration 0660/1263: training loss 0.194
Epoch 92 iteration 0680/1263: training loss 0.195
Epoch 92 iteration 0700/1263: training loss 0.195
Epoch 92 iteration 0720/1263: training loss 0.196
Epoch 92 iteration 0740/1263: training loss 0.196
Epoch 92 iteration 0760/1263: training loss 0.196
Epoch 92 iteration 0780/1263: training loss 0.196
Epoch 92 iteration 0800/1263: training loss 0.195
Epoch 92 iteration 0820/1263: training loss 0.195
Epoch 92 iteration 0840/1263: training loss 0.195
Epoch 92 iteration 0860/1263: training loss 0.195
Epoch 92 iteration 0880/1263: training loss 0.195
Epoch 92 iteration 0900/1263: training loss 0.196
Epoch 92 iteration 0920/1263: training loss 0.196
Epoch 92 iteration 0940/1263: training loss 0.196
Epoch 92 iteration 0960/1263: training loss 0.196
Epoch 92 iteration 0980/1263: training loss 0.196
Epoch 92 iteration 1000/1263: training loss 0.196
Epoch 92 iteration 1020/1263: training loss 0.196
Epoch 92 iteration 1040/1263: training loss 0.196
Epoch 92 iteration 1060/1263: training loss 0.196
Epoch 92 iteration 1080/1263: training loss 0.196
Epoch 92 iteration 1100/1263: training loss 0.196
Epoch 92 iteration 1120/1263: training loss 0.196
Epoch 92 iteration 1140/1263: training loss 0.196
Epoch 92 iteration 1160/1263: training loss 0.196
Epoch 92 iteration 1180/1263: training loss 0.196
Epoch 92 iteration 1200/1263: training loss 0.196
Epoch 92 iteration 1220/1263: training loss 0.196
Epoch 92 iteration 1240/1263: training loss 0.196
Epoch 92 iteration 1260/1263: training loss 0.196
Epoch 92 validation pixAcc: 0.810, mIoU: 0.468
Epoch 93 iteration 0020/1263: training loss 0.194
Epoch 93 iteration 0040/1263: training loss 0.192
Epoch 93 iteration 0060/1263: training loss 0.187
Epoch 93 iteration 0080/1263: training loss 0.189
Epoch 93 iteration 0100/1263: training loss 0.190
Epoch 93 iteration 0120/1263: training loss 0.190
Epoch 93 iteration 0140/1263: training loss 0.191
Epoch 93 iteration 0160/1263: training loss 0.191
Epoch 93 iteration 0180/1263: training loss 0.191
Epoch 93 iteration 0200/1263: training loss 0.190
Epoch 93 iteration 0220/1263: training loss 0.190
Epoch 93 iteration 0240/1263: training loss 0.190
Epoch 93 iteration 0260/1263: training loss 0.191
Epoch 93 iteration 0280/1263: training loss 0.191
Epoch 93 iteration 0300/1263: training loss 0.192
Epoch 93 iteration 0320/1263: training loss 0.191
Epoch 93 iteration 0340/1263: training loss 0.190
Epoch 93 iteration 0360/1263: training loss 0.190
Epoch 93 iteration 0380/1263: training loss 0.190
Epoch 93 iteration 0400/1263: training loss 0.190
Epoch 93 iteration 0420/1263: training loss 0.190
Epoch 93 iteration 0440/1263: training loss 0.190
Epoch 93 iteration 0460/1263: training loss 0.190
Epoch 93 iteration 0480/1263: training loss 0.191
Epoch 93 iteration 0500/1263: training loss 0.190
Epoch 93 iteration 0520/1263: training loss 0.191
Epoch 93 iteration 0540/1263: training loss 0.191
Epoch 93 iteration 0560/1263: training loss 0.191
Epoch 93 iteration 0580/1263: training loss 0.191
Epoch 93 iteration 0600/1263: training loss 0.191
Epoch 93 iteration 0620/1263: training loss 0.192
Epoch 93 iteration 0640/1263: training loss 0.192
Epoch 93 iteration 0660/1263: training loss 0.192
Epoch 93 iteration 0680/1263: training loss 0.191
Epoch 93 iteration 0700/1263: training loss 0.191
Epoch 93 iteration 0720/1263: training loss 0.191
Epoch 93 iteration 0740/1263: training loss 0.191
Epoch 93 iteration 0760/1263: training loss 0.191
Epoch 93 iteration 0780/1263: training loss 0.191
Epoch 93 iteration 0800/1263: training loss 0.190
Epoch 93 iteration 0820/1263: training loss 0.190
Epoch 93 iteration 0840/1263: training loss 0.190
Epoch 93 iteration 0860/1263: training loss 0.190
Epoch 93 iteration 0880/1263: training loss 0.190
Epoch 93 iteration 0900/1263: training loss 0.190
Epoch 93 iteration 0920/1263: training loss 0.190
Epoch 93 iteration 0940/1263: training loss 0.190
Epoch 93 iteration 0960/1263: training loss 0.191
Epoch 93 iteration 0980/1263: training loss 0.190
Epoch 93 iteration 1000/1263: training loss 0.190
Epoch 93 iteration 1020/1263: training loss 0.191
Epoch 93 iteration 1040/1263: training loss 0.191
Epoch 93 iteration 1060/1263: training loss 0.191
Epoch 93 iteration 1080/1263: training loss 0.191
Epoch 93 iteration 1100/1263: training loss 0.192
Epoch 93 iteration 1120/1263: training loss 0.192
Epoch 93 iteration 1140/1263: training loss 0.192
Epoch 93 iteration 1160/1263: training loss 0.192
Epoch 93 iteration 1180/1263: training loss 0.192
Epoch 93 iteration 1200/1263: training loss 0.191
Epoch 93 iteration 1220/1263: training loss 0.191
Epoch 93 iteration 1240/1263: training loss 0.191
Epoch 93 iteration 1260/1263: training loss 0.191
Epoch 93 validation pixAcc: 0.807, mIoU: 0.473
Epoch 94 iteration 0020/1263: training loss 0.184
Epoch 94 iteration 0040/1263: training loss 0.184
Epoch 94 iteration 0060/1263: training loss 0.184
Epoch 94 iteration 0080/1263: training loss 0.180
Epoch 94 iteration 0100/1263: training loss 0.178
Epoch 94 iteration 0120/1263: training loss 0.181
Epoch 94 iteration 0140/1263: training loss 0.182
Epoch 94 iteration 0160/1263: training loss 0.181
Epoch 94 iteration 0180/1263: training loss 0.180
Epoch 94 iteration 0200/1263: training loss 0.180
Epoch 94 iteration 0220/1263: training loss 0.181
Epoch 94 iteration 0240/1263: training loss 0.181
Epoch 94 iteration 0260/1263: training loss 0.181
Epoch 94 iteration 0280/1263: training loss 0.182
Epoch 94 iteration 0300/1263: training loss 0.182
Epoch 94 iteration 0320/1263: training loss 0.181
Epoch 94 iteration 0340/1263: training loss 0.182
Epoch 94 iteration 0360/1263: training loss 0.183
Epoch 94 iteration 0380/1263: training loss 0.184
Epoch 94 iteration 0400/1263: training loss 0.183
Epoch 94 iteration 0420/1263: training loss 0.183
Epoch 94 iteration 0440/1263: training loss 0.183
Epoch 94 iteration 0460/1263: training loss 0.183
Epoch 94 iteration 0480/1263: training loss 0.183
Epoch 94 iteration 0500/1263: training loss 0.183
Epoch 94 iteration 0520/1263: training loss 0.184
Epoch 94 iteration 0540/1263: training loss 0.183
Epoch 94 iteration 0560/1263: training loss 0.183
Epoch 94 iteration 0580/1263: training loss 0.184
Epoch 94 iteration 0600/1263: training loss 0.184
Epoch 94 iteration 0620/1263: training loss 0.184
Epoch 94 iteration 0640/1263: training loss 0.184
Epoch 94 iteration 0660/1263: training loss 0.185
Epoch 94 iteration 0680/1263: training loss 0.185
Epoch 94 iteration 0700/1263: training loss 0.185
Epoch 94 iteration 0720/1263: training loss 0.185
Epoch 94 iteration 0740/1263: training loss 0.185
Epoch 94 iteration 0760/1263: training loss 0.186
Epoch 94 iteration 0780/1263: training loss 0.186
Epoch 94 iteration 0800/1263: training loss 0.185
Epoch 94 iteration 0820/1263: training loss 0.186
Epoch 94 iteration 0840/1263: training loss 0.186
Epoch 94 iteration 0860/1263: training loss 0.186
Epoch 94 iteration 0880/1263: training loss 0.186
Epoch 94 iteration 0900/1263: training loss 0.186
Epoch 94 iteration 0920/1263: training loss 0.186
Epoch 94 iteration 0940/1263: training loss 0.186
Epoch 94 iteration 0960/1263: training loss 0.186
Epoch 94 iteration 0980/1263: training loss 0.186
Epoch 94 iteration 1000/1263: training loss 0.187
Epoch 94 iteration 1020/1263: training loss 0.187
Epoch 94 iteration 1040/1263: training loss 0.187
Epoch 94 iteration 1060/1263: training loss 0.187
Epoch 94 iteration 1080/1263: training loss 0.187
Epoch 94 iteration 1100/1263: training loss 0.187
Epoch 94 iteration 1120/1263: training loss 0.187
Epoch 94 iteration 1140/1263: training loss 0.186
Epoch 94 iteration 1160/1263: training loss 0.187
Epoch 94 iteration 1180/1264: training loss 0.187
Epoch 94 iteration 1200/1264: training loss 0.187
Epoch 94 iteration 1220/1264: training loss 0.187
Epoch 94 iteration 1240/1264: training loss 0.187
Epoch 94 iteration 1260/1264: training loss 0.187
Epoch 94 validation pixAcc: 0.809, mIoU: 0.473
Epoch 95 iteration 0020/1263: training loss 0.193
Epoch 95 iteration 0040/1263: training loss 0.191
Epoch 95 iteration 0060/1263: training loss 0.188
Epoch 95 iteration 0080/1263: training loss 0.193
Epoch 95 iteration 0100/1263: training loss 0.193
Epoch 95 iteration 0120/1263: training loss 0.192
Epoch 95 iteration 0140/1263: training loss 0.194
Epoch 95 iteration 0160/1263: training loss 0.193
Epoch 95 iteration 0180/1263: training loss 0.193
Epoch 95 iteration 0200/1263: training loss 0.192
Epoch 95 iteration 0220/1263: training loss 0.191
Epoch 95 iteration 0240/1263: training loss 0.190
Epoch 95 iteration 0260/1263: training loss 0.190
Epoch 95 iteration 0280/1263: training loss 0.192
Epoch 95 iteration 0300/1263: training loss 0.192
Epoch 95 iteration 0320/1263: training loss 0.192
Epoch 95 iteration 0340/1263: training loss 0.192
Epoch 95 iteration 0360/1263: training loss 0.191
Epoch 95 iteration 0380/1263: training loss 0.192
Epoch 95 iteration 0400/1263: training loss 0.192
Epoch 95 iteration 0420/1263: training loss 0.192
Epoch 95 iteration 0440/1263: training loss 0.192
Epoch 95 iteration 0460/1263: training loss 0.193
Epoch 95 iteration 0480/1263: training loss 0.193
Epoch 95 iteration 0500/1263: training loss 0.193
Epoch 95 iteration 0520/1263: training loss 0.193
Epoch 95 iteration 0540/1263: training loss 0.192
Epoch 95 iteration 0560/1263: training loss 0.192
Epoch 95 iteration 0580/1263: training loss 0.192
Epoch 95 iteration 0600/1263: training loss 0.192
Epoch 95 iteration 0620/1263: training loss 0.192
Epoch 95 iteration 0640/1263: training loss 0.192
Epoch 95 iteration 0660/1263: training loss 0.192
Epoch 95 iteration 0680/1263: training loss 0.191
Epoch 95 iteration 0700/1263: training loss 0.191
Epoch 95 iteration 0720/1263: training loss 0.192
Epoch 95 iteration 0740/1263: training loss 0.191
Epoch 95 iteration 0760/1263: training loss 0.191
Epoch 95 iteration 0780/1263: training loss 0.191
Epoch 95 iteration 0800/1263: training loss 0.191
Epoch 95 iteration 0820/1263: training loss 0.191
Epoch 95 iteration 0840/1263: training loss 0.191
Epoch 95 iteration 0860/1263: training loss 0.191
Epoch 95 iteration 0880/1263: training loss 0.191
Epoch 95 iteration 0900/1263: training loss 0.191
Epoch 95 iteration 0920/1263: training loss 0.191
Epoch 95 iteration 0940/1263: training loss 0.192
Epoch 95 iteration 0960/1263: training loss 0.192
Epoch 95 iteration 0980/1263: training loss 0.192
Epoch 95 iteration 1000/1263: training loss 0.192
Epoch 95 iteration 1020/1263: training loss 0.192
Epoch 95 iteration 1040/1263: training loss 0.193
Epoch 95 iteration 1060/1263: training loss 0.193
Epoch 95 iteration 1080/1263: training loss 0.193
Epoch 95 iteration 1100/1263: training loss 0.194
Epoch 95 iteration 1120/1263: training loss 0.194
Epoch 95 iteration 1140/1263: training loss 0.194
Epoch 95 iteration 1160/1263: training loss 0.194
Epoch 95 iteration 1180/1263: training loss 0.194
Epoch 95 iteration 1200/1263: training loss 0.193
Epoch 95 iteration 1220/1263: training loss 0.193
Epoch 95 iteration 1240/1263: training loss 0.193
Epoch 95 iteration 1260/1263: training loss 0.193
Epoch 95 validation pixAcc: 0.810, mIoU: 0.473
Epoch 96 iteration 0020/1263: training loss 0.206
Epoch 96 iteration 0040/1263: training loss 0.192
Epoch 96 iteration 0060/1263: training loss 0.184
Epoch 96 iteration 0080/1263: training loss 0.184
Epoch 96 iteration 0100/1263: training loss 0.185
Epoch 96 iteration 0120/1263: training loss 0.186
Epoch 96 iteration 0140/1263: training loss 0.186
Epoch 96 iteration 0160/1263: training loss 0.185
Epoch 96 iteration 0180/1263: training loss 0.185
Epoch 96 iteration 0200/1263: training loss 0.185
Epoch 96 iteration 0220/1263: training loss 0.185
Epoch 96 iteration 0240/1263: training loss 0.187
Epoch 96 iteration 0260/1263: training loss 0.188
Epoch 96 iteration 0280/1263: training loss 0.188
Epoch 96 iteration 0300/1263: training loss 0.188
Epoch 96 iteration 0320/1263: training loss 0.189
Epoch 96 iteration 0340/1263: training loss 0.188
Epoch 96 iteration 0360/1263: training loss 0.189
Epoch 96 iteration 0380/1263: training loss 0.188
Epoch 96 iteration 0400/1263: training loss 0.189
Epoch 96 iteration 0420/1263: training loss 0.189
Epoch 96 iteration 0440/1263: training loss 0.188
Epoch 96 iteration 0460/1263: training loss 0.188
Epoch 96 iteration 0480/1263: training loss 0.189
Epoch 96 iteration 0500/1263: training loss 0.188
Epoch 96 iteration 0520/1263: training loss 0.188
Epoch 96 iteration 0540/1263: training loss 0.189
Epoch 96 iteration 0560/1263: training loss 0.189
Epoch 96 iteration 0580/1263: training loss 0.189
Epoch 96 iteration 0600/1263: training loss 0.189
Epoch 96 iteration 0620/1263: training loss 0.189
Epoch 96 iteration 0640/1263: training loss 0.189
Epoch 96 iteration 0660/1263: training loss 0.189
Epoch 96 iteration 0680/1263: training loss 0.189
Epoch 96 iteration 0700/1263: training loss 0.189
Epoch 96 iteration 0720/1263: training loss 0.189
Epoch 96 iteration 0740/1263: training loss 0.188
Epoch 96 iteration 0760/1263: training loss 0.188
Epoch 96 iteration 0780/1263: training loss 0.188
Epoch 96 iteration 0800/1263: training loss 0.188
Epoch 96 iteration 0820/1263: training loss 0.188
Epoch 96 iteration 0840/1263: training loss 0.188
Epoch 96 iteration 0860/1263: training loss 0.188
Epoch 96 iteration 0880/1263: training loss 0.188
Epoch 96 iteration 0900/1263: training loss 0.188
Epoch 96 iteration 0920/1263: training loss 0.188
Epoch 96 iteration 0940/1263: training loss 0.188
Epoch 96 iteration 0960/1263: training loss 0.187
Epoch 96 iteration 0980/1263: training loss 0.187
Epoch 96 iteration 1000/1263: training loss 0.187
Epoch 96 iteration 1020/1263: training loss 0.187
Epoch 96 iteration 1040/1263: training loss 0.188
Epoch 96 iteration 1060/1263: training loss 0.187
Epoch 96 iteration 1080/1263: training loss 0.187
Epoch 96 iteration 1100/1263: training loss 0.187
Epoch 96 iteration 1120/1263: training loss 0.187
Epoch 96 iteration 1140/1263: training loss 0.187
Epoch 96 iteration 1160/1263: training loss 0.187
Epoch 96 iteration 1180/1263: training loss 0.187
Epoch 96 iteration 1200/1263: training loss 0.187
Epoch 96 iteration 1220/1263: training loss 0.187
Epoch 96 iteration 1240/1263: training loss 0.187
Epoch 96 iteration 1260/1263: training loss 0.187
Epoch 96 validation pixAcc: 0.809, mIoU: 0.471
Epoch 97 iteration 0020/1263: training loss 0.175
Epoch 97 iteration 0040/1263: training loss 0.172
Epoch 97 iteration 0060/1263: training loss 0.176
Epoch 97 iteration 0080/1263: training loss 0.179
Epoch 97 iteration 0100/1263: training loss 0.180
Epoch 97 iteration 0120/1263: training loss 0.180
Epoch 97 iteration 0140/1263: training loss 0.181
Epoch 97 iteration 0160/1263: training loss 0.180
Epoch 97 iteration 0180/1263: training loss 0.181
Epoch 97 iteration 0200/1263: training loss 0.181
Epoch 97 iteration 0220/1263: training loss 0.181
Epoch 97 iteration 0240/1263: training loss 0.181
Epoch 97 iteration 0260/1263: training loss 0.180
Epoch 97 iteration 0280/1263: training loss 0.181
Epoch 97 iteration 0300/1263: training loss 0.179
Epoch 97 iteration 0320/1263: training loss 0.180
Epoch 97 iteration 0340/1263: training loss 0.180
Epoch 97 iteration 0360/1263: training loss 0.180
Epoch 97 iteration 0380/1263: training loss 0.180
Epoch 97 iteration 0400/1263: training loss 0.181
Epoch 97 iteration 0420/1263: training loss 0.181
Epoch 97 iteration 0440/1263: training loss 0.182
Epoch 97 iteration 0460/1263: training loss 0.182
Epoch 97 iteration 0480/1263: training loss 0.182
Epoch 97 iteration 0500/1263: training loss 0.181
Epoch 97 iteration 0520/1263: training loss 0.182
Epoch 97 iteration 0540/1263: training loss 0.182
Epoch 97 iteration 0560/1263: training loss 0.182
Epoch 97 iteration 0580/1263: training loss 0.182
Epoch 97 iteration 0600/1263: training loss 0.182
Epoch 97 iteration 0620/1263: training loss 0.182
Epoch 97 iteration 0640/1263: training loss 0.182
Epoch 97 iteration 0660/1263: training loss 0.183
Epoch 97 iteration 0680/1263: training loss 0.183
Epoch 97 iteration 0700/1263: training loss 0.183
Epoch 97 iteration 0720/1263: training loss 0.183
Epoch 97 iteration 0740/1263: training loss 0.183
Epoch 97 iteration 0760/1263: training loss 0.183
Epoch 97 iteration 0780/1263: training loss 0.183
Epoch 97 iteration 0800/1263: training loss 0.183
Epoch 97 iteration 0820/1263: training loss 0.183
Epoch 97 iteration 0840/1263: training loss 0.183
Epoch 97 iteration 0860/1263: training loss 0.183
Epoch 97 iteration 0880/1263: training loss 0.183
Epoch 97 iteration 0900/1263: training loss 0.183
Epoch 97 iteration 0920/1263: training loss 0.183
Epoch 97 iteration 0940/1263: training loss 0.183
Epoch 97 iteration 0960/1263: training loss 0.183
Epoch 97 iteration 0980/1263: training loss 0.182
Epoch 97 iteration 1000/1263: training loss 0.183
Epoch 97 iteration 1020/1263: training loss 0.183
Epoch 97 iteration 1040/1263: training loss 0.183
Epoch 97 iteration 1060/1263: training loss 0.183
Epoch 97 iteration 1080/1263: training loss 0.183
Epoch 97 iteration 1100/1263: training loss 0.183
Epoch 97 iteration 1120/1263: training loss 0.183
Epoch 97 iteration 1140/1263: training loss 0.183
Epoch 97 iteration 1160/1263: training loss 0.183
Epoch 97 iteration 1180/1263: training loss 0.183
Epoch 97 iteration 1200/1263: training loss 0.183
Epoch 97 iteration 1220/1263: training loss 0.184
Epoch 97 iteration 1240/1263: training loss 0.184
Epoch 97 iteration 1260/1263: training loss 0.184
Epoch 97 validation pixAcc: 0.809, mIoU: 0.465
Epoch 98 iteration 0020/1263: training loss 0.189
Epoch 98 iteration 0040/1263: training loss 0.183
Epoch 98 iteration 0060/1263: training loss 0.183
Epoch 98 iteration 0080/1263: training loss 0.183
Epoch 98 iteration 0100/1263: training loss 0.181
Epoch 98 iteration 0120/1263: training loss 0.182
Epoch 98 iteration 0140/1263: training loss 0.182
Epoch 98 iteration 0160/1263: training loss 0.182
Epoch 98 iteration 0180/1263: training loss 0.183
Epoch 98 iteration 0200/1263: training loss 0.183
Epoch 98 iteration 0220/1263: training loss 0.181
Epoch 98 iteration 0240/1263: training loss 0.182
Epoch 98 iteration 0260/1263: training loss 0.182
Epoch 98 iteration 0280/1263: training loss 0.182
Epoch 98 iteration 0300/1263: training loss 0.182
Epoch 98 iteration 0320/1263: training loss 0.182
Epoch 98 iteration 0340/1263: training loss 0.182
Epoch 98 iteration 0360/1263: training loss 0.181
Epoch 98 iteration 0380/1263: training loss 0.181
Epoch 98 iteration 0400/1263: training loss 0.181
Epoch 98 iteration 0420/1263: training loss 0.181
Epoch 98 iteration 0440/1263: training loss 0.180
Epoch 98 iteration 0460/1263: training loss 0.181
Epoch 98 iteration 0480/1263: training loss 0.181
Epoch 98 iteration 0500/1263: training loss 0.180
Epoch 98 iteration 0520/1263: training loss 0.180
Epoch 98 iteration 0540/1263: training loss 0.181
Epoch 98 iteration 0560/1263: training loss 0.181
Epoch 98 iteration 0580/1263: training loss 0.181
Epoch 98 iteration 0600/1263: training loss 0.181
Epoch 98 iteration 0620/1263: training loss 0.181
Epoch 98 iteration 0640/1263: training loss 0.182
Epoch 98 iteration 0660/1263: training loss 0.182
Epoch 98 iteration 0680/1263: training loss 0.181
Epoch 98 iteration 0700/1263: training loss 0.181
Epoch 98 iteration 0720/1263: training loss 0.181
Epoch 98 iteration 0740/1263: training loss 0.181
Epoch 98 iteration 0760/1263: training loss 0.181
Epoch 98 iteration 0780/1263: training loss 0.181
Epoch 98 iteration 0800/1263: training loss 0.181
Epoch 98 iteration 0820/1263: training loss 0.181
Epoch 98 iteration 0840/1263: training loss 0.181
Epoch 98 iteration 0860/1263: training loss 0.181
Epoch 98 iteration 0880/1263: training loss 0.181
Epoch 98 iteration 0900/1263: training loss 0.181
Epoch 98 iteration 0920/1263: training loss 0.181
Epoch 98 iteration 0940/1263: training loss 0.181
Epoch 98 iteration 0960/1263: training loss 0.181
Epoch 98 iteration 0980/1263: training loss 0.181
Epoch 98 iteration 1000/1263: training loss 0.182
Epoch 98 iteration 1020/1263: training loss 0.182
Epoch 98 iteration 1040/1263: training loss 0.182
Epoch 98 iteration 1060/1263: training loss 0.182
Epoch 98 iteration 1080/1263: training loss 0.182
Epoch 98 iteration 1100/1263: training loss 0.182
Epoch 98 iteration 1120/1263: training loss 0.182
Epoch 98 iteration 1140/1263: training loss 0.182
Epoch 98 iteration 1160/1263: training loss 0.182
Epoch 98 iteration 1180/1263: training loss 0.182
Epoch 98 iteration 1200/1263: training loss 0.182
Epoch 98 iteration 1220/1263: training loss 0.182
Epoch 98 iteration 1240/1263: training loss 0.182
Epoch 98 iteration 1260/1263: training loss 0.182
Epoch 98 validation pixAcc: 0.809, mIoU: 0.469
Epoch 99 iteration 0020/1263: training loss 0.184
Epoch 99 iteration 0040/1263: training loss 0.182
Epoch 99 iteration 0060/1263: training loss 0.187
Epoch 99 iteration 0080/1263: training loss 0.184
Epoch 99 iteration 0100/1263: training loss 0.183
Epoch 99 iteration 0120/1263: training loss 0.184
Epoch 99 iteration 0140/1263: training loss 0.185
Epoch 99 iteration 0160/1263: training loss 0.185
Epoch 99 iteration 0180/1263: training loss 0.184
Epoch 99 iteration 0200/1263: training loss 0.185
Epoch 99 iteration 0220/1263: training loss 0.184
Epoch 99 iteration 0240/1263: training loss 0.185
Epoch 99 iteration 0260/1263: training loss 0.185
Epoch 99 iteration 0280/1263: training loss 0.186
Epoch 99 iteration 0300/1263: training loss 0.186
Epoch 99 iteration 0320/1263: training loss 0.187
Epoch 99 iteration 0340/1263: training loss 0.188
Epoch 99 iteration 0360/1263: training loss 0.188
Epoch 99 iteration 0380/1263: training loss 0.188
Epoch 99 iteration 0400/1263: training loss 0.187
Epoch 99 iteration 0420/1263: training loss 0.187
Epoch 99 iteration 0440/1263: training loss 0.187
Epoch 99 iteration 0460/1263: training loss 0.188
Epoch 99 iteration 0480/1263: training loss 0.187
Epoch 99 iteration 0500/1263: training loss 0.187
Epoch 99 iteration 0520/1263: training loss 0.187
Epoch 99 iteration 0540/1263: training loss 0.187
Epoch 99 iteration 0560/1263: training loss 0.187
Epoch 99 iteration 0580/1263: training loss 0.187
Epoch 99 iteration 0600/1263: training loss 0.186
Epoch 99 iteration 0620/1263: training loss 0.186
Epoch 99 iteration 0640/1263: training loss 0.186
Epoch 99 iteration 0660/1263: training loss 0.185
Epoch 99 iteration 0680/1263: training loss 0.185
Epoch 99 iteration 0700/1263: training loss 0.185
Epoch 99 iteration 0720/1263: training loss 0.185
Epoch 99 iteration 0740/1263: training loss 0.185
Epoch 99 iteration 0760/1263: training loss 0.185
Epoch 99 iteration 0780/1263: training loss 0.184
Epoch 99 iteration 0800/1263: training loss 0.184
Epoch 99 iteration 0820/1263: training loss 0.184
Epoch 99 iteration 0840/1263: training loss 0.184
Epoch 99 iteration 0860/1263: training loss 0.184
Epoch 99 iteration 0880/1263: training loss 0.184
Epoch 99 iteration 0900/1263: training loss 0.184
Epoch 99 iteration 0920/1263: training loss 0.183
Epoch 99 iteration 0940/1263: training loss 0.183
Epoch 99 iteration 0960/1263: training loss 0.183
Epoch 99 iteration 0980/1263: training loss 0.183
Epoch 99 iteration 1000/1263: training loss 0.183
Epoch 99 iteration 1020/1263: training loss 0.183
Epoch 99 iteration 1040/1263: training loss 0.183
Epoch 99 iteration 1060/1263: training loss 0.183
Epoch 99 iteration 1080/1263: training loss 0.183
Epoch 99 iteration 1100/1263: training loss 0.182
Epoch 99 iteration 1120/1263: training loss 0.183
Epoch 99 iteration 1140/1263: training loss 0.183
Epoch 99 iteration 1160/1263: training loss 0.182
Epoch 99 iteration 1180/1263: training loss 0.182
Epoch 99 iteration 1200/1263: training loss 0.182
Epoch 99 iteration 1220/1263: training loss 0.182
Epoch 99 iteration 1240/1263: training loss 0.182
Epoch 99 iteration 1260/1263: training loss 0.183
Epoch 99 validation pixAcc: 0.810, mIoU: 0.472
Epoch 100 iteration 0020/1263: training loss 0.179
Epoch 100 iteration 0040/1263: training loss 0.175
Epoch 100 iteration 0060/1263: training loss 0.178
Epoch 100 iteration 0080/1263: training loss 0.175
Epoch 100 iteration 0100/1263: training loss 0.173
Epoch 100 iteration 0120/1263: training loss 0.176
Epoch 100 iteration 0140/1263: training loss 0.175
Epoch 100 iteration 0160/1263: training loss 0.174
Epoch 100 iteration 0180/1263: training loss 0.173
Epoch 100 iteration 0200/1263: training loss 0.174
Epoch 100 iteration 0220/1263: training loss 0.174
Epoch 100 iteration 0240/1263: training loss 0.173
Epoch 100 iteration 0260/1263: training loss 0.173
Epoch 100 iteration 0280/1263: training loss 0.174
Epoch 100 iteration 0300/1263: training loss 0.173
Epoch 100 iteration 0320/1263: training loss 0.172
Epoch 100 iteration 0340/1263: training loss 0.172
Epoch 100 iteration 0360/1263: training loss 0.172
Epoch 100 iteration 0380/1263: training loss 0.173
Epoch 100 iteration 0400/1263: training loss 0.173
Epoch 100 iteration 0420/1263: training loss 0.173
Epoch 100 iteration 0440/1263: training loss 0.174
Epoch 100 iteration 0460/1263: training loss 0.174
Epoch 100 iteration 0480/1263: training loss 0.175
Epoch 100 iteration 0500/1263: training loss 0.175
Epoch 100 iteration 0520/1263: training loss 0.174
Epoch 100 iteration 0540/1263: training loss 0.174
Epoch 100 iteration 0560/1263: training loss 0.173
Epoch 100 iteration 0580/1263: training loss 0.173
Epoch 100 iteration 0600/1263: training loss 0.173
Epoch 100 iteration 0620/1263: training loss 0.174
Epoch 100 iteration 0640/1263: training loss 0.174
Epoch 100 iteration 0660/1263: training loss 0.176
Epoch 100 iteration 0680/1263: training loss 0.176
Epoch 100 iteration 0700/1263: training loss 0.176
Epoch 100 iteration 0720/1263: training loss 0.176
Epoch 100 iteration 0740/1263: training loss 0.176
Epoch 100 iteration 0760/1263: training loss 0.176
Epoch 100 iteration 0780/1263: training loss 0.176
Epoch 100 iteration 0800/1263: training loss 0.176
Epoch 100 iteration 0820/1263: training loss 0.176
Epoch 100 iteration 0840/1263: training loss 0.176
Epoch 100 iteration 0860/1263: training loss 0.176
Epoch 100 iteration 0880/1263: training loss 0.176
Epoch 100 iteration 0900/1263: training loss 0.177
Epoch 100 iteration 0920/1263: training loss 0.177
Epoch 100 iteration 0940/1263: training loss 0.177
Epoch 100 iteration 0960/1263: training loss 0.177
Epoch 100 iteration 0980/1263: training loss 0.177
Epoch 100 iteration 1000/1263: training loss 0.177
Epoch 100 iteration 1020/1263: training loss 0.177
Epoch 100 iteration 1040/1263: training loss 0.177
Epoch 100 iteration 1060/1263: training loss 0.177
Epoch 100 iteration 1080/1263: training loss 0.178
Epoch 100 iteration 1100/1263: training loss 0.178
Epoch 100 iteration 1120/1263: training loss 0.178
Epoch 100 iteration 1140/1263: training loss 0.178
Epoch 100 iteration 1160/1263: training loss 0.177
Epoch 100 iteration 1180/1263: training loss 0.177
Epoch 100 iteration 1200/1263: training loss 0.177
Epoch 100 iteration 1220/1263: training loss 0.177
Epoch 100 iteration 1240/1263: training loss 0.178
Epoch 100 iteration 1260/1263: training loss 0.177
Epoch 100 validation pixAcc: 0.810, mIoU: 0.473
Epoch 101 iteration 0020/1263: training loss 0.157
Epoch 101 iteration 0040/1263: training loss 0.157
Epoch 101 iteration 0060/1263: training loss 0.161
Epoch 101 iteration 0080/1263: training loss 0.161
Epoch 101 iteration 0100/1263: training loss 0.162
Epoch 101 iteration 0120/1263: training loss 0.164
Epoch 101 iteration 0140/1263: training loss 0.165
Epoch 101 iteration 0160/1263: training loss 0.167
Epoch 101 iteration 0180/1263: training loss 0.169
Epoch 101 iteration 0200/1263: training loss 0.170
Epoch 101 iteration 0220/1263: training loss 0.170
Epoch 101 iteration 0240/1263: training loss 0.170
Epoch 101 iteration 0260/1263: training loss 0.170
Epoch 101 iteration 0280/1263: training loss 0.170
Epoch 101 iteration 0300/1263: training loss 0.171
Epoch 101 iteration 0320/1263: training loss 0.171
Epoch 101 iteration 0340/1263: training loss 0.172
Epoch 101 iteration 0360/1263: training loss 0.172
Epoch 101 iteration 0380/1263: training loss 0.173
Epoch 101 iteration 0400/1263: training loss 0.174
Epoch 101 iteration 0420/1263: training loss 0.175
Epoch 101 iteration 0440/1263: training loss 0.174
Epoch 101 iteration 0460/1263: training loss 0.174
Epoch 101 iteration 0480/1263: training loss 0.174
Epoch 101 iteration 0500/1263: training loss 0.173
Epoch 101 iteration 0520/1263: training loss 0.173
Epoch 101 iteration 0540/1263: training loss 0.173
Epoch 101 iteration 0560/1263: training loss 0.173
Epoch 101 iteration 0580/1263: training loss 0.174
Epoch 101 iteration 0600/1263: training loss 0.174
Epoch 101 iteration 0620/1263: training loss 0.174
Epoch 101 iteration 0640/1263: training loss 0.174
Epoch 101 iteration 0660/1263: training loss 0.174
Epoch 101 iteration 0680/1263: training loss 0.175
Epoch 101 iteration 0700/1263: training loss 0.175
Epoch 101 iteration 0720/1263: training loss 0.175
Epoch 101 iteration 0740/1263: training loss 0.175
Epoch 101 iteration 0760/1263: training loss 0.176
Epoch 101 iteration 0780/1263: training loss 0.176
Epoch 101 iteration 0800/1263: training loss 0.176
Epoch 101 iteration 0820/1263: training loss 0.176
Epoch 101 iteration 0840/1263: training loss 0.176
Epoch 101 iteration 0860/1263: training loss 0.177
Epoch 101 iteration 0880/1263: training loss 0.177
Epoch 101 iteration 0900/1263: training loss 0.176
Epoch 101 iteration 0920/1263: training loss 0.176
Epoch 101 iteration 0940/1263: training loss 0.176
Epoch 101 iteration 0960/1263: training loss 0.177
Epoch 101 iteration 0980/1263: training loss 0.176
Epoch 101 iteration 1000/1263: training loss 0.176
Epoch 101 iteration 1020/1263: training loss 0.176
Epoch 101 iteration 1040/1263: training loss 0.176
Epoch 101 iteration 1060/1263: training loss 0.176
Epoch 101 iteration 1080/1263: training loss 0.176
Epoch 101 iteration 1100/1263: training loss 0.177
Epoch 101 iteration 1120/1263: training loss 0.177
Epoch 101 iteration 1140/1263: training loss 0.177
Epoch 101 iteration 1160/1263: training loss 0.177
Epoch 101 iteration 1180/1263: training loss 0.177
Epoch 101 iteration 1200/1263: training loss 0.177
Epoch 101 iteration 1220/1263: training loss 0.177
Epoch 101 iteration 1240/1263: training loss 0.177
Epoch 101 iteration 1260/1263: training loss 0.177
Epoch 101 validation pixAcc: 0.809, mIoU: 0.468
Epoch 102 iteration 0020/1263: training loss 0.185
Epoch 102 iteration 0040/1263: training loss 0.178
Epoch 102 iteration 0060/1263: training loss 0.176
Epoch 102 iteration 0080/1263: training loss 0.174
Epoch 102 iteration 0100/1263: training loss 0.173
Epoch 102 iteration 0120/1263: training loss 0.172
Epoch 102 iteration 0140/1263: training loss 0.171
Epoch 102 iteration 0160/1263: training loss 0.169
Epoch 102 iteration 0180/1263: training loss 0.173
Epoch 102 iteration 0200/1263: training loss 0.171
Epoch 102 iteration 0220/1263: training loss 0.173
Epoch 102 iteration 0240/1263: training loss 0.173
Epoch 102 iteration 0260/1263: training loss 0.173
Epoch 102 iteration 0280/1263: training loss 0.174
Epoch 102 iteration 0300/1263: training loss 0.175
Epoch 102 iteration 0320/1263: training loss 0.175
Epoch 102 iteration 0340/1263: training loss 0.174
Epoch 102 iteration 0360/1263: training loss 0.174
Epoch 102 iteration 0380/1263: training loss 0.173
Epoch 102 iteration 0400/1263: training loss 0.173
Epoch 102 iteration 0420/1263: training loss 0.173
Epoch 102 iteration 0440/1263: training loss 0.173
Epoch 102 iteration 0460/1263: training loss 0.173
Epoch 102 iteration 0480/1263: training loss 0.173
Epoch 102 iteration 0500/1263: training loss 0.173
Epoch 102 iteration 0520/1263: training loss 0.174
Epoch 102 iteration 0540/1263: training loss 0.174
Epoch 102 iteration 0560/1263: training loss 0.174
Epoch 102 iteration 0580/1263: training loss 0.174
Epoch 102 iteration 0600/1263: training loss 0.174
Epoch 102 iteration 0620/1263: training loss 0.174
Epoch 102 iteration 0640/1263: training loss 0.174
Epoch 102 iteration 0660/1263: training loss 0.173
Epoch 102 iteration 0680/1263: training loss 0.174
Epoch 102 iteration 0700/1263: training loss 0.174
Epoch 102 iteration 0720/1263: training loss 0.174
Epoch 102 iteration 0740/1263: training loss 0.174
Epoch 102 iteration 0760/1263: training loss 0.174
Epoch 102 iteration 0780/1263: training loss 0.174
Epoch 102 iteration 0800/1263: training loss 0.174
Epoch 102 iteration 0820/1263: training loss 0.174
Epoch 102 iteration 0840/1263: training loss 0.174
Epoch 102 iteration 0860/1263: training loss 0.174
Epoch 102 iteration 0880/1263: training loss 0.174
Epoch 102 iteration 0900/1263: training loss 0.173
Epoch 102 iteration 0920/1263: training loss 0.173
Epoch 102 iteration 0940/1263: training loss 0.173
Epoch 102 iteration 0960/1263: training loss 0.173
Epoch 102 iteration 0980/1263: training loss 0.173
Epoch 102 iteration 1000/1263: training loss 0.173
Epoch 102 iteration 1020/1263: training loss 0.173
Epoch 102 iteration 1040/1263: training loss 0.173
Epoch 102 iteration 1060/1263: training loss 0.173
Epoch 102 iteration 1080/1263: training loss 0.173
Epoch 102 iteration 1100/1263: training loss 0.174
Epoch 102 iteration 1120/1263: training loss 0.173
Epoch 102 iteration 1140/1263: training loss 0.174
Epoch 102 iteration 1160/1263: training loss 0.174
Epoch 102 iteration 1180/1264: training loss 0.174
Epoch 102 iteration 1200/1264: training loss 0.174
Epoch 102 iteration 1220/1264: training loss 0.174
Epoch 102 iteration 1240/1264: training loss 0.174
Epoch 102 iteration 1260/1264: training loss 0.174
Epoch 102 validation pixAcc: 0.810, mIoU: 0.474
Epoch 103 iteration 0020/1263: training loss 0.177
Epoch 103 iteration 0040/1263: training loss 0.173
Epoch 103 iteration 0060/1263: training loss 0.170
Epoch 103 iteration 0080/1263: training loss 0.169
Epoch 103 iteration 0100/1263: training loss 0.169
Epoch 103 iteration 0120/1263: training loss 0.169
Epoch 103 iteration 0140/1263: training loss 0.169
Epoch 103 iteration 0160/1263: training loss 0.167
Epoch 103 iteration 0180/1263: training loss 0.166
Epoch 103 iteration 0200/1263: training loss 0.166
Epoch 103 iteration 0220/1263: training loss 0.167
Epoch 103 iteration 0240/1263: training loss 0.166
Epoch 103 iteration 0260/1263: training loss 0.166
Epoch 103 iteration 0280/1263: training loss 0.167
Epoch 103 iteration 0300/1263: training loss 0.167
Epoch 103 iteration 0320/1263: training loss 0.167
Epoch 103 iteration 0340/1263: training loss 0.168
Epoch 103 iteration 0360/1263: training loss 0.168
Epoch 103 iteration 0380/1263: training loss 0.168
Epoch 103 iteration 0400/1263: training loss 0.168
Epoch 103 iteration 0420/1263: training loss 0.168
Epoch 103 iteration 0440/1263: training loss 0.168
Epoch 103 iteration 0460/1263: training loss 0.167
Epoch 103 iteration 0480/1263: training loss 0.168
Epoch 103 iteration 0500/1263: training loss 0.167
Epoch 103 iteration 0520/1263: training loss 0.167
Epoch 103 iteration 0540/1263: training loss 0.168
Epoch 103 iteration 0560/1263: training loss 0.167
Epoch 103 iteration 0580/1263: training loss 0.169
Epoch 103 iteration 0600/1263: training loss 0.170
Epoch 103 iteration 0620/1263: training loss 0.170
Epoch 103 iteration 0640/1263: training loss 0.171
Epoch 103 iteration 0660/1263: training loss 0.171
Epoch 103 iteration 0680/1263: training loss 0.171
Epoch 103 iteration 0700/1263: training loss 0.171
Epoch 103 iteration 0720/1263: training loss 0.172
Epoch 103 iteration 0740/1263: training loss 0.171
Epoch 103 iteration 0760/1263: training loss 0.171
Epoch 103 iteration 0780/1263: training loss 0.172
Epoch 103 iteration 0800/1263: training loss 0.172
Epoch 103 iteration 0820/1263: training loss 0.172
Epoch 103 iteration 0840/1263: training loss 0.172
Epoch 103 iteration 0860/1263: training loss 0.172
Epoch 103 iteration 0880/1263: training loss 0.172
Epoch 103 iteration 0900/1263: training loss 0.173
Epoch 103 iteration 0920/1263: training loss 0.173
Epoch 103 iteration 0940/1263: training loss 0.173
Epoch 103 iteration 0960/1263: training loss 0.173
Epoch 103 iteration 0980/1263: training loss 0.172
Epoch 103 iteration 1000/1263: training loss 0.172
Epoch 103 iteration 1020/1263: training loss 0.172
Epoch 103 iteration 1040/1263: training loss 0.172
Epoch 103 iteration 1060/1263: training loss 0.172
Epoch 103 iteration 1080/1263: training loss 0.172
Epoch 103 iteration 1100/1263: training loss 0.172
Epoch 103 iteration 1120/1263: training loss 0.172
Epoch 103 iteration 1140/1263: training loss 0.172
Epoch 103 iteration 1160/1263: training loss 0.172
Epoch 103 iteration 1180/1263: training loss 0.172
Epoch 103 iteration 1200/1263: training loss 0.172
Epoch 103 iteration 1220/1263: training loss 0.172
Epoch 103 iteration 1240/1263: training loss 0.172
Epoch 103 iteration 1260/1263: training loss 0.172
Epoch 103 validation pixAcc: 0.809, mIoU: 0.471
Epoch 104 iteration 0020/1263: training loss 0.180
Epoch 104 iteration 0040/1263: training loss 0.175
Epoch 104 iteration 0060/1263: training loss 0.170
Epoch 104 iteration 0080/1263: training loss 0.171
Epoch 104 iteration 0100/1263: training loss 0.171
Epoch 104 iteration 0120/1263: training loss 0.169
Epoch 104 iteration 0140/1263: training loss 0.169
Epoch 104 iteration 0160/1263: training loss 0.168
Epoch 104 iteration 0180/1263: training loss 0.167
Epoch 104 iteration 0200/1263: training loss 0.167
Epoch 104 iteration 0220/1263: training loss 0.167
Epoch 104 iteration 0240/1263: training loss 0.167
Epoch 104 iteration 0260/1263: training loss 0.168
Epoch 104 iteration 0280/1263: training loss 0.168
Epoch 104 iteration 0300/1263: training loss 0.168
Epoch 104 iteration 0320/1263: training loss 0.169
Epoch 104 iteration 0340/1263: training loss 0.169
Epoch 104 iteration 0360/1263: training loss 0.169
Epoch 104 iteration 0380/1263: training loss 0.170
Epoch 104 iteration 0400/1263: training loss 0.169
Epoch 104 iteration 0420/1263: training loss 0.170
Epoch 104 iteration 0440/1263: training loss 0.169
Epoch 104 iteration 0460/1263: training loss 0.169
Epoch 104 iteration 0480/1263: training loss 0.169
Epoch 104 iteration 0500/1263: training loss 0.169
Epoch 104 iteration 0520/1263: training loss 0.169
Epoch 104 iteration 0540/1263: training loss 0.169
Epoch 104 iteration 0560/1263: training loss 0.168
Epoch 104 iteration 0580/1263: training loss 0.169
Epoch 104 iteration 0600/1263: training loss 0.169
Epoch 104 iteration 0620/1263: training loss 0.169
Epoch 104 iteration 0640/1263: training loss 0.169
Epoch 104 iteration 0660/1263: training loss 0.169
Epoch 104 iteration 0680/1263: training loss 0.169
Epoch 104 iteration 0700/1263: training loss 0.169
Epoch 104 iteration 0720/1263: training loss 0.169
Epoch 104 iteration 0740/1263: training loss 0.169
Epoch 104 iteration 0760/1263: training loss 0.169
Epoch 104 iteration 0780/1263: training loss 0.169
Epoch 104 iteration 0800/1263: training loss 0.169
Epoch 104 iteration 0820/1263: training loss 0.169
Epoch 104 iteration 0840/1263: training loss 0.169
Epoch 104 iteration 0860/1263: training loss 0.169
Epoch 104 iteration 0880/1263: training loss 0.168
Epoch 104 iteration 0900/1263: training loss 0.168
Epoch 104 iteration 0920/1263: training loss 0.169
Epoch 104 iteration 0940/1263: training loss 0.169
Epoch 104 iteration 0960/1263: training loss 0.169
Epoch 104 iteration 0980/1263: training loss 0.169
Epoch 104 iteration 1000/1263: training loss 0.169
Epoch 104 iteration 1020/1263: training loss 0.169
Epoch 104 iteration 1040/1263: training loss 0.170
Epoch 104 iteration 1060/1263: training loss 0.170
Epoch 104 iteration 1080/1263: training loss 0.169
Epoch 104 iteration 1100/1263: training loss 0.170
Epoch 104 iteration 1120/1263: training loss 0.170
Epoch 104 iteration 1140/1263: training loss 0.170
Epoch 104 iteration 1160/1263: training loss 0.171
Epoch 104 iteration 1180/1263: training loss 0.171
Epoch 104 iteration 1200/1263: training loss 0.171
Epoch 104 iteration 1220/1263: training loss 0.171
Epoch 104 iteration 1240/1263: training loss 0.171
Epoch 104 iteration 1260/1263: training loss 0.171
Epoch 104 validation pixAcc: 0.809, mIoU: 0.470
Epoch 105 iteration 0020/1263: training loss 0.164
Epoch 105 iteration 0040/1263: training loss 0.170
Epoch 105 iteration 0060/1263: training loss 0.177
Epoch 105 iteration 0080/1263: training loss 0.172
Epoch 105 iteration 0100/1263: training loss 0.175
Epoch 105 iteration 0120/1263: training loss 0.174
Epoch 105 iteration 0140/1263: training loss 0.173
Epoch 105 iteration 0160/1263: training loss 0.173
Epoch 105 iteration 0180/1263: training loss 0.174
Epoch 105 iteration 0200/1263: training loss 0.173
Epoch 105 iteration 0220/1263: training loss 0.172
Epoch 105 iteration 0240/1263: training loss 0.172
Epoch 105 iteration 0260/1263: training loss 0.171
Epoch 105 iteration 0280/1263: training loss 0.172
Epoch 105 iteration 0300/1263: training loss 0.172
Epoch 105 iteration 0320/1263: training loss 0.172
Epoch 105 iteration 0340/1263: training loss 0.171
Epoch 105 iteration 0360/1263: training loss 0.171
Epoch 105 iteration 0380/1263: training loss 0.170
Epoch 105 iteration 0400/1263: training loss 0.170
Epoch 105 iteration 0420/1263: training loss 0.170
Epoch 105 iteration 0440/1263: training loss 0.170
Epoch 105 iteration 0460/1263: training loss 0.171
Epoch 105 iteration 0480/1263: training loss 0.171
Epoch 105 iteration 0500/1263: training loss 0.171
Epoch 105 iteration 0520/1263: training loss 0.171
Epoch 105 iteration 0540/1263: training loss 0.172
Epoch 105 iteration 0560/1263: training loss 0.171
Epoch 105 iteration 0580/1263: training loss 0.171
Epoch 105 iteration 0600/1263: training loss 0.171
Epoch 105 iteration 0620/1263: training loss 0.170
Epoch 105 iteration 0640/1263: training loss 0.170
Epoch 105 iteration 0660/1263: training loss 0.170
Epoch 105 iteration 0680/1263: training loss 0.170
Epoch 105 iteration 0700/1263: training loss 0.170
Epoch 105 iteration 0720/1263: training loss 0.169
Epoch 105 iteration 0740/1263: training loss 0.169
Epoch 105 iteration 0760/1263: training loss 0.169
Epoch 105 iteration 0780/1263: training loss 0.169
Epoch 105 iteration 0800/1263: training loss 0.169
Epoch 105 iteration 0820/1263: training loss 0.169
Epoch 105 iteration 0840/1263: training loss 0.169
Epoch 105 iteration 0860/1263: training loss 0.168
Epoch 105 iteration 0880/1263: training loss 0.169
Epoch 105 iteration 0900/1263: training loss 0.169
Epoch 105 iteration 0920/1263: training loss 0.169
Epoch 105 iteration 0940/1263: training loss 0.169
Epoch 105 iteration 0960/1263: training loss 0.168
Epoch 105 iteration 0980/1263: training loss 0.169
Epoch 105 iteration 1000/1263: training loss 0.169
Epoch 105 iteration 1020/1263: training loss 0.169
Epoch 105 iteration 1040/1263: training loss 0.169
Epoch 105 iteration 1060/1263: training loss 0.169
Epoch 105 iteration 1080/1263: training loss 0.169
Epoch 105 iteration 1100/1263: training loss 0.169
Epoch 105 iteration 1120/1263: training loss 0.169
Epoch 105 iteration 1140/1263: training loss 0.169
Epoch 105 iteration 1160/1263: training loss 0.169
Epoch 105 iteration 1180/1263: training loss 0.170
Epoch 105 iteration 1200/1263: training loss 0.169
Epoch 105 iteration 1220/1263: training loss 0.170
Epoch 105 iteration 1240/1263: training loss 0.170
Epoch 105 iteration 1260/1263: training loss 0.170
Epoch 105 validation pixAcc: 0.811, mIoU: 0.471
Epoch 106 iteration 0020/1263: training loss 0.167
Epoch 106 iteration 0040/1263: training loss 0.164
Epoch 106 iteration 0060/1263: training loss 0.164
Epoch 106 iteration 0080/1263: training loss 0.165
Epoch 106 iteration 0100/1263: training loss 0.166
Epoch 106 iteration 0120/1263: training loss 0.166
Epoch 106 iteration 0140/1263: training loss 0.166
Epoch 106 iteration 0160/1263: training loss 0.165
Epoch 106 iteration 0180/1263: training loss 0.164
Epoch 106 iteration 0200/1263: training loss 0.165
Epoch 106 iteration 0220/1263: training loss 0.166
Epoch 106 iteration 0240/1263: training loss 0.167
Epoch 106 iteration 0260/1263: training loss 0.166
Epoch 106 iteration 0280/1263: training loss 0.167
Epoch 106 iteration 0300/1263: training loss 0.166
Epoch 106 iteration 0320/1263: training loss 0.166
Epoch 106 iteration 0340/1263: training loss 0.166
Epoch 106 iteration 0360/1263: training loss 0.167
Epoch 106 iteration 0380/1263: training loss 0.166
Epoch 106 iteration 0400/1263: training loss 0.166
Epoch 106 iteration 0420/1263: training loss 0.167
Epoch 106 iteration 0440/1263: training loss 0.166
Epoch 106 iteration 0460/1263: training loss 0.166
Epoch 106 iteration 0480/1263: training loss 0.166
Epoch 106 iteration 0500/1263: training loss 0.165
Epoch 106 iteration 0520/1263: training loss 0.165
Epoch 106 iteration 0540/1263: training loss 0.165
Epoch 106 iteration 0560/1263: training loss 0.165
Epoch 106 iteration 0580/1263: training loss 0.165
Epoch 106 iteration 0600/1263: training loss 0.165
Epoch 106 iteration 0620/1263: training loss 0.165
Epoch 106 iteration 0640/1263: training loss 0.165
Epoch 106 iteration 0660/1263: training loss 0.165
Epoch 106 iteration 0680/1263: training loss 0.165
Epoch 106 iteration 0700/1263: training loss 0.165
Epoch 106 iteration 0720/1263: training loss 0.165
Epoch 106 iteration 0740/1263: training loss 0.165
Epoch 106 iteration 0760/1263: training loss 0.165
Epoch 106 iteration 0780/1263: training loss 0.165
Epoch 106 iteration 0800/1263: training loss 0.166
Epoch 106 iteration 0820/1263: training loss 0.166
Epoch 106 iteration 0840/1263: training loss 0.166
Epoch 106 iteration 0860/1263: training loss 0.166
Epoch 106 iteration 0880/1263: training loss 0.166
Epoch 106 iteration 0900/1263: training loss 0.166
Epoch 106 iteration 0920/1263: training loss 0.166
Epoch 106 iteration 0940/1263: training loss 0.166
Epoch 106 iteration 0960/1263: training loss 0.166
Epoch 106 iteration 0980/1263: training loss 0.166
Epoch 106 iteration 1000/1263: training loss 0.166
Epoch 106 iteration 1020/1263: training loss 0.166
Epoch 106 iteration 1040/1263: training loss 0.166
Epoch 106 iteration 1060/1263: training loss 0.166
Epoch 106 iteration 1080/1263: training loss 0.166
Epoch 106 iteration 1100/1263: training loss 0.166
Epoch 106 iteration 1120/1263: training loss 0.166
Epoch 106 iteration 1140/1263: training loss 0.166
Epoch 106 iteration 1160/1263: training loss 0.166
Epoch 106 iteration 1180/1263: training loss 0.166
Epoch 106 iteration 1200/1263: training loss 0.166
Epoch 106 iteration 1220/1263: training loss 0.166
Epoch 106 iteration 1240/1263: training loss 0.166
Epoch 106 iteration 1260/1263: training loss 0.166
Epoch 106 validation pixAcc: 0.811, mIoU: 0.472
Epoch 107 iteration 0020/1263: training loss 0.163
Epoch 107 iteration 0040/1263: training loss 0.168
Epoch 107 iteration 0060/1263: training loss 0.163
Epoch 107 iteration 0080/1263: training loss 0.164
Epoch 107 iteration 0100/1263: training loss 0.165
Epoch 107 iteration 0120/1263: training loss 0.166
Epoch 107 iteration 0140/1263: training loss 0.165
Epoch 107 iteration 0160/1263: training loss 0.163
Epoch 107 iteration 0180/1263: training loss 0.163
Epoch 107 iteration 0200/1263: training loss 0.164
Epoch 107 iteration 0220/1263: training loss 0.163
Epoch 107 iteration 0240/1263: training loss 0.164
Epoch 107 iteration 0260/1263: training loss 0.164
Epoch 107 iteration 0280/1263: training loss 0.164
Epoch 107 iteration 0300/1263: training loss 0.165
Epoch 107 iteration 0320/1263: training loss 0.165
Epoch 107 iteration 0340/1263: training loss 0.165
Epoch 107 iteration 0360/1263: training loss 0.166
Epoch 107 iteration 0380/1263: training loss 0.167
Epoch 107 iteration 0400/1263: training loss 0.166
Epoch 107 iteration 0420/1263: training loss 0.166
Epoch 107 iteration 0440/1263: training loss 0.167
Epoch 107 iteration 0460/1263: training loss 0.167
Epoch 107 iteration 0480/1263: training loss 0.167
Epoch 107 iteration 0500/1263: training loss 0.168
Epoch 107 iteration 0520/1263: training loss 0.168
Epoch 107 iteration 0540/1263: training loss 0.168
Epoch 107 iteration 0560/1263: training loss 0.168
Epoch 107 iteration 0580/1263: training loss 0.168
Epoch 107 iteration 0600/1263: training loss 0.167
Epoch 107 iteration 0620/1263: training loss 0.167
Epoch 107 iteration 0640/1263: training loss 0.166
Epoch 107 iteration 0660/1263: training loss 0.167
Epoch 107 iteration 0680/1263: training loss 0.167
Epoch 107 iteration 0700/1263: training loss 0.166
Epoch 107 iteration 0720/1263: training loss 0.166
Epoch 107 iteration 0740/1263: training loss 0.166
Epoch 107 iteration 0760/1263: training loss 0.166
Epoch 107 iteration 0780/1263: training loss 0.166
Epoch 107 iteration 0800/1263: training loss 0.166
Epoch 107 iteration 0820/1263: training loss 0.166
Epoch 107 iteration 0840/1263: training loss 0.166
Epoch 107 iteration 0860/1263: training loss 0.166
Epoch 107 iteration 0880/1263: training loss 0.166
Epoch 107 iteration 0900/1263: training loss 0.166
Epoch 107 iteration 0920/1263: training loss 0.166
Epoch 107 iteration 0940/1263: training loss 0.166
Epoch 107 iteration 0960/1263: training loss 0.166
Epoch 107 iteration 0980/1263: training loss 0.166
Epoch 107 iteration 1000/1263: training loss 0.166
Epoch 107 iteration 1020/1263: training loss 0.165
Epoch 107 iteration 1040/1263: training loss 0.165
Epoch 107 iteration 1060/1263: training loss 0.165
Epoch 107 iteration 1080/1263: training loss 0.165
Epoch 107 iteration 1100/1263: training loss 0.165
Epoch 107 iteration 1120/1263: training loss 0.165
Epoch 107 iteration 1140/1263: training loss 0.165
Epoch 107 iteration 1160/1263: training loss 0.165
Epoch 107 iteration 1180/1263: training loss 0.165
Epoch 107 iteration 1200/1263: training loss 0.165
Epoch 107 iteration 1220/1263: training loss 0.165
Epoch 107 iteration 1240/1263: training loss 0.165
Epoch 107 iteration 1260/1263: training loss 0.165
Epoch 107 validation pixAcc: 0.813, mIoU: 0.475
Epoch 108 iteration 0020/1263: training loss 0.173
Epoch 108 iteration 0040/1263: training loss 0.170
Epoch 108 iteration 0060/1263: training loss 0.166
Epoch 108 iteration 0080/1263: training loss 0.166
Epoch 108 iteration 0100/1263: training loss 0.164
Epoch 108 iteration 0120/1263: training loss 0.161
Epoch 108 iteration 0140/1263: training loss 0.163
Epoch 108 iteration 0160/1263: training loss 0.162
Epoch 108 iteration 0180/1263: training loss 0.162
Epoch 108 iteration 0200/1263: training loss 0.162
Epoch 108 iteration 0220/1263: training loss 0.169
Epoch 108 iteration 0240/1263: training loss 0.172
Epoch 108 iteration 0260/1263: training loss 0.174
Epoch 108 iteration 0280/1263: training loss 0.175
Epoch 108 iteration 0300/1263: training loss 0.175
Epoch 108 iteration 0320/1263: training loss 0.175
Epoch 108 iteration 0340/1263: training loss 0.175
Epoch 108 iteration 0360/1263: training loss 0.175
Epoch 108 iteration 0380/1263: training loss 0.175
Epoch 108 iteration 0400/1263: training loss 0.175
Epoch 108 iteration 0420/1263: training loss 0.174
Epoch 108 iteration 0440/1263: training loss 0.174
Epoch 108 iteration 0460/1263: training loss 0.173
Epoch 108 iteration 0480/1263: training loss 0.173
Epoch 108 iteration 0500/1263: training loss 0.173
Epoch 108 iteration 0520/1263: training loss 0.173
Epoch 108 iteration 0540/1263: training loss 0.173
Epoch 108 iteration 0560/1263: training loss 0.172
Epoch 108 iteration 0580/1263: training loss 0.173
Epoch 108 iteration 0600/1263: training loss 0.172
Epoch 108 iteration 0620/1263: training loss 0.172
Epoch 108 iteration 0640/1263: training loss 0.172
Epoch 108 iteration 0660/1263: training loss 0.172
Epoch 108 iteration 0680/1263: training loss 0.171
Epoch 108 iteration 0700/1263: training loss 0.172
Epoch 108 iteration 0720/1263: training loss 0.171
Epoch 108 iteration 0740/1263: training loss 0.171
Epoch 108 iteration 0760/1263: training loss 0.170
Epoch 108 iteration 0780/1263: training loss 0.170
Epoch 108 iteration 0800/1263: training loss 0.170
Epoch 108 iteration 0820/1263: training loss 0.170
Epoch 108 iteration 0840/1263: training loss 0.170
Epoch 108 iteration 0860/1263: training loss 0.169
Epoch 108 iteration 0880/1263: training loss 0.169
Epoch 108 iteration 0900/1263: training loss 0.169
Epoch 108 iteration 0920/1263: training loss 0.169
Epoch 108 iteration 0940/1263: training loss 0.169
Epoch 108 iteration 0960/1263: training loss 0.168
Epoch 108 iteration 0980/1263: training loss 0.168
Epoch 108 iteration 1000/1263: training loss 0.168
Epoch 108 iteration 1020/1263: training loss 0.168
Epoch 108 iteration 1040/1263: training loss 0.168
Epoch 108 iteration 1060/1263: training loss 0.167
Epoch 108 iteration 1080/1263: training loss 0.167
Epoch 108 iteration 1100/1263: training loss 0.168
Epoch 108 iteration 1120/1263: training loss 0.168
Epoch 108 iteration 1140/1263: training loss 0.168
Epoch 108 iteration 1160/1263: training loss 0.168
Epoch 108 iteration 1180/1263: training loss 0.168
Epoch 108 iteration 1200/1263: training loss 0.168
Epoch 108 iteration 1220/1263: training loss 0.168
Epoch 108 iteration 1240/1263: training loss 0.168
Epoch 108 iteration 1260/1263: training loss 0.168
Epoch 108 validation pixAcc: 0.809, mIoU: 0.472
Epoch 109 iteration 0020/1263: training loss 0.175
Epoch 109 iteration 0040/1263: training loss 0.176
Epoch 109 iteration 0060/1263: training loss 0.175
Epoch 109 iteration 0080/1263: training loss 0.172
Epoch 109 iteration 0100/1263: training loss 0.169
Epoch 109 iteration 0120/1263: training loss 0.167
Epoch 109 iteration 0140/1263: training loss 0.167
Epoch 109 iteration 0160/1263: training loss 0.167
Epoch 109 iteration 0180/1263: training loss 0.166
Epoch 109 iteration 0200/1263: training loss 0.164
Epoch 109 iteration 0220/1263: training loss 0.163
Epoch 109 iteration 0240/1263: training loss 0.163
Epoch 109 iteration 0260/1263: training loss 0.163
Epoch 109 iteration 0280/1263: training loss 0.163
Epoch 109 iteration 0300/1263: training loss 0.163
Epoch 109 iteration 0320/1263: training loss 0.163
Epoch 109 iteration 0340/1263: training loss 0.163
Epoch 109 iteration 0360/1263: training loss 0.163
Epoch 109 iteration 0380/1263: training loss 0.164
Epoch 109 iteration 0400/1263: training loss 0.164
Epoch 109 iteration 0420/1263: training loss 0.164
Epoch 109 iteration 0440/1263: training loss 0.164
Epoch 109 iteration 0460/1263: training loss 0.165
Epoch 109 iteration 0480/1263: training loss 0.164
Epoch 109 iteration 0500/1263: training loss 0.164
Epoch 109 iteration 0520/1263: training loss 0.164
Epoch 109 iteration 0540/1263: training loss 0.164
Epoch 109 iteration 0560/1263: training loss 0.164
Epoch 109 iteration 0580/1263: training loss 0.164
Epoch 109 iteration 0600/1263: training loss 0.164
Epoch 109 iteration 0620/1263: training loss 0.164
Epoch 109 iteration 0640/1263: training loss 0.165
Epoch 109 iteration 0660/1263: training loss 0.165
Epoch 109 iteration 0680/1263: training loss 0.165
Epoch 109 iteration 0700/1263: training loss 0.166
Epoch 109 iteration 0720/1263: training loss 0.166
Epoch 109 iteration 0740/1263: training loss 0.165
Epoch 109 iteration 0760/1263: training loss 0.165
Epoch 109 iteration 0780/1263: training loss 0.165
Epoch 109 iteration 0800/1263: training loss 0.166
Epoch 109 iteration 0820/1263: training loss 0.166
Epoch 109 iteration 0840/1263: training loss 0.166
Epoch 109 iteration 0860/1263: training loss 0.166
Epoch 109 iteration 0880/1263: training loss 0.166
Epoch 109 iteration 0900/1263: training loss 0.166
Epoch 109 iteration 0920/1263: training loss 0.166
Epoch 109 iteration 0940/1263: training loss 0.166
Epoch 109 iteration 0960/1263: training loss 0.166
Epoch 109 iteration 0980/1263: training loss 0.166
Epoch 109 iteration 1000/1263: training loss 0.165
Epoch 109 iteration 1020/1263: training loss 0.165
Epoch 109 iteration 1040/1263: training loss 0.165
Epoch 109 iteration 1060/1263: training loss 0.165
Epoch 109 iteration 1080/1263: training loss 0.165
Epoch 109 iteration 1100/1263: training loss 0.165
Epoch 109 iteration 1120/1263: training loss 0.165
Epoch 109 iteration 1140/1263: training loss 0.165
Epoch 109 iteration 1160/1263: training loss 0.165
Epoch 109 iteration 1180/1263: training loss 0.165
Epoch 109 iteration 1200/1263: training loss 0.165
Epoch 109 iteration 1220/1263: training loss 0.165
Epoch 109 iteration 1240/1263: training loss 0.165
Epoch 109 iteration 1260/1263: training loss 0.164
Epoch 109 validation pixAcc: 0.812, mIoU: 0.475
Epoch 110 iteration 0020/1263: training loss 0.155
Epoch 110 iteration 0040/1263: training loss 0.160
Epoch 110 iteration 0060/1263: training loss 0.154
Epoch 110 iteration 0080/1263: training loss 0.161
Epoch 110 iteration 0100/1263: training loss 0.162
Epoch 110 iteration 0120/1263: training loss 0.164
Epoch 110 iteration 0140/1263: training loss 0.163
Epoch 110 iteration 0160/1263: training loss 0.162
Epoch 110 iteration 0180/1263: training loss 0.163
Epoch 110 iteration 0200/1263: training loss 0.161
Epoch 110 iteration 0220/1263: training loss 0.161
Epoch 110 iteration 0240/1263: training loss 0.160
Epoch 110 iteration 0260/1263: training loss 0.160
Epoch 110 iteration 0280/1263: training loss 0.160
Epoch 110 iteration 0300/1263: training loss 0.160
Epoch 110 iteration 0320/1263: training loss 0.160
Epoch 110 iteration 0340/1263: training loss 0.160
Epoch 110 iteration 0360/1263: training loss 0.160
Epoch 110 iteration 0380/1263: training loss 0.160
Epoch 110 iteration 0400/1263: training loss 0.160
Epoch 110 iteration 0420/1263: training loss 0.160
Epoch 110 iteration 0440/1263: training loss 0.160
Epoch 110 iteration 0460/1263: training loss 0.160
Epoch 110 iteration 0480/1263: training loss 0.160
Epoch 110 iteration 0500/1263: training loss 0.160
Epoch 110 iteration 0520/1263: training loss 0.160
Epoch 110 iteration 0540/1263: training loss 0.159
Epoch 110 iteration 0560/1263: training loss 0.160
Epoch 110 iteration 0580/1263: training loss 0.160
Epoch 110 iteration 0600/1263: training loss 0.160
Epoch 110 iteration 0620/1263: training loss 0.160
Epoch 110 iteration 0640/1263: training loss 0.160
Epoch 110 iteration 0660/1263: training loss 0.160
Epoch 110 iteration 0680/1263: training loss 0.160
Epoch 110 iteration 0700/1263: training loss 0.160
Epoch 110 iteration 0720/1263: training loss 0.160
Epoch 110 iteration 0740/1263: training loss 0.160
Epoch 110 iteration 0760/1263: training loss 0.159
Epoch 110 iteration 0780/1263: training loss 0.160
Epoch 110 iteration 0800/1263: training loss 0.159
Epoch 110 iteration 0820/1263: training loss 0.159
Epoch 110 iteration 0840/1263: training loss 0.159
Epoch 110 iteration 0860/1263: training loss 0.159
Epoch 110 iteration 0880/1263: training loss 0.159
Epoch 110 iteration 0900/1263: training loss 0.159
Epoch 110 iteration 0920/1263: training loss 0.159
Epoch 110 iteration 0940/1263: training loss 0.160
Epoch 110 iteration 0960/1263: training loss 0.159
Epoch 110 iteration 0980/1263: training loss 0.159
Epoch 110 iteration 1000/1263: training loss 0.160
Epoch 110 iteration 1020/1263: training loss 0.160
Epoch 110 iteration 1040/1263: training loss 0.160
Epoch 110 iteration 1060/1263: training loss 0.160
Epoch 110 iteration 1080/1263: training loss 0.160
Epoch 110 iteration 1100/1263: training loss 0.160
Epoch 110 iteration 1120/1263: training loss 0.160
Epoch 110 iteration 1140/1263: training loss 0.160
Epoch 110 iteration 1160/1263: training loss 0.161
Epoch 110 iteration 1180/1264: training loss 0.160
Epoch 110 iteration 1200/1264: training loss 0.160
Epoch 110 iteration 1220/1264: training loss 0.160
Epoch 110 iteration 1240/1264: training loss 0.160
Epoch 110 iteration 1260/1264: training loss 0.160
Epoch 110 validation pixAcc: 0.812, mIoU: 0.476
Epoch 111 iteration 0020/1263: training loss 0.170
Epoch 111 iteration 0040/1263: training loss 0.163
Epoch 111 iteration 0060/1263: training loss 0.165
Epoch 111 iteration 0080/1263: training loss 0.161
Epoch 111 iteration 0100/1263: training loss 0.160
Epoch 111 iteration 0120/1263: training loss 0.159
Epoch 111 iteration 0140/1263: training loss 0.160
Epoch 111 iteration 0160/1263: training loss 0.160
Epoch 111 iteration 0180/1263: training loss 0.160
Epoch 111 iteration 0200/1263: training loss 0.160
Epoch 111 iteration 0220/1263: training loss 0.159
Epoch 111 iteration 0240/1263: training loss 0.160
Epoch 111 iteration 0260/1263: training loss 0.159
Epoch 111 iteration 0280/1263: training loss 0.159
Epoch 111 iteration 0300/1263: training loss 0.158
Epoch 111 iteration 0320/1263: training loss 0.158
Epoch 111 iteration 0340/1263: training loss 0.158
Epoch 111 iteration 0360/1263: training loss 0.159
Epoch 111 iteration 0380/1263: training loss 0.159
Epoch 111 iteration 0400/1263: training loss 0.160
Epoch 111 iteration 0420/1263: training loss 0.160
Epoch 111 iteration 0440/1263: training loss 0.160
Epoch 111 iteration 0460/1263: training loss 0.160
Epoch 111 iteration 0480/1263: training loss 0.160
Epoch 111 iteration 0500/1263: training loss 0.160
Epoch 111 iteration 0520/1263: training loss 0.159
Epoch 111 iteration 0540/1263: training loss 0.160
Epoch 111 iteration 0560/1263: training loss 0.160
Epoch 111 iteration 0580/1263: training loss 0.161
Epoch 111 iteration 0600/1263: training loss 0.161
Epoch 111 iteration 0620/1263: training loss 0.161
Epoch 111 iteration 0640/1263: training loss 0.161
Epoch 111 iteration 0660/1263: training loss 0.161
Epoch 111 iteration 0680/1263: training loss 0.161
Epoch 111 iteration 0700/1263: training loss 0.161
Epoch 111 iteration 0720/1263: training loss 0.161
Epoch 111 iteration 0740/1263: training loss 0.160
Epoch 111 iteration 0760/1263: training loss 0.160
Epoch 111 iteration 0780/1263: training loss 0.160
Epoch 111 iteration 0800/1263: training loss 0.160
Epoch 111 iteration 0820/1263: training loss 0.160
Epoch 111 iteration 0840/1263: training loss 0.161
Epoch 111 iteration 0860/1263: training loss 0.160
Epoch 111 iteration 0880/1263: training loss 0.160
Epoch 111 iteration 0900/1263: training loss 0.160
Epoch 111 iteration 0920/1263: training loss 0.161
Epoch 111 iteration 0940/1263: training loss 0.161
Epoch 111 iteration 0960/1263: training loss 0.160
Epoch 111 iteration 0980/1263: training loss 0.160
Epoch 111 iteration 1000/1263: training loss 0.160
Epoch 111 iteration 1020/1263: training loss 0.160
Epoch 111 iteration 1040/1263: training loss 0.160
Epoch 111 iteration 1060/1263: training loss 0.160
Epoch 111 iteration 1080/1263: training loss 0.159
Epoch 111 iteration 1100/1263: training loss 0.159
Epoch 111 iteration 1120/1263: training loss 0.159
Epoch 111 iteration 1140/1263: training loss 0.159
Epoch 111 iteration 1160/1263: training loss 0.159
Epoch 111 iteration 1180/1263: training loss 0.159
Epoch 111 iteration 1200/1263: training loss 0.159
Epoch 111 iteration 1220/1263: training loss 0.160
Epoch 111 iteration 1240/1263: training loss 0.159
Epoch 111 iteration 1260/1263: training loss 0.159
Epoch 111 validation pixAcc: 0.813, mIoU: 0.478
Epoch 112 iteration 0020/1263: training loss 0.158
Epoch 112 iteration 0040/1263: training loss 0.153
Epoch 112 iteration 0060/1263: training loss 0.155
Epoch 112 iteration 0080/1263: training loss 0.154
Epoch 112 iteration 0100/1263: training loss 0.154
Epoch 112 iteration 0120/1263: training loss 0.161
Epoch 112 iteration 0140/1263: training loss 0.161
Epoch 112 iteration 0160/1263: training loss 0.158
Epoch 112 iteration 0180/1263: training loss 0.159
Epoch 112 iteration 0200/1263: training loss 0.159
Epoch 112 iteration 0220/1263: training loss 0.159
Epoch 112 iteration 0240/1263: training loss 0.159
Epoch 112 iteration 0260/1263: training loss 0.159
Epoch 112 iteration 0280/1263: training loss 0.159
Epoch 112 iteration 0300/1263: training loss 0.159
Epoch 112 iteration 0320/1263: training loss 0.158
Epoch 112 iteration 0340/1263: training loss 0.158
Epoch 112 iteration 0360/1263: training loss 0.158
Epoch 112 iteration 0380/1263: training loss 0.157
Epoch 112 iteration 0400/1263: training loss 0.156
Epoch 112 iteration 0420/1263: training loss 0.157
Epoch 112 iteration 0440/1263: training loss 0.157
Epoch 112 iteration 0460/1263: training loss 0.157
Epoch 112 iteration 0480/1263: training loss 0.157
Epoch 112 iteration 0500/1263: training loss 0.157
Epoch 112 iteration 0520/1263: training loss 0.157
Epoch 112 iteration 0540/1263: training loss 0.156
Epoch 112 iteration 0560/1263: training loss 0.156
Epoch 112 iteration 0580/1263: training loss 0.156
Epoch 112 iteration 0600/1263: training loss 0.156
Epoch 112 iteration 0620/1263: training loss 0.155
Epoch 112 iteration 0640/1263: training loss 0.155
Epoch 112 iteration 0660/1263: training loss 0.155
Epoch 112 iteration 0680/1263: training loss 0.156
Epoch 112 iteration 0700/1263: training loss 0.156
Epoch 112 iteration 0720/1263: training loss 0.156
Epoch 112 iteration 0740/1263: training loss 0.156
Epoch 112 iteration 0760/1263: training loss 0.157
Epoch 112 iteration 0780/1263: training loss 0.156
Epoch 112 iteration 0800/1263: training loss 0.157
Epoch 112 iteration 0820/1263: training loss 0.157
Epoch 112 iteration 0840/1263: training loss 0.157
Epoch 112 iteration 0860/1263: training loss 0.157
Epoch 112 iteration 0880/1263: training loss 0.157
Epoch 112 iteration 0900/1263: training loss 0.157
Epoch 112 iteration 0920/1263: training loss 0.157
Epoch 112 iteration 0940/1263: training loss 0.157
Epoch 112 iteration 0960/1263: training loss 0.157
Epoch 112 iteration 0980/1263: training loss 0.157
Epoch 112 iteration 1000/1263: training loss 0.157
Epoch 112 iteration 1020/1263: training loss 0.157
Epoch 112 iteration 1040/1263: training loss 0.156
Epoch 112 iteration 1060/1263: training loss 0.156
Epoch 112 iteration 1080/1263: training loss 0.156
Epoch 112 iteration 1100/1263: training loss 0.156
Epoch 112 iteration 1120/1263: training loss 0.156
Epoch 112 iteration 1140/1263: training loss 0.156
Epoch 112 iteration 1160/1263: training loss 0.156
Epoch 112 iteration 1180/1263: training loss 0.156
Epoch 112 iteration 1200/1263: training loss 0.156
Epoch 112 iteration 1220/1263: training loss 0.156
Epoch 112 iteration 1240/1263: training loss 0.156
Epoch 112 iteration 1260/1263: training loss 0.156
Epoch 112 validation pixAcc: 0.812, mIoU: 0.480
Epoch 113 iteration 0020/1263: training loss 0.158
Epoch 113 iteration 0040/1263: training loss 0.158
Epoch 113 iteration 0060/1263: training loss 0.155
Epoch 113 iteration 0080/1263: training loss 0.159
Epoch 113 iteration 0100/1263: training loss 0.159
Epoch 113 iteration 0120/1263: training loss 0.158
Epoch 113 iteration 0140/1263: training loss 0.159
Epoch 113 iteration 0160/1263: training loss 0.158
Epoch 113 iteration 0180/1263: training loss 0.158
Epoch 113 iteration 0200/1263: training loss 0.158
Epoch 113 iteration 0220/1263: training loss 0.158
Epoch 113 iteration 0240/1263: training loss 0.158
Epoch 113 iteration 0260/1263: training loss 0.157
Epoch 113 iteration 0280/1263: training loss 0.157
Epoch 113 iteration 0300/1263: training loss 0.157
Epoch 113 iteration 0320/1263: training loss 0.157
Epoch 113 iteration 0340/1263: training loss 0.158
Epoch 113 iteration 0360/1263: training loss 0.157
Epoch 113 iteration 0380/1263: training loss 0.157
Epoch 113 iteration 0400/1263: training loss 0.157
Epoch 113 iteration 0420/1263: training loss 0.157
Epoch 113 iteration 0440/1263: training loss 0.157
Epoch 113 iteration 0460/1263: training loss 0.157
Epoch 113 iteration 0480/1263: training loss 0.157
Epoch 113 iteration 0500/1263: training loss 0.157
Epoch 113 iteration 0520/1263: training loss 0.157
Epoch 113 iteration 0540/1263: training loss 0.157
Epoch 113 iteration 0560/1263: training loss 0.157
Epoch 113 iteration 0580/1263: training loss 0.157
Epoch 113 iteration 0600/1263: training loss 0.158
Epoch 113 iteration 0620/1263: training loss 0.158
Epoch 113 iteration 0640/1263: training loss 0.157
Epoch 113 iteration 0660/1263: training loss 0.157
Epoch 113 iteration 0680/1263: training loss 0.157
Epoch 113 iteration 0700/1263: training loss 0.157
Epoch 113 iteration 0720/1263: training loss 0.158
Epoch 113 iteration 0740/1263: training loss 0.158
Epoch 113 iteration 0760/1263: training loss 0.157
Epoch 113 iteration 0780/1263: training loss 0.157
Epoch 113 iteration 0800/1263: training loss 0.157
Epoch 113 iteration 0820/1263: training loss 0.157
Epoch 113 iteration 0840/1263: training loss 0.157
Epoch 113 iteration 0860/1263: training loss 0.157
Epoch 113 iteration 0880/1263: training loss 0.157
Epoch 113 iteration 0900/1263: training loss 0.157
Epoch 113 iteration 0920/1263: training loss 0.157
Epoch 113 iteration 0940/1263: training loss 0.157
Epoch 113 iteration 0960/1263: training loss 0.157
Epoch 113 iteration 0980/1263: training loss 0.157
Epoch 113 iteration 1000/1263: training loss 0.157
Epoch 113 iteration 1020/1263: training loss 0.157
Epoch 113 iteration 1040/1263: training loss 0.156
Epoch 113 iteration 1060/1263: training loss 0.156
Epoch 113 iteration 1080/1263: training loss 0.156
Epoch 113 iteration 1100/1263: training loss 0.156
Epoch 113 iteration 1120/1263: training loss 0.156
Epoch 113 iteration 1140/1263: training loss 0.156
Epoch 113 iteration 1160/1263: training loss 0.156
Epoch 113 iteration 1180/1263: training loss 0.156
Epoch 113 iteration 1200/1263: training loss 0.156
Epoch 113 iteration 1220/1263: training loss 0.156
Epoch 113 iteration 1240/1263: training loss 0.156
Epoch 113 iteration 1260/1263: training loss 0.156
Epoch 113 validation pixAcc: 0.813, mIoU: 0.479
Epoch 114 iteration 0020/1263: training loss 0.152
Epoch 114 iteration 0040/1263: training loss 0.156
Epoch 114 iteration 0060/1263: training loss 0.154
Epoch 114 iteration 0080/1263: training loss 0.156
Epoch 114 iteration 0100/1263: training loss 0.153
Epoch 114 iteration 0120/1263: training loss 0.153
Epoch 114 iteration 0140/1263: training loss 0.155
Epoch 114 iteration 0160/1263: training loss 0.155
Epoch 114 iteration 0180/1263: training loss 0.155
Epoch 114 iteration 0200/1263: training loss 0.154
Epoch 114 iteration 0220/1263: training loss 0.154
Epoch 114 iteration 0240/1263: training loss 0.155
Epoch 114 iteration 0260/1263: training loss 0.155
Epoch 114 iteration 0280/1263: training loss 0.155
Epoch 114 iteration 0300/1263: training loss 0.155
Epoch 114 iteration 0320/1263: training loss 0.155
Epoch 114 iteration 0340/1263: training loss 0.155
Epoch 114 iteration 0360/1263: training loss 0.156
Epoch 114 iteration 0380/1263: training loss 0.156
Epoch 114 iteration 0400/1263: training loss 0.155
Epoch 114 iteration 0420/1263: training loss 0.155
Epoch 114 iteration 0440/1263: training loss 0.156
Epoch 114 iteration 0460/1263: training loss 0.155
Epoch 114 iteration 0480/1263: training loss 0.155
Epoch 114 iteration 0500/1263: training loss 0.155
Epoch 114 iteration 0520/1263: training loss 0.155
Epoch 114 iteration 0540/1263: training loss 0.154
Epoch 114 iteration 0560/1263: training loss 0.154
Epoch 114 iteration 0580/1263: training loss 0.154
Epoch 114 iteration 0600/1263: training loss 0.154
Epoch 114 iteration 0620/1263: training loss 0.154
Epoch 114 iteration 0640/1263: training loss 0.154
Epoch 114 iteration 0660/1263: training loss 0.154
Epoch 114 iteration 0680/1263: training loss 0.154
Epoch 114 iteration 0700/1263: training loss 0.154
Epoch 114 iteration 0720/1263: training loss 0.154
Epoch 114 iteration 0740/1263: training loss 0.154
Epoch 114 iteration 0760/1263: training loss 0.154
Epoch 114 iteration 0780/1263: training loss 0.154
Epoch 114 iteration 0800/1263: training loss 0.154
Epoch 114 iteration 0820/1263: training loss 0.154
Epoch 114 iteration 0840/1263: training loss 0.154
Epoch 114 iteration 0860/1263: training loss 0.154
Epoch 114 iteration 0880/1263: training loss 0.154
Epoch 114 iteration 0900/1263: training loss 0.154
Epoch 114 iteration 0920/1263: training loss 0.154
Epoch 114 iteration 0940/1263: training loss 0.155
Epoch 114 iteration 0960/1263: training loss 0.155
Epoch 114 iteration 0980/1263: training loss 0.155
Epoch 114 iteration 1000/1263: training loss 0.155
Epoch 114 iteration 1020/1263: training loss 0.155
Epoch 114 iteration 1040/1263: training loss 0.155
Epoch 114 iteration 1060/1263: training loss 0.155
Epoch 114 iteration 1080/1263: training loss 0.155
Epoch 114 iteration 1100/1263: training loss 0.155
Epoch 114 iteration 1120/1263: training loss 0.155
Epoch 114 iteration 1140/1263: training loss 0.155
Epoch 114 iteration 1160/1263: training loss 0.155
Epoch 114 iteration 1180/1263: training loss 0.155
Epoch 114 iteration 1200/1263: training loss 0.155
Epoch 114 iteration 1220/1263: training loss 0.155
Epoch 114 iteration 1240/1263: training loss 0.155
Epoch 114 iteration 1260/1263: training loss 0.155
Epoch 114 validation pixAcc: 0.812, mIoU: 0.476
Epoch 115 iteration 0020/1263: training loss 0.160
Epoch 115 iteration 0040/1263: training loss 0.157
Epoch 115 iteration 0060/1263: training loss 0.151
Epoch 115 iteration 0080/1263: training loss 0.153
Epoch 115 iteration 0100/1263: training loss 0.153
Epoch 115 iteration 0120/1263: training loss 0.153
Epoch 115 iteration 0140/1263: training loss 0.156
Epoch 115 iteration 0160/1263: training loss 0.156
Epoch 115 iteration 0180/1263: training loss 0.156
Epoch 115 iteration 0200/1263: training loss 0.155
Epoch 115 iteration 0220/1263: training loss 0.157
Epoch 115 iteration 0240/1263: training loss 0.156
Epoch 115 iteration 0260/1263: training loss 0.157
Epoch 115 iteration 0280/1263: training loss 0.157
Epoch 115 iteration 0300/1263: training loss 0.158
Epoch 115 iteration 0320/1263: training loss 0.157
Epoch 115 iteration 0340/1263: training loss 0.157
Epoch 115 iteration 0360/1263: training loss 0.157
Epoch 115 iteration 0380/1263: training loss 0.157
Epoch 115 iteration 0400/1263: training loss 0.157
Epoch 115 iteration 0420/1263: training loss 0.157
Epoch 115 iteration 0440/1263: training loss 0.157
Epoch 115 iteration 0460/1263: training loss 0.157
Epoch 115 iteration 0480/1263: training loss 0.157
Epoch 115 iteration 0500/1263: training loss 0.156
Epoch 115 iteration 0520/1263: training loss 0.156
Epoch 115 iteration 0540/1263: training loss 0.155
Epoch 115 iteration 0560/1263: training loss 0.155
Epoch 115 iteration 0580/1263: training loss 0.155
Epoch 115 iteration 0600/1263: training loss 0.155
Epoch 115 iteration 0620/1263: training loss 0.155
Epoch 115 iteration 0640/1263: training loss 0.155
Epoch 115 iteration 0660/1263: training loss 0.155
Epoch 115 iteration 0680/1263: training loss 0.155
Epoch 115 iteration 0700/1263: training loss 0.155
Epoch 115 iteration 0720/1263: training loss 0.155
Epoch 115 iteration 0740/1263: training loss 0.155
Epoch 115 iteration 0760/1263: training loss 0.155
Epoch 115 iteration 0780/1263: training loss 0.155
Epoch 115 iteration 0800/1263: training loss 0.155
Epoch 115 iteration 0820/1263: training loss 0.155
Epoch 115 iteration 0840/1263: training loss 0.155
Epoch 115 iteration 0860/1263: training loss 0.155
Epoch 115 iteration 0880/1263: training loss 0.154
Epoch 115 iteration 0900/1263: training loss 0.154
Epoch 115 iteration 0920/1263: training loss 0.155
Epoch 115 iteration 0940/1263: training loss 0.154
Epoch 115 iteration 0960/1263: training loss 0.154
Epoch 115 iteration 0980/1263: training loss 0.154
Epoch 115 iteration 1000/1263: training loss 0.155
Epoch 115 iteration 1020/1263: training loss 0.155
Epoch 115 iteration 1040/1263: training loss 0.155
Epoch 115 iteration 1060/1263: training loss 0.155
Epoch 115 iteration 1080/1263: training loss 0.155
Epoch 115 iteration 1100/1263: training loss 0.155
Epoch 115 iteration 1120/1263: training loss 0.155
Epoch 115 iteration 1140/1263: training loss 0.155
Epoch 115 iteration 1160/1263: training loss 0.155
Epoch 115 iteration 1180/1263: training loss 0.155
Epoch 115 iteration 1200/1263: training loss 0.154
Epoch 115 iteration 1220/1263: training loss 0.154
Epoch 115 iteration 1240/1263: training loss 0.154
Epoch 115 iteration 1260/1263: training loss 0.154
Epoch 115 validation pixAcc: 0.812, mIoU: 0.478
Epoch 116 iteration 0020/1263: training loss 0.153
Epoch 116 iteration 0040/1263: training loss 0.145
Epoch 116 iteration 0060/1263: training loss 0.149
Epoch 116 iteration 0080/1263: training loss 0.148
Epoch 116 iteration 0100/1263: training loss 0.149
Epoch 116 iteration 0120/1263: training loss 0.149
Epoch 116 iteration 0140/1263: training loss 0.149
Epoch 116 iteration 0160/1263: training loss 0.150
Epoch 116 iteration 0180/1263: training loss 0.150
Epoch 116 iteration 0200/1263: training loss 0.150
Epoch 116 iteration 0220/1263: training loss 0.149
Epoch 116 iteration 0240/1263: training loss 0.151
Epoch 116 iteration 0260/1263: training loss 0.152
Epoch 116 iteration 0280/1263: training loss 0.152
Epoch 116 iteration 0300/1263: training loss 0.152
Epoch 116 iteration 0320/1263: training loss 0.152
Epoch 116 iteration 0340/1263: training loss 0.153
Epoch 116 iteration 0360/1263: training loss 0.153
Epoch 116 iteration 0380/1263: training loss 0.153
Epoch 116 iteration 0400/1263: training loss 0.153
Epoch 116 iteration 0420/1263: training loss 0.153
Epoch 116 iteration 0440/1263: training loss 0.153
Epoch 116 iteration 0460/1263: training loss 0.153
Epoch 116 iteration 0480/1263: training loss 0.152
Epoch 116 iteration 0500/1263: training loss 0.153
Epoch 116 iteration 0520/1263: training loss 0.153
Epoch 116 iteration 0540/1263: training loss 0.152
Epoch 116 iteration 0560/1263: training loss 0.152
Epoch 116 iteration 0580/1263: training loss 0.152
Epoch 116 iteration 0600/1263: training loss 0.152
Epoch 116 iteration 0620/1263: training loss 0.152
Epoch 116 iteration 0640/1263: training loss 0.152
Epoch 116 iteration 0660/1263: training loss 0.152
Epoch 116 iteration 0680/1263: training loss 0.152
Epoch 116 iteration 0700/1263: training loss 0.152
Epoch 116 iteration 0720/1263: training loss 0.152
Epoch 116 iteration 0740/1263: training loss 0.152
Epoch 116 iteration 0760/1263: training loss 0.152
Epoch 116 iteration 0780/1263: training loss 0.153
Epoch 116 iteration 0800/1263: training loss 0.152
Epoch 116 iteration 0820/1263: training loss 0.152
Epoch 116 iteration 0840/1263: training loss 0.152
Epoch 116 iteration 0860/1263: training loss 0.152
Epoch 116 iteration 0880/1263: training loss 0.152
Epoch 116 iteration 0900/1263: training loss 0.152
Epoch 116 iteration 0920/1263: training loss 0.152
Epoch 116 iteration 0940/1263: training loss 0.152
Epoch 116 iteration 0960/1263: training loss 0.152
Epoch 116 iteration 0980/1263: training loss 0.153
Epoch 116 iteration 1000/1263: training loss 0.153
Epoch 116 iteration 1020/1263: training loss 0.152
Epoch 116 iteration 1040/1263: training loss 0.152
Epoch 116 iteration 1060/1263: training loss 0.153
Epoch 116 iteration 1080/1263: training loss 0.153
Epoch 116 iteration 1100/1263: training loss 0.153
Epoch 116 iteration 1120/1263: training loss 0.152
Epoch 116 iteration 1140/1263: training loss 0.152
Epoch 116 iteration 1160/1263: training loss 0.153
Epoch 116 iteration 1180/1263: training loss 0.153
Epoch 116 iteration 1200/1263: training loss 0.153
Epoch 116 iteration 1220/1263: training loss 0.153
Epoch 116 iteration 1240/1263: training loss 0.152
Epoch 116 iteration 1260/1263: training loss 0.152
Epoch 116 validation pixAcc: 0.814, mIoU: 0.479
Epoch 117 iteration 0020/1263: training loss 0.153
Epoch 117 iteration 0040/1263: training loss 0.151
Epoch 117 iteration 0060/1263: training loss 0.153
Epoch 117 iteration 0080/1263: training loss 0.151
Epoch 117 iteration 0100/1263: training loss 0.152
Epoch 117 iteration 0120/1263: training loss 0.150
Epoch 117 iteration 0140/1263: training loss 0.151
Epoch 117 iteration 0160/1263: training loss 0.151
Epoch 117 iteration 0180/1263: training loss 0.151
Epoch 117 iteration 0200/1263: training loss 0.151
Epoch 117 iteration 0220/1263: training loss 0.151
Epoch 117 iteration 0240/1263: training loss 0.151
Epoch 117 iteration 0260/1263: training loss 0.151
Epoch 117 iteration 0280/1263: training loss 0.150
Epoch 117 iteration 0300/1263: training loss 0.151
Epoch 117 iteration 0320/1263: training loss 0.150
Epoch 117 iteration 0340/1263: training loss 0.151
Epoch 117 iteration 0360/1263: training loss 0.152
Epoch 117 iteration 0380/1263: training loss 0.151
Epoch 117 iteration 0400/1263: training loss 0.152
Epoch 117 iteration 0420/1263: training loss 0.152
Epoch 117 iteration 0440/1263: training loss 0.152
Epoch 117 iteration 0460/1263: training loss 0.152
Epoch 117 iteration 0480/1263: training loss 0.152
Epoch 117 iteration 0500/1263: training loss 0.152
Epoch 117 iteration 0520/1263: training loss 0.152
Epoch 117 iteration 0540/1263: training loss 0.152
Epoch 117 iteration 0560/1263: training loss 0.152
Epoch 117 iteration 0580/1263: training loss 0.152
Epoch 117 iteration 0600/1263: training loss 0.152
Epoch 117 iteration 0620/1263: training loss 0.152
Epoch 117 iteration 0640/1263: training loss 0.152
Epoch 117 iteration 0660/1263: training loss 0.152
Epoch 117 iteration 0680/1263: training loss 0.152
Epoch 117 iteration 0700/1263: training loss 0.152
Epoch 117 iteration 0720/1263: training loss 0.152
Epoch 117 iteration 0740/1263: training loss 0.152
Epoch 117 iteration 0760/1263: training loss 0.152
Epoch 117 iteration 0780/1263: training loss 0.152
Epoch 117 iteration 0800/1263: training loss 0.152
Epoch 117 iteration 0820/1263: training loss 0.152
Epoch 117 iteration 0840/1263: training loss 0.152
Epoch 117 iteration 0860/1263: training loss 0.152
Epoch 117 iteration 0880/1263: training loss 0.152
Epoch 117 iteration 0900/1263: training loss 0.152
Epoch 117 iteration 0920/1263: training loss 0.152
Epoch 117 iteration 0940/1263: training loss 0.152
Epoch 117 iteration 0960/1263: training loss 0.152
Epoch 117 iteration 0980/1263: training loss 0.152
Epoch 117 iteration 1000/1263: training loss 0.152
Epoch 117 iteration 1020/1263: training loss 0.152
Epoch 117 iteration 1040/1263: training loss 0.152
Epoch 117 iteration 1060/1263: training loss 0.152
Epoch 117 iteration 1080/1263: training loss 0.152
Epoch 117 iteration 1100/1263: training loss 0.152
Epoch 117 iteration 1120/1263: training loss 0.152
Epoch 117 iteration 1140/1263: training loss 0.152
Epoch 117 iteration 1160/1263: training loss 0.152
Epoch 117 iteration 1180/1263: training loss 0.152
Epoch 117 iteration 1200/1263: training loss 0.152
Epoch 117 iteration 1220/1263: training loss 0.152
Epoch 117 iteration 1240/1263: training loss 0.152
Epoch 117 iteration 1260/1263: training loss 0.152
Epoch 117 validation pixAcc: 0.813, mIoU: 0.479
Epoch 118 iteration 0020/1263: training loss 0.152
Epoch 118 iteration 0040/1263: training loss 0.152
Epoch 118 iteration 0060/1263: training loss 0.158
Epoch 118 iteration 0080/1263: training loss 0.157
Epoch 118 iteration 0100/1263: training loss 0.154
Epoch 118 iteration 0120/1263: training loss 0.150
Epoch 118 iteration 0140/1263: training loss 0.149
Epoch 118 iteration 0160/1263: training loss 0.149
Epoch 118 iteration 0180/1263: training loss 0.150
Epoch 118 iteration 0200/1263: training loss 0.149
Epoch 118 iteration 0220/1263: training loss 0.148
Epoch 118 iteration 0240/1263: training loss 0.148
Epoch 118 iteration 0260/1263: training loss 0.148
Epoch 118 iteration 0280/1263: training loss 0.148
Epoch 118 iteration 0300/1263: training loss 0.148
Epoch 118 iteration 0320/1263: training loss 0.148
Epoch 118 iteration 0340/1263: training loss 0.149
Epoch 118 iteration 0360/1263: training loss 0.149
Epoch 118 iteration 0380/1263: training loss 0.149
Epoch 118 iteration 0400/1263: training loss 0.150
Epoch 118 iteration 0420/1263: training loss 0.150
Epoch 118 iteration 0440/1263: training loss 0.150
Epoch 118 iteration 0460/1263: training loss 0.150
Epoch 118 iteration 0480/1263: training loss 0.149
Epoch 118 iteration 0500/1263: training loss 0.149
Epoch 118 iteration 0520/1263: training loss 0.149
Epoch 118 iteration 0540/1263: training loss 0.149
Epoch 118 iteration 0560/1263: training loss 0.149
Epoch 118 iteration 0580/1263: training loss 0.149
Epoch 118 iteration 0600/1263: training loss 0.149
Epoch 118 iteration 0620/1263: training loss 0.149
Epoch 118 iteration 0640/1263: training loss 0.149
Epoch 118 iteration 0660/1263: training loss 0.150
Epoch 118 iteration 0680/1263: training loss 0.150
Epoch 118 iteration 0700/1263: training loss 0.150
Epoch 118 iteration 0720/1263: training loss 0.150
Epoch 118 iteration 0740/1263: training loss 0.150
Epoch 118 iteration 0760/1263: training loss 0.150
Epoch 118 iteration 0780/1263: training loss 0.150
Epoch 118 iteration 0800/1263: training loss 0.150
Epoch 118 iteration 0820/1263: training loss 0.150
Epoch 118 iteration 0840/1263: training loss 0.149
Epoch 118 iteration 0860/1263: training loss 0.150
Epoch 118 iteration 0880/1263: training loss 0.149
Epoch 118 iteration 0900/1263: training loss 0.149
Epoch 118 iteration 0920/1263: training loss 0.149
Epoch 118 iteration 0940/1263: training loss 0.149
Epoch 118 iteration 0960/1263: training loss 0.149
Epoch 118 iteration 0980/1263: training loss 0.150
Epoch 118 iteration 1000/1263: training loss 0.150
Epoch 118 iteration 1020/1263: training loss 0.150
Epoch 118 iteration 1040/1263: training loss 0.149
Epoch 118 iteration 1060/1263: training loss 0.150
Epoch 118 iteration 1080/1263: training loss 0.150
Epoch 118 iteration 1100/1263: training loss 0.150
Epoch 118 iteration 1120/1263: training loss 0.151
Epoch 118 iteration 1140/1263: training loss 0.151
Epoch 118 iteration 1160/1263: training loss 0.151
Epoch 118 iteration 1180/1264: training loss 0.151
Epoch 118 iteration 1200/1264: training loss 0.151
Epoch 118 iteration 1220/1264: training loss 0.151
Epoch 118 iteration 1240/1264: training loss 0.151
Epoch 118 iteration 1260/1264: training loss 0.151
Epoch 118 validation pixAcc: 0.814, mIoU: 0.479
Epoch 119 iteration 0020/1263: training loss 0.139
Epoch 119 iteration 0040/1263: training loss 0.148
Epoch 119 iteration 0060/1263: training loss 0.151
Epoch 119 iteration 0080/1263: training loss 0.151
Epoch 119 iteration 0100/1263: training loss 0.150
Epoch 119 iteration 0120/1263: training loss 0.149
Epoch 119 iteration 0140/1263: training loss 0.148
Epoch 119 iteration 0160/1263: training loss 0.149
Epoch 119 iteration 0180/1263: training loss 0.148
Epoch 119 iteration 0200/1263: training loss 0.149
Epoch 119 iteration 0220/1263: training loss 0.150
Epoch 119 iteration 0240/1263: training loss 0.150
Epoch 119 iteration 0260/1263: training loss 0.150
Epoch 119 iteration 0280/1263: training loss 0.150
Epoch 119 iteration 0300/1263: training loss 0.150
Epoch 119 iteration 0320/1263: training loss 0.149
Epoch 119 iteration 0340/1263: training loss 0.148
Epoch 119 iteration 0360/1263: training loss 0.148
Epoch 119 iteration 0380/1263: training loss 0.148
Epoch 119 iteration 0400/1263: training loss 0.148
Epoch 119 iteration 0420/1263: training loss 0.148
Epoch 119 iteration 0440/1263: training loss 0.149
Epoch 119 iteration 0460/1263: training loss 0.149
Epoch 119 iteration 0480/1263: training loss 0.150
Epoch 119 iteration 0500/1263: training loss 0.150
Epoch 119 iteration 0520/1263: training loss 0.150
Epoch 119 iteration 0540/1263: training loss 0.150
Epoch 119 iteration 0560/1263: training loss 0.150
Epoch 119 iteration 0580/1263: training loss 0.150
Epoch 119 iteration 0600/1263: training loss 0.150
Epoch 119 iteration 0620/1263: training loss 0.151
Epoch 119 iteration 0640/1263: training loss 0.151
Epoch 119 iteration 0660/1263: training loss 0.151
Epoch 119 iteration 0680/1263: training loss 0.151
Epoch 119 iteration 0700/1263: training loss 0.151
Epoch 119 iteration 0720/1263: training loss 0.151
Epoch 119 iteration 0740/1263: training loss 0.151
Epoch 119 iteration 0760/1263: training loss 0.151
Epoch 119 iteration 0780/1263: training loss 0.150
Epoch 119 iteration 0800/1263: training loss 0.150
Epoch 119 iteration 0820/1263: training loss 0.150
Epoch 119 iteration 0840/1263: training loss 0.150
Epoch 119 iteration 0860/1263: training loss 0.150
Epoch 119 iteration 0880/1263: training loss 0.151
Epoch 119 iteration 0900/1263: training loss 0.150
Epoch 119 iteration 0920/1263: training loss 0.150
Epoch 119 iteration 0940/1263: training loss 0.150
Epoch 119 iteration 0960/1263: training loss 0.150
Epoch 119 iteration 0980/1263: training loss 0.151
Epoch 119 iteration 1000/1263: training loss 0.151
Epoch 119 iteration 1020/1263: training loss 0.151
Epoch 119 iteration 1040/1263: training loss 0.150
Epoch 119 iteration 1060/1263: training loss 0.151
Epoch 119 iteration 1080/1263: training loss 0.150
Epoch 119 iteration 1100/1263: training loss 0.150
Epoch 119 iteration 1120/1263: training loss 0.150
Epoch 119 iteration 1140/1263: training loss 0.151
Epoch 119 iteration 1160/1263: training loss 0.151
Epoch 119 iteration 1180/1263: training loss 0.150
Epoch 119 iteration 1200/1263: training loss 0.151
Epoch 119 iteration 1220/1263: training loss 0.150
Epoch 119 iteration 1240/1263: training loss 0.151
Epoch 119 iteration 1260/1263: training loss 0.151
Epoch 119 validation pixAcc: 0.813, mIoU: 0.479
